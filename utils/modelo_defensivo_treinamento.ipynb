{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a254a35",
   "metadata": {},
   "source": [
    "# Sistema de Detec√ß√£o de Malware Polim√≥rfico Controlado por LLM\n",
    "# Treinamento do Modelo Defensivo - Google Colab\n",
    "\n",
    "Este notebook cont√©m o treinamento completo do modelo de detec√ß√£o de malware keylogger polim√≥rfico baseado no framework te√≥rico-pr√°tico com Random Forest e MALAPI2019.\n",
    "\n",
    "**Objetivo:** Treinar um modelo capaz de identificar malware keylogger polim√≥rfico cujo polimorfismo acontece via comunica√ß√£o com um modelo de LLM.\n",
    "\n",
    "**Dataset:** MALAPI2019 - Cont√©m sequ√™ncias de chamadas de API de malwares e softwares benignos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498ee65",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas e Configura√ß√£o Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d91561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o de depend√™ncias espec√≠ficas para Google Colab\n",
    "!pip install xgboost\n",
    "!pip install shap\n",
    "!pip install joblib\n",
    "\n",
    "# Importa√ß√µes principais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Bibliotecas de ML\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Interpretabilidade\n",
    "import shap\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96a638",
   "metadata": {},
   "source": [
    "## 2. Defini√ß√£o da Classe Principal do Sistema de Detec√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalwareDetectionSystem:\n",
    "    \"\"\"\n",
    "    Sistema de Detec√ß√£o de Malware Polim√≥rfico Controlado por LLM\n",
    "    Implementa Random Forest otimizado com an√°lise em tempo real\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path=None):\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.model = None\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.pca = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.feature_selector = None\n",
    "        self.shap_explainer = None\n",
    "        \n",
    "        # Configura√ß√µes para detec√ß√£o em tempo real\n",
    "        self.api_calls_buffer = defaultdict(list)\n",
    "        self.detection_threshold = 0.7\n",
    "        self.temporal_window = 60  # segundos\n",
    "        \n",
    "        # M√©tricas de performance\n",
    "        self.detection_metrics = {\n",
    "            'true_positives': 0,\n",
    "            'false_positives': 0,\n",
    "            'true_negatives': 0,\n",
    "            'false_negatives': 0,\n",
    "            'detection_times': []\n",
    "        }\n",
    "        \n",
    "        self._setup_logging()\n",
    "        \n",
    "    def _load_config(self, config_path):\n",
    "        \"\"\"Carrega configura√ß√µes baseadas no framework te√≥rico\"\"\"\n",
    "        default_config = {\n",
    "            'random_forest': {\n",
    "                'n_estimators': 300,  # Baseado na literatura: 100-500\n",
    "                'max_depth': None,\n",
    "                'min_samples_split': 5,\n",
    "                'min_samples_leaf': 2,\n",
    "                'criterion': 'gini',\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            },\n",
    "            'tfidf': {\n",
    "                'max_features': 10000,\n",
    "                'ngram_range': (1, 2),\n",
    "                'min_df': 2,\n",
    "                'max_df': 0.95\n",
    "            },\n",
    "            'pca': {\n",
    "                'n_components': 0.95,  # Preservar 95% da vari√¢ncia\n",
    "                'random_state': 42\n",
    "            },\n",
    "            'feature_selection': {\n",
    "                'k_best': 2500  # 25-50% das caracter√≠sticas originais\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'n_estimators': 100,\n",
    "                'max_depth': 6,\n",
    "                'learning_rate': 0.1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if config_path and Path(config_path).exists():\n",
    "            with open(config_path, 'r') as f:\n",
    "                user_config = json.load(f)\n",
    "            default_config.update(user_config)\n",
    "            \n",
    "        return default_config\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configurar sistema de logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Classe MalwareDetectionSystem definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c2d8a",
   "metadata": {},
   "source": [
    "## 3. Carregamento e An√°lise Explorat√≥ria do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar m√©todos de carregamento de dados √† classe\n",
    "def load_malapi_dataset(self, dataset_path):\n",
    "    \"\"\"\n",
    "    Carrega e processa o dataset MALAPI2019\n",
    "    \"\"\"\n",
    "    self.logger.info(\"Carregando dataset MALAPI2019...\")\n",
    "    \n",
    "    try:\n",
    "        # Carregar dataset\n",
    "        if dataset_path.endswith('.csv'):\n",
    "            df = pd.read_csv(dataset_path)\n",
    "        elif dataset_path.endswith('.json'):\n",
    "            df = pd.read_json(dataset_path)\n",
    "        elif dataset_path.endswith('.txt'):\n",
    "            # Para arquivos .txt, assumir formato espec√≠fico do MALAPI2019\n",
    "            df = pd.read_csv(dataset_path, delimiter='\\t', header=0)\n",
    "        else:\n",
    "            raise ValueError(\"Formato de dataset n√£o suportado\")\n",
    "        \n",
    "        self.logger.info(f\"Dataset carregado: {df.shape}\")\n",
    "        \n",
    "        # An√°lise explorat√≥ria\n",
    "        self._exploratory_analysis(df)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Erro ao carregar dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def _exploratory_analysis(self, df):\n",
    "    \"\"\"An√°lise explorat√≥ria do dataset\"\"\"\n",
    "    self.logger.info(\"=== AN√ÅLISE EXPLORAT√ìRIA ===\")\n",
    "    self.logger.info(f\"Formato do dataset: {df.shape}\")\n",
    "    self.logger.info(f\"Colunas: {list(df.columns)}\")\n",
    "    \n",
    "    if 'label' in df.columns or 'class' in df.columns:\n",
    "        target_col = 'label' if 'label' in df.columns else 'class'\n",
    "        self.logger.info(f\"Distribui√ß√£o de classes:\")\n",
    "        self.logger.info(f\"{df[target_col].value_counts()}\")\n",
    "    \n",
    "    # Verificar valores ausentes\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.any():\n",
    "        self.logger.warning(f\"Valores ausentes encontrados: {missing_values.sum()}\")\n",
    "\n",
    "# Adicionar m√©todos √† classe\n",
    "MalwareDetectionSystem.load_malapi_dataset = load_malapi_dataset\n",
    "MalwareDetectionSystem._exploratory_analysis = _exploratory_analysis\n",
    "\n",
    "print(\"‚úÖ M√©todos de carregamento adicionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d895c333",
   "metadata": {},
   "source": [
    "## 4. Upload e Carregamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f70943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para Google Colab - Upload do arquivo de dataset\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Por favor, fa√ßa upload do arquivo 'all_analysis_data.txt':\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Obter o nome do arquivo uploaded\n",
    "dataset_filename = list(uploaded.keys())[0]\n",
    "print(f\"‚úÖ Arquivo '{dataset_filename}' carregado com sucesso!\")\n",
    "\n",
    "# Inicializar o sistema de detec√ß√£o\n",
    "detector = MalwareDetectionSystem()\n",
    "\n",
    "# Carregar o dataset\n",
    "df = detector.load_malapi_dataset(dataset_filename)\n",
    "\n",
    "print(f\"üìä Dataset carregado: {df.shape}\")\n",
    "print(f\"üìã Colunas dispon√≠veis: {list(df.columns)}\")\n",
    "print(f\"üìà Primeiras linhas do dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a6667",
   "metadata": {},
   "source": [
    "## 4.1. Filtragem de Dados - Foco em Spyware\n",
    "\n",
    "Para este estudo espec√≠fico sobre malware keylogger polim√≥rfico, vamos filtrar o dataset para manter apenas os dados classificados como **\"Spyware\"** conforme o arquivo `labels.csv`. Esta filtragem √© essencial pois nosso modelo defensivo tem como objetivo detectar especificamente este tipo de amea√ßa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload do arquivo labels.csv\n",
    "print(\"Por favor, fa√ßa upload do arquivo 'labels.csv':\")\n",
    "uploaded_labels = files.upload()\n",
    "\n",
    "# Obter o nome do arquivo de labels uploaded\n",
    "labels_filename = list(uploaded_labels.keys())[0]\n",
    "print(f\"‚úÖ Arquivo '{labels_filename}' carregado com sucesso!\")\n",
    "\n",
    "# Carregar as labels\n",
    "labels_df = pd.read_csv(labels_filename, header=None, names=['label'])\n",
    "print(f\"üìä Labels carregadas: {labels_df.shape}\")\n",
    "\n",
    "# Verificar distribui√ß√£o de labels\n",
    "print(f\"üìà Distribui√ß√£o original de labels:\")\n",
    "label_counts = labels_df['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# Verificar se temos correspond√™ncia entre dataset e labels\n",
    "if len(df) != len(labels_df):\n",
    "    print(f\"‚ö†Ô∏è ATEN√á√ÉO: Tamanhos diferentes - Dataset: {len(df)}, Labels: {len(labels_df)}\")\n",
    "    # Ajustar para o menor tamanho\n",
    "    min_size = min(len(df), len(labels_df))\n",
    "    df = df.iloc[:min_size].copy()\n",
    "    labels_df = labels_df.iloc[:min_size].copy()\n",
    "    print(f\"üîß Ajustado para tamanho comum: {min_size}\")\n",
    "\n",
    "# Adicionar coluna de labels ao dataset principal\n",
    "df_with_labels = df.copy()\n",
    "df_with_labels['malware_type'] = labels_df['label']\n",
    "\n",
    "print(f\"‚úÖ Labels adicionadas ao dataset principal!\")\n",
    "\n",
    "# Filtrar apenas dados classificados como \"Spyware\"\n",
    "spyware_mask = df_with_labels['malware_type'] == 'Spyware'\n",
    "df_spyware = df_with_labels[spyware_mask].copy()\n",
    "\n",
    "print(f\"\\nüéØ FILTRAGEM REALIZADA:\")\n",
    "print(f\"üìä Dataset original: {len(df_with_labels)} amostras\")\n",
    "print(f\"üïµÔ∏è Amostras Spyware: {len(df_spyware)} amostras\")\n",
    "print(f\"üìâ Percentual Spyware: {len(df_spyware)/len(df_with_labels)*100:.2f}%\")\n",
    "\n",
    "# Verificar se temos dados suficientes para treinamento\n",
    "if len(df_spyware) < 100:\n",
    "    print(\"‚ö†Ô∏è ATEN√á√ÉO: Poucos dados de Spyware encontrados!\")\n",
    "    print(\"Considere incluir dados benignos para criar um dataset balanceado...\")\n",
    "    \n",
    "    # Criar dataset balanceado: Spyware vs Outros (como \"Benign\")\n",
    "    print(\"\\nüîÑ Criando dataset balanceado: Spyware vs Outros...\")\n",
    "    \n",
    "    # Amostrar outros tipos de malware como \"Outros\"\n",
    "    other_mask = df_with_labels['malware_type'] != 'Spyware'\n",
    "    df_others = df_with_labels[other_mask].copy()\n",
    "    \n",
    "    # Pegar quantidade similar de \"outros\" para balancear\n",
    "    n_others = min(len(df_others), len(df_spyware) * 3)  # 3x mais \"outros\" para simular dados reais\n",
    "    df_others_sampled = df_others.sample(n=n_others, random_state=42)\n",
    "    df_others_sampled['malware_type'] = 'Others'  # Renomear para \"Others\"\n",
    "    \n",
    "    # Combinar Spyware + Others\n",
    "    df_final = pd.concat([df_spyware, df_others_sampled], ignore_index=True)\n",
    "    \n",
    "    print(f\"üìä Dataset final balanceado:\")\n",
    "    print(f\"  üïµÔ∏è Spyware: {len(df_spyware)} amostras\")\n",
    "    print(f\"  üîí Others: {len(df_others_sampled)} amostras\")\n",
    "    print(f\"  üìà Total: {len(df_final)} amostras\")\n",
    "    \n",
    "else:\n",
    "    # Se temos dados suficientes de Spyware, criar dataset bin√°rio: Spyware vs Benign\n",
    "    print(\"\\nüîÑ Criando dataset bin√°rio: Spyware vs Benign...\")\n",
    "    \n",
    "    # Simular dados benignos (assumindo que parte dos dados s√£o benignos)\n",
    "    # Em uma implementa√ß√£o real, voc√™ teria dados benignos separados\n",
    "    df_final = df_spyware.copy()\n",
    "    \n",
    "    # Para fins de demonstra√ß√£o, vamos criar uma classe \"Benign\" artificial\n",
    "    # baseada em uma amostra do dataset original (pode ser melhorada)\n",
    "    benign_size = len(df_spyware)\n",
    "    df_benign = df.sample(n=benign_size, random_state=42).copy()\n",
    "    df_benign['malware_type'] = 'Benign'\n",
    "    \n",
    "    df_final = pd.concat([df_spyware, df_benign], ignore_index=True)\n",
    "    \n",
    "    print(f\"üìä Dataset final:\")\n",
    "    print(f\"  üïµÔ∏è Spyware: {len(df_spyware)} amostras\")\n",
    "    print(f\"  ‚úÖ Benign: {len(df_benign)} amostras\")\n",
    "    print(f\"  üìà Total: {len(df_final)} amostras\")\n",
    "\n",
    "# Atualizar o dataset global para usar o filtrado\n",
    "df = df_final.copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Filtragem conclu√≠da! Dataset atualizado com foco em Spyware.\")\n",
    "print(f\"üìä Novo formato do dataset: {df.shape}\")\n",
    "print(f\"üè∑Ô∏è Distribui√ß√£o final de classes:\")\n",
    "print(df['malware_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4900018b",
   "metadata": {},
   "source": [
    "## 5. M√©todos de Pr√©-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a982f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(self, df, target_column='class'):\n",
    "    \"\"\"\n",
    "    Pr√©-processamento completo baseado no framework te√≥rico\n",
    "    \"\"\"\n",
    "    self.logger.info(\"Iniciando pr√©-processamento...\")\n",
    "    \n",
    "    try:\n",
    "        # Separar features e target\n",
    "        if target_column not in df.columns:\n",
    "            # Tentar encontrar coluna de target automaticamente\n",
    "            possible_targets = ['class', 'label', 'malware_type', 'family']\n",
    "            target_column = next((col for col in possible_targets if col in df.columns), None)\n",
    "            \n",
    "            if target_column is None:\n",
    "                raise ValueError(\"Coluna de target n√£o encontrada\")\n",
    "        \n",
    "        y = df[target_column]\n",
    "        X = df.drop(columns=[target_column])\n",
    "        \n",
    "        # Codificar labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Processar chamadas de API\n",
    "        if 'api_calls' in X.columns:\n",
    "            # Assumindo que as chamadas de API est√£o em formato de texto/lista\n",
    "            api_sequences = self._process_api_calls(X['api_calls'])\n",
    "        else:\n",
    "            # Se n√£o houver coluna espec√≠fica, usar todas as features num√©ricas\n",
    "            api_sequences = X.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Aplicar TF-IDF para an√°lise de padr√µes de API\n",
    "        if isinstance(api_sequences, pd.Series):\n",
    "            X_tfidf = self._apply_tfidf(api_sequences)\n",
    "        else:\n",
    "            # Para features num√©ricas, normalizar\n",
    "            X_tfidf = self.scaler.fit_transform(api_sequences)\n",
    "        \n",
    "        # Sele√ß√£o de caracter√≠sticas usando Mutual Information\n",
    "        X_selected = self._feature_selection(X_tfidf, y_encoded)\n",
    "        \n",
    "        # Aplicar PCA se necess√°rio\n",
    "        if X_selected.shape[1] > 1000:\n",
    "            X_final = self._apply_pca(X_selected)\n",
    "        else:\n",
    "            X_final = X_selected\n",
    "        \n",
    "        self.logger.info(f\"Pr√©-processamento conclu√≠do: {X_final.shape}\")\n",
    "        \n",
    "        return X_final, y_encoded\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Erro no pr√©-processamento: {e}\")\n",
    "        raise\n",
    "\n",
    "def _process_api_calls(self, api_calls_series):\n",
    "    \"\"\"Processar sequ√™ncias de chamadas de API\"\"\"\n",
    "    processed_calls = []\n",
    "    \n",
    "    for calls in api_calls_series:\n",
    "        if isinstance(calls, str):\n",
    "            # Se for string, assumir que s√£o chamadas separadas por v√≠rgula/espa√ßo\n",
    "            call_sequence = ' '.join(calls.split())\n",
    "        elif isinstance(calls, list):\n",
    "            # Se for lista, juntar em string\n",
    "            call_sequence = ' '.join(str(call) for call in calls)\n",
    "        else:\n",
    "            call_sequence = str(calls)\n",
    "        \n",
    "        processed_calls.append(call_sequence)\n",
    "    \n",
    "    return pd.Series(processed_calls)\n",
    "\n",
    "def _apply_tfidf(self, text_series):\n",
    "    \"\"\"Aplicar TF-IDF conforme framework te√≥rico\"\"\"\n",
    "    self.logger.info(\"Aplicando TF-IDF...\")\n",
    "    \n",
    "    self.tfidf_vectorizer = TfidfVectorizer(**self.config['tfidf'])\n",
    "    X_tfidf = self.tfidf_vectorizer.fit_transform(text_series)\n",
    "    \n",
    "    return X_tfidf.toarray()\n",
    "\n",
    "def _feature_selection(self, X, y):\n",
    "    \"\"\"Sele√ß√£o de caracter√≠sticas usando Mutual Information\"\"\"\n",
    "    self.logger.info(\"Aplicando sele√ß√£o de caracter√≠sticas...\")\n",
    "    \n",
    "    k_best = min(self.config['feature_selection']['k_best'], X.shape[1])\n",
    "    self.feature_selector = SelectKBest(score_func=mutual_info_classif, k=k_best)\n",
    "    X_selected = self.feature_selector.fit_transform(X, y)\n",
    "    \n",
    "    self.logger.info(f\"Caracter√≠sticas selecionadas: {X_selected.shape[1]}\")\n",
    "    \n",
    "    return X_selected\n",
    "\n",
    "def _apply_pca(self, X):\n",
    "    \"\"\"Aplicar PCA para redu√ß√£o de dimensionalidade\"\"\"\n",
    "    self.logger.info(\"Aplicando PCA...\")\n",
    "    \n",
    "    self.pca = PCA(**self.config['pca'])\n",
    "    X_pca = self.pca.fit_transform(X)\n",
    "    \n",
    "    explained_variance = sum(self.pca.explained_variance_ratio_)\n",
    "    self.logger.info(f\"Vari√¢ncia explicada pelo PCA: {explained_variance:.3f}\")\n",
    "    \n",
    "    return X_pca\n",
    "\n",
    "# Adicionar m√©todos √† classe\n",
    "MalwareDetectionSystem.preprocess_data = preprocess_data\n",
    "MalwareDetectionSystem._process_api_calls = _process_api_calls\n",
    "MalwareDetectionSystem._apply_tfidf = _apply_tfidf\n",
    "MalwareDetectionSystem._feature_selection = _feature_selection\n",
    "MalwareDetectionSystem._apply_pca = _apply_pca\n",
    "\n",
    "print(\"‚úÖ M√©todos de pr√©-processamento adicionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041fda6a",
   "metadata": {},
   "source": [
    "## 6. Pr√©-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8000f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar pr√©-processamento\n",
    "print(\"üîÑ Iniciando pr√©-processamento dos dados filtrados...\")\n",
    "\n",
    "# Identificar automaticamente a coluna de target\n",
    "target_candidates = ['malware_type', 'class', 'label', 'family']\n",
    "target_column = None\n",
    "\n",
    "for candidate in target_candidates:\n",
    "    if candidate in df.columns:\n",
    "        target_column = candidate\n",
    "        break\n",
    "\n",
    "if target_column is None:\n",
    "    print(\"‚ùå Coluna de target n√£o encontrada. Colunas dispon√≠veis:\")\n",
    "    print(df.columns.tolist())\n",
    "    # Permitir sele√ß√£o manual\n",
    "    print(\"Por favor, especifique manualmente a coluna de target:\")\n",
    "    target_column = input()\n",
    "\n",
    "print(f\"üéØ Coluna de target identificada: {target_column}\")\n",
    "print(f\"üìä Classes dispon√≠veis: {df[target_column].unique()}\")\n",
    "\n",
    "# Executar pr√©-processamento\n",
    "X_processed, y_processed = detector.preprocess_data(df, target_column=target_column)\n",
    "\n",
    "print(f\"‚úÖ Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"üìä Formato dos dados processados: {X_processed.shape}\")\n",
    "print(f\"üè∑Ô∏è Classes identificadas: {detector.label_encoder.classes_}\")\n",
    "print(f\"üìà Distribui√ß√£o de classes: {np.bincount(y_processed)}\")\n",
    "\n",
    "# Exibir estat√≠sticas detalhadas\n",
    "unique_labels, label_counts = np.unique(y_processed, return_counts=True)\n",
    "for i, (label_encoded, count) in enumerate(zip(unique_labels, label_counts)):\n",
    "    label_name = detector.label_encoder.inverse_transform([label_encoded])[0]\n",
    "    percentage = count / len(y_processed) * 100\n",
    "    print(f\"  {label_name}: {count} amostras ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c87843",
   "metadata": {},
   "source": [
    "## 7. M√©todos de Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4801d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(self, X, y, test_size=0.2, validation=True):\n",
    "    \"\"\"\n",
    "    Treinar modelo Random Forest otimizado com ensemble\n",
    "    \"\"\"\n",
    "    self.logger.info(\"Iniciando treinamento do modelo...\")\n",
    "    \n",
    "    # Dividir dados\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Configurar Random Forest\n",
    "    rf_model = RandomForestClassifier(**self.config['random_forest'])\n",
    "    \n",
    "    # Configurar XGBoost para ensemble\n",
    "    xgb_model = xgb.XGBClassifier(**self.config['xgboost'])\n",
    "    \n",
    "    # Criar ensemble com Voting Classifier\n",
    "    self.model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf_model),\n",
    "            ('xgb', xgb_model)\n",
    "        ],\n",
    "        voting='soft'  # Usar probabilidades\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo\n",
    "    self.logger.info(\"Treinando ensemble...\")\n",
    "    self.model.fit(X_train, y_train)\n",
    "    \n",
    "    # Valida√ß√£o\n",
    "    if validation:\n",
    "        self._validate_model(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Configurar SHAP para interpretabilidade\n",
    "    self._setup_shap_explainer(X_train)\n",
    "    \n",
    "    self.logger.info(\"Treinamento conclu√≠do!\")\n",
    "    \n",
    "    return self.model\n",
    "\n",
    "def _validate_model(self, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Valida√ß√£o abrangente do modelo\"\"\"\n",
    "    self.logger.info(\"=== VALIDA√á√ÉO DO MODELO ===\")\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = self.model.predict(X_test)\n",
    "    y_pred_proba = self.model.predict_proba(X_test)\n",
    "    \n",
    "    # M√©tricas b√°sicas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "    \n",
    "    self.logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "    self.logger.info(f\"AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Relat√≥rio detalhado\n",
    "    report = classification_report(y_test, y_pred, \n",
    "                                 target_names=self.label_encoder.classes_)\n",
    "    self.logger.info(f\"Relat√≥rio de Classifica√ß√£o:\\n{report}\")\n",
    "    \n",
    "    # Valida√ß√£o cruzada\n",
    "    cv_scores = cross_val_score(self.model, X_train, y_train, cv=5)\n",
    "    self.logger.info(f\"Cross-validation scores: {cv_scores}\")\n",
    "    self.logger.info(f\"CV Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Retornar m√©tricas para visualiza√ß√£o\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'cv_scores': cv_scores,\n",
    "        'classification_report': report,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "def _setup_shap_explainer(self, X_train):\n",
    "    \"\"\"Configurar SHAP para interpretabilidade\"\"\"\n",
    "    try:\n",
    "        # Usar uma amostra menor para SHAP devido √† complexidade computacional\n",
    "        sample_size = min(100, len(X_train))\n",
    "        X_sample = X_train[:sample_size]\n",
    "        \n",
    "        self.shap_explainer = shap.Explainer(self.model.predict, X_sample)\n",
    "        self.logger.info(\"SHAP explainer configurado\")\n",
    "    except Exception as e:\n",
    "        self.logger.warning(f\"N√£o foi poss√≠vel configurar SHAP: {e}\")\n",
    "\n",
    "def save_model(self, filepath):\n",
    "    \"\"\"Salvar modelo treinado\"\"\"\n",
    "    model_data = {\n",
    "        'model': self.model,\n",
    "        'tfidf_vectorizer': self.tfidf_vectorizer,\n",
    "        'pca': self.pca,\n",
    "        'scaler': self.scaler,\n",
    "        'label_encoder': self.label_encoder,\n",
    "        'feature_selector': self.feature_selector,\n",
    "        'config': self.config\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, filepath)\n",
    "    self.logger.info(f\"Modelo salvo em: {filepath}\")\n",
    "\n",
    "# Adicionar m√©todos √† classe\n",
    "MalwareDetectionSystem.train_model = train_model\n",
    "MalwareDetectionSystem._validate_model = _validate_model\n",
    "MalwareDetectionSystem._setup_shap_explainer = _setup_shap_explainer\n",
    "MalwareDetectionSystem.save_model = save_model\n",
    "\n",
    "print(\"‚úÖ M√©todos de treinamento adicionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27798895",
   "metadata": {},
   "source": [
    "## 8. Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a015f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "print(\"ü§ñ Iniciando treinamento do modelo...\")\n",
    "print(\"‚è±Ô∏è Este processo pode levar alguns minutos...\")\n",
    "\n",
    "# Treinar modelo com valida√ß√£o\n",
    "model = detector.train_model(X_processed, y_processed, test_size=0.2, validation=True)\n",
    "\n",
    "print(\"‚úÖ Treinamento conclu√≠do com sucesso!\")\n",
    "print(f\"üéØ Modelo treinado: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9bb87f",
   "metadata": {},
   "source": [
    "## 9. Visualiza√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac574eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar valida√ß√£o detalhada para obter m√©tricas\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed\n",
    ")\n",
    "\n",
    "validation_results = detector._validate_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Visualizar matriz de confus√£o\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(validation_results['y_test'], validation_results['y_pred'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=detector.label_encoder.classes_,\n",
    "            yticklabels=detector.label_encoder.classes_)\n",
    "plt.title('Matriz de Confus√£o - Modelo de Detec√ß√£o de Malware')\n",
    "plt.xlabel('Predi√ß√£o')\n",
    "plt.ylabel('Real')\n",
    "plt.show()\n",
    "\n",
    "# Visualizar scores de valida√ß√£o cruzada\n",
    "plt.figure(figsize=(10, 6))\n",
    "cv_scores = validation_results['cv_scores']\n",
    "plt.bar(range(1, len(cv_scores) + 1), cv_scores)\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', \n",
    "            label=f'M√©dia: {cv_scores.mean():.4f}')\n",
    "plt.title('Scores de Valida√ß√£o Cruzada')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Imprimir resumo das m√©tricas\n",
    "print(\"\\nüìä RESUMO DAS M√âTRICAS DE PERFORMANCE:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üéØ Accuracy: {validation_results['accuracy']:.4f}\")\n",
    "print(f\"üìà AUC Score: {validation_results['auc']:.4f}\")\n",
    "print(f\"üîÑ CV Mean: {validation_results['cv_scores'].mean():.4f} (¬±{validation_results['cv_scores'].std():.4f})\")\n",
    "print(\"\\nüìã Relat√≥rio de Classifica√ß√£o:\")\n",
    "print(validation_results['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ae421",
   "metadata": {},
   "source": [
    "## 10. Interpretabilidade com SHAP (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e991892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de interpretabilidade com SHAP (se dispon√≠vel)\n",
    "if detector.shap_explainer is not None:\n",
    "    print(\"üîç Gerando an√°lise de interpretabilidade com SHAP...\")\n",
    "    \n",
    "    try:\n",
    "        # Usar uma pequena amostra para an√°lise SHAP\n",
    "        sample_size = min(20, len(X_test))\n",
    "        X_sample = X_test[:sample_size]\n",
    "        \n",
    "        # Calcular valores SHAP\n",
    "        shap_values = detector.shap_explainer(X_sample)\n",
    "        \n",
    "        # Visualizar import√¢ncia das caracter√≠sticas\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_sample, show=False)\n",
    "        plt.title('Import√¢ncia das Caracter√≠sticas - SHAP')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ An√°lise SHAP conclu√≠da!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro na an√°lise SHAP: {e}\")\n",
    "        print(\"Continuando sem an√°lise de interpretabilidade...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SHAP explainer n√£o dispon√≠vel. Pulando an√°lise de interpretabilidade.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b70ff7",
   "metadata": {},
   "source": [
    "## 11. Salvar Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2191e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo treinado\n",
    "model_filename = 'malware_detector_model.joblib'\n",
    "detector.save_model(model_filename)\n",
    "\n",
    "print(f\"üíæ Modelo salvo como: {model_filename}\")\n",
    "\n",
    "# Salvar informa√ß√µes do treinamento\n",
    "training_info = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset_shape': df.shape,\n",
    "    'processed_shape': X_processed.shape,\n",
    "    'target_column': target_column,\n",
    "    'classes': detector.label_encoder.classes_.tolist(),\n",
    "    'accuracy': validation_results['accuracy'],\n",
    "    'auc': validation_results['auc'],\n",
    "    'cv_mean': validation_results['cv_scores'].mean(),\n",
    "    'cv_std': validation_results['cv_scores'].std()\n",
    "}\n",
    "\n",
    "with open('training_info.json', 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"üìã Informa√ß√µes do treinamento salvas em: training_info.json\")\n",
    "\n",
    "# Download dos arquivos para uso local (Google Colab)\n",
    "print(\"\\nüì• Baixando arquivos do modelo...\")\n",
    "files.download(model_filename)\n",
    "files.download('training_info.json')\n",
    "\n",
    "print(\"‚úÖ Arquivos baixados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83985f",
   "metadata": {},
   "source": [
    "## 12. Teste de Predi√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de teste para predi√ß√£o\n",
    "def test_prediction(detector, X_test_sample, y_test_sample, num_samples=5):\n",
    "    \"\"\"Testar predi√ß√µes em algumas amostras\"\"\"\n",
    "    \n",
    "    print(\"üß™ Testando predi√ß√µes em amostras do conjunto de teste...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i in range(min(num_samples, len(X_test_sample))):\n",
    "        sample = X_test_sample[i:i+1]\n",
    "        true_label = detector.label_encoder.inverse_transform([y_test_sample[i]])[0]\n",
    "        \n",
    "        # Fazer predi√ß√£o\n",
    "        prediction = detector.model.predict(sample)[0]\n",
    "        predicted_label = detector.label_encoder.inverse_transform([prediction])[0]\n",
    "        \n",
    "        # Obter probabilidades\n",
    "        probabilities = detector.model.predict_proba(sample)[0]\n",
    "        confidence = np.max(probabilities)\n",
    "        \n",
    "        # Exibir resultado\n",
    "        status = \"‚úÖ CORRETO\" if true_label == predicted_label else \"‚ùå INCORRETO\"\n",
    "        print(f\"Amostra {i+1}:\")\n",
    "        print(f\"  Real: {true_label}\")\n",
    "        print(f\"  Predito: {predicted_label}\")\n",
    "        print(f\"  Confian√ßa: {confidence:.3f}\")\n",
    "        print(f\"  Status: {status}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Executar teste de predi√ß√£o\n",
    "test_prediction(detector, X_test, y_test, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38541f",
   "metadata": {},
   "source": [
    "## 13. Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ TREINAMENTO DO MODELO DEFENSIVO CONCLU√çDO!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìã RESUMO DO TREINAMENTO:\")\n",
    "print(f\"üóÉÔ∏è Dataset original: {df.shape}\")\n",
    "print(f\"üîß Dados processados: {X_processed.shape}\")\n",
    "print(f\"üéØ Classes identificadas: {len(detector.label_encoder.classes_)}\")\n",
    "print(f\"üìä Accuracy: {validation_results['accuracy']:.4f}\")\n",
    "print(f\"üìà AUC Score: {validation_results['auc']:.4f}\")\n",
    "print(f\"üîÑ CV Score: {validation_results['cv_scores'].mean():.4f} (¬±{validation_results['cv_scores'].std():.4f})\")\n",
    "\n",
    "print(\"\\nüõ°Ô∏è O modelo est√° pronto para detectar malware keylogger polim√≥rfico!\")\n",
    "print(\"\\nüìÅ Arquivos gerados:\")\n",
    "print(\"  ‚Ä¢ malware_detector_model.joblib - Modelo treinado\")\n",
    "print(\"  ‚Ä¢ training_info.json - Informa√ß√µes do treinamento\")\n",
    "\n",
    "print(\"\\nüöÄ Pr√≥ximos passos:\")\n",
    "print(\"  1. Integrar o modelo com sistema de monitoramento Sysmon\")\n",
    "print(\"  2. Implementar detec√ß√£o em tempo real\")\n",
    "print(\"  3. Configurar alertas e quarentena autom√°tica\")\n",
    "print(\"\\n‚úÖ Treinamento finalizado com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
