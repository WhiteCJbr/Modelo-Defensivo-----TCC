# Sistema de Detecção de Malware Polimórfico Controlado por LLM
# Baseado no Framework Teórico-Prático com Random Forest e MALAPI2019

import pandas as pd
import numpy as np
import joblib
import warnings
from datetime import datetime, timedelta
import threading
import time
import json
import xml.etree.ElementTree as ET
from pathlib import Path
import logging
from collections import defaultdict, Counter
import subprocess
import psutil
import win32evtlog
import win32con

# Bibliotecas de ML
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score
import xgboost as xgb
from sklearn.ensemble import VotingClassifier

# Interpretabilidade
import shap

# Visualização
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

class MalwareDetectionSystem:
    """
    Sistema de Detecção de Malware Polimórfico Controlado por LLM
    Implementa Random Forest otimizado com análise em tempo real
    """
    
    def __init__(self, config_path=None):
        self.config = self._load_config(config_path)
        self.model = None
        self.tfidf_vectorizer = None
        self.pca = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_selector = None
        self.shap_explainer = None
        
        # Configurações para detecção em tempo real
        self.api_calls_buffer = defaultdict(list)
        self.detection_threshold = 0.7
        self.monitoring_active = False
        self.temporal_window = 60  # segundos
        
        # Métricas de performance
        self.detection_metrics = {
            'true_positives': 0,
            'false_positives': 0,
            'true_negatives': 0,
            'false_negatives': 0,
            'detection_times': []
        }
        
        self._setup_logging()
        
    def _load_config(self, config_path):
        """Carrega configurações baseadas no framework teórico"""
        default_config = {
            'random_forest': {
                'n_estimators': 300,  # Baseado na literatura: 100-500
                'max_depth': None,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'criterion': 'gini',
                'random_state': 42,
                'n_jobs': -1
            },
            'tfidf': {
                'max_features': 10000,
                'ngram_range': (1, 2),
                'min_df': 2,
                'max_df': 0.95
            },
            'pca': {
                'n_components': 0.95,  # Preservar 95% da variância
                'random_state': 42
            },
            'feature_selection': {
                'k_best': 2500  # 25-50% das características originais
            },
            'xgboost': {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'random_state': 42
            }
        }
        
        if config_path and Path(config_path).exists():
            with open(config_path, 'r') as f:
                user_config = json.load(f)
            default_config.update(user_config)
            
        return default_config
    
    def _setup_logging(self):
        """Configurar sistema de logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('malware_detection.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_malapi_dataset(self, dataset_path):
        """
        Carrega e processa o dataset MALAPI2019
        """
        self.logger.info("Carregando dataset MALAPI2019...")
        
        try:
            # Carregar dataset
            if dataset_path.endswith('.csv'):
                df = pd.read_csv(dataset_path)
            elif dataset_path.endswith('.json'):
                df = pd.read_json(dataset_path)
            else:
                raise ValueError("Formato de dataset não suportado")
            
            self.logger.info(f"Dataset carregado: {df.shape}")
            
            # Análise exploratória
            self._exploratory_analysis(df)
            
            return df
            
        except Exception as e:
            self.logger.error(f"Erro ao carregar dataset: {e}")
            raise
    
    def _exploratory_analysis(self, df):
        """Análise exploratória do dataset"""
        self.logger.info("=== ANÁLISE EXPLORATÓRIA ===")
        self.logger.info(f"Formato do dataset: {df.shape}")
        self.logger.info(f"Colunas: {list(df.columns)}")
        
        if 'label' in df.columns or 'class' in df.columns:
            target_col = 'label' if 'label' in df.columns else 'class'
            self.logger.info(f"Distribuição de classes:")
            self.logger.info(f"{df[target_col].value_counts()}")
        
        # Verificar valores ausentes
        missing_values = df.isnull().sum()
        if missing_values.any():
            self.logger.warning(f"Valores ausentes encontrados: {missing_values.sum()}")
    
    def preprocess_data(self, df, target_column='class'):
        """
        Pré-processamento completo baseado no framework teórico
        """
        self.logger.info("Iniciando pré-processamento...")
        
        try:
            # Separar features e target
            if target_column not in df.columns:
                # Tentar encontrar coluna de target automaticamente
                possible_targets = ['class', 'label', 'malware_type', 'family']
                target_column = next((col for col in possible_targets if col in df.columns), None)
                
                if target_column is None:
                    raise ValueError("Coluna de target não encontrada")
            
            y = df[target_column]
            X = df.drop(columns=[target_column])
            
            # Codificar labels
            y_encoded = self.label_encoder.fit_transform(y)
            
            # Processar chamadas de API
            if 'api_calls' in X.columns:
                # Assumindo que as chamadas de API estão em formato de texto/lista
                api_sequences = self._process_api_calls(X['api_calls'])
            else:
                # Se não houver coluna específica, usar todas as features numéricas
                api_sequences = X.select_dtypes(include=[np.number])
            
            # Aplicar TF-IDF para análise de padrões de API
            if isinstance(api_sequences, pd.Series):
                X_tfidf = self._apply_tfidf(api_sequences)
            else:
                # Para features numéricas, normalizar
                X_tfidf = self.scaler.fit_transform(api_sequences)
            
            # Seleção de características usando Mutual Information
            X_selected = self._feature_selection(X_tfidf, y_encoded)
            
            # Aplicar PCA se necessário
            if X_selected.shape[1] > 1000:
                X_final = self._apply_pca(X_selected)
            else:
                X_final = X_selected
            
            self.logger.info(f"Pré-processamento concluído: {X_final.shape}")
            
            return X_final, y_encoded
            
        except Exception as e:
            self.logger.error(f"Erro no pré-processamento: {e}")
            raise
    
    def _process_api_calls(self, api_calls_series):
        """Processar sequências de chamadas de API"""
        processed_calls = []
        
        for calls in api_calls_series:
            if isinstance(calls, str):
                # Se for string, assumir que são chamadas separadas por vírgula/espaço
                call_sequence = ' '.join(calls.split())
            elif isinstance(calls, list):
                # Se for lista, juntar em string
                call_sequence = ' '.join(str(call) for call in calls)
            else:
                call_sequence = str(calls)
            
            processed_calls.append(call_sequence)
        
        return pd.Series(processed_calls)
    
    def _apply_tfidf(self, text_series):
        """Aplicar TF-IDF conforme framework teórico"""
        self.logger.info("Aplicando TF-IDF...")
        
        self.tfidf_vectorizer = TfidfVectorizer(**self.config['tfidf'])
        X_tfidf = self.tfidf_vectorizer.fit_transform(text_series)
        
        return X_tfidf.toarray()
    
    def _feature_selection(self, X, y):
        """Seleção de características usando Mutual Information"""
        self.logger.info("Aplicando seleção de características...")
        
        k_best = min(self.config['feature_selection']['k_best'], X.shape[1])
        self.feature_selector = SelectKBest(score_func=mutual_info_classif, k=k_best)
        X_selected = self.feature_selector.fit_transform(X, y)
        
        self.logger.info(f"Características selecionadas: {X_selected.shape[1]}")
        
        return X_selected
    
    def _apply_pca(self, X):
        """Aplicar PCA para redução de dimensionalidade"""
        self.logger.info("Aplicando PCA...")
        
        self.pca = PCA(**self.config['pca'])
        X_pca = self.pca.fit_transform(X)
        
        explained_variance = sum(self.pca.explained_variance_ratio_)
        self.logger.info(f"Variância explicada pelo PCA: {explained_variance:.3f}")
        
        return X_pca
    
    def train_model(self, X, y, test_size=0.2, validation=True):
        """
        Treinar modelo Random Forest otimizado com ensemble
        """
        self.logger.info("Iniciando treinamento do modelo...")
        
        # Dividir dados
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # Configurar Random Forest
        rf_model = RandomForestClassifier(**self.config['random_forest'])
        
        # Configurar XGBoost para ensemble
        xgb_model = xgb.XGBClassifier(**self.config['xgboost'])
        
        # Criar ensemble com Voting Classifier
        self.model = VotingClassifier(
            estimators=[
                ('rf', rf_model),
                ('xgb', xgb_model)
            ],
            voting='soft'  # Usar probabilidades
        )
        
        # Treinar modelo
        self.logger.info("Treinando ensemble...")
        self.model.fit(X_train, y_train)
        
        # Validação
        if validation:
            self._validate_model(X_train, y_train, X_test, y_test)
        
        # Configurar SHAP para interpretabilidade
        self._setup_shap_explainer(X_train)
        
        self.logger.info("Treinamento concluído!")
        
        return self.model
    
    def _validate_model(self, X_train, y_train, X_test, y_test):
        """Validação abrangente do modelo"""
        self.logger.info("=== VALIDAÇÃO DO MODELO ===")
        
        # Predições
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)
        
        # Métricas básicas
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
        
        self.logger.info(f"Accuracy: {accuracy:.4f}")
        self.logger.info(f"AUC: {auc:.4f}")
        
        # Relatório detalhado
        report = classification_report(y_test, y_pred, 
                                     target_names=self.label_encoder.classes_)
        self.logger.info(f"Relatório de Classificação:\n{report}")
        
        # Validação cruzada
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=5)
        self.logger.info(f"Cross-validation scores: {cv_scores}")
        self.logger.info(f"CV Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    def _setup_shap_explainer(self, X_train):
        """Configurar SHAP para interpretabilidade"""
        try:
            # Usar uma amostra menor para SHAP devido à complexidade computacional
            sample_size = min(100, len(X_train))
            X_sample = X_train[:sample_size]
            
            self.shap_explainer = shap.Explainer(self.model.predict, X_sample)
            self.logger.info("SHAP explainer configurado")
        except Exception as e:
            self.logger.warning(f"Não foi possível configurar SHAP: {e}")
    
    def explain_prediction(self, X_sample):
        """Explicar predições usando SHAP"""
        if self.shap_explainer is None:
            return "SHAP explainer não disponível"
        
        try:
            shap_values = self.shap_explainer(X_sample[:1])
            return shap_values
        except Exception as e:
            self.logger.error(f"Erro ao explicar predição: {e}")
            return None
    
    def predict_realtime(self, api_calls, process_info=None):
        """
        Predição em tempo real para detecção de malware
        """
        if self.model is None:
            raise ValueError("Modelo não foi treinado")
        
        try:
            # Processar chamadas de API
            processed_calls = self._process_single_api_sequence(api_calls)
            
            # Aplicar mesmo pré-processamento
            X_processed = self._preprocess_single_sample(processed_calls)
            
            # Predição
            prediction = self.model.predict([X_processed])[0]
            probability = self.model.predict_proba([X_processed])[0]
            
            # Converter predição para rótulo original
            predicted_label = self.label_encoder.inverse_transform([prediction])[0]
            
            # Calcular confiança
            confidence = np.max(probability)
            
            # Determinar se é malware
            is_malware = confidence > self.detection_threshold and predicted_label != 'benign'
            
            result = {
                'prediction': predicted_label,
                'confidence': confidence,
                'is_malware': is_malware,
                'probabilities': dict(zip(self.label_encoder.classes_, probability)),
                'timestamp': datetime.now(),
                'process_info': process_info
            }
            
            if is_malware:
                self.logger.warning(f"MALWARE DETECTADO: {predicted_label} (confiança: {confidence:.3f})")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erro na predição em tempo real: {e}")
            return None
    
    def _process_single_api_sequence(self, api_calls):
        """Processar uma única sequência de API calls"""
        if isinstance(api_calls, list):
            return ' '.join(str(call) for call in api_calls)
        elif isinstance(api_calls, str):
            return api_calls
        else:
            return str(api_calls)
    
    def _preprocess_single_sample(self, api_sequence):
        """Pré-processar uma única amostra para predição"""
        # Aplicar TF-IDF
        if self.tfidf_vectorizer:
            X_tfidf = self.tfidf_vectorizer.transform([api_sequence]).toarray()
        else:
            # Se não houver TF-IDF, assumir que é numérico
            X_tfidf = np.array([api_sequence])
        
        # Seleção de características
        if self.feature_selector:
            X_selected = self.feature_selector.transform(X_tfidf)
        else:
            X_selected = X_tfidf
        
        # PCA
        if self.pca:
            X_final = self.pca.transform(X_selected)
        else:
            X_final = X_selected
        
        return X_final[0]
    
    def start_realtime_monitoring(self):
        """Iniciar monitoramento em tempo real com Sysmon"""
        self.monitoring_active = True
        self.logger.info("Iniciando monitoramento em tempo real...")
        
        # Thread para monitorar eventos do Sysmon
        sysmon_thread = threading.Thread(target=self._monitor_sysmon_events)
        sysmon_thread.daemon = True
        sysmon_thread.start()
        
        # Thread para análise periódica
        analysis_thread = threading.Thread(target=self._periodic_analysis)
        analysis_thread.daemon = True
        analysis_thread.start()
    
    def stop_realtime_monitoring(self):
        """Parar monitoramento em tempo real"""
        self.monitoring_active = False
        self.logger.info("Monitoramento em tempo real parado")
    
    def _monitor_sysmon_events(self):
        """Monitorar eventos do Sysmon"""
        try:
            # Conectar ao log do Sysmon
            hand = win32evtlog.OpenEventLog(None, "Microsoft-Windows-Sysmon/Operational")
            
            while self.monitoring_active:
                # Ler eventos recentes
                events = win32evtlog.ReadEventLog(
                    hand,
                    win32evtlog.EVENTLOG_BACKWARDS_READ | win32evtlog.EVENTLOG_SEQUENTIAL_READ,
                    0
                )
                
                for event in events:
                    if event.EventID in [1, 3, 7, 8, 10]:  # Process creation, Network, Image load, etc.
                        self._process_sysmon_event(event)
                
                time.sleep(1)  # Verificar a cada segundo
                
        except Exception as e:
            self.logger.error(f"Erro no monitoramento Sysmon: {e}")
    
    def _process_sysmon_event(self, event):
        """Processar evento individual do Sysmon"""
        try:
            # Extrair informações do evento
            event_data = self._parse_sysmon_event(event)
            
            if event_data:
                process_id = event_data.get('ProcessId')
                if process_id:
                    # Coletar chamadas de API para este processo
                    api_calls = self._collect_api_calls(process_id)
                    
                    if api_calls:
                        # Adicionar ao buffer para análise
                        self.api_calls_buffer[process_id].extend(api_calls)
                        
        except Exception as e:
            self.logger.error(f"Erro ao processar evento Sysmon: {e}")
    
    def _parse_sysmon_event(self, event):
        """Parsear evento do Sysmon"""
        try:
            # Converter evento para dicionário
            event_data = {}
            
            if hasattr(event, 'StringInserts'):
                # Extrair dados básicos
                inserts = event.StringInserts
                if len(inserts) > 0:
                    event_data['ProcessId'] = inserts[0] if len(inserts) > 0 else None
                    event_data['Image'] = inserts[4] if len(inserts) > 4 else None
                    event_data['CommandLine'] = inserts[5] if len(inserts) > 5 else None
            
            return event_data
            
        except Exception as e:
            self.logger.error(f"Erro ao parsear evento: {e}")
            return None
    
    def _collect_api_calls(self, process_id):
        """Coletar chamadas de API para um processo específico"""
        try:
            # Esta função deveria integrar com uma ferramenta de API monitoring
            # Por simplicidade, vamos simular com informações do processo
            
            process = psutil.Process(int(process_id))
            
            # Coletar informações básicas do processo
            api_calls = [
                f"CreateProcess:{process.name()}",
                f"OpenProcess:{process.pid}",
                f"GetProcAddress:{process.exe()}",
            ]
            
            # Adicionar informações de rede se disponível
            try:
                connections = process.connections()
                for conn in connections:
                    api_calls.append(f"WSAConnect:{conn.raddr}")
            except:
                pass
            
            return api_calls
            
        except Exception as e:
            self.logger.debug(f"Erro ao coletar API calls para processo {process_id}: {e}")
            return []
    
    def _periodic_analysis(self):
        """Análise periódica dos dados coletados"""
        while self.monitoring_active:
            try:
                current_time = datetime.now()
                
                # Analisar processos no buffer
                for process_id, api_calls in list(self.api_calls_buffer.items()):
                    if len(api_calls) >= 10:  # Mínimo de API calls para análise
                        # Realizar predição
                        result = self.predict_realtime(api_calls, {'pid': process_id})
                        
                        if result and result['is_malware']:
                            self._handle_malware_detection(process_id, result)
                        
                        # Limpar buffer antigo
                        self.api_calls_buffer[process_id] = api_calls[-50:]  # Manter últimas 50
                
                # Limpar buffers muito antigos
                self._cleanup_old_buffers()
                
                time.sleep(self.temporal_window)  # Analisar a cada minuto
                
            except Exception as e:
                self.logger.error(f"Erro na análise periódica: {e}")
    
    def _handle_malware_detection(self, process_id, detection_result):
        """Lidar com detecção de malware"""
        self.logger.critical(f"MALWARE DETECTADO - PID: {process_id}")
        self.logger.critical(f"Tipo: {detection_result['prediction']}")
        self.logger.critical(f"Confiança: {detection_result['confidence']:.3f}")
        
        # Atualizar métricas
        self.detection_metrics['true_positives'] += 1
        self.detection_metrics['detection_times'].append(datetime.now())
        
        # Ações adicionais (quarentena, alerta, etc.)
        self._quarantine_process(process_id)
    
    def _quarantine_process(self, process_id):
        """Colocar processo em quarentena"""
        try:
            process = psutil.Process(int(process_id))
            self.logger.warning(f"Terminando processo malicioso: {process.name()} (PID: {process_id})")
            process.terminate()
        except Exception as e:
            self.logger.error(f"Erro ao terminar processo {process_id}: {e}")
    
    def _cleanup_old_buffers(self):
        """Limpar buffers antigos"""
        # Remover entradas de processos que não existem mais
        for process_id in list(self.api_calls_buffer.keys()):
            try:
                psutil.Process(int(process_id))
            except psutil.NoSuchProcess:
                del self.api_calls_buffer[process_id]
    
    def save_model(self, filepath):
        """Salvar modelo treinado"""
        model_data = {
            'model': self.model,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'pca': self.pca,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_selector': self.feature_selector,
            'config': self.config
        }
        
        joblib.dump(model_data, filepath)
        self.logger.info(f"Modelo salvo em: {filepath}")
    
    def load_model(self, filepath):
        """Carregar modelo treinado"""
        model_data = joblib.load(filepath)
        
        self.model = model_data['model']
        self.tfidf_vectorizer = model_data['tfidf_vectorizer']
        self.pca = model_data['pca']
        self.scaler = model_data['scaler']
        self.label_encoder = model_data['label_encoder']
        self.feature_selector = model_data['feature_selector']
        self.config = model_data['config']
        
        self.logger.info(f"Modelo carregado de: {filepath}")
    
    def get_performance_metrics(self):
        """Obter métricas de performance"""
        metrics = self.detection_metrics.copy()
        
        if metrics['detection_times']:
            # Calcular tempo médio de detecção
            times = metrics['detection_times']
            if len(times) > 1:
                avg_detection_time = sum((times[i] - times[i-1]).total_seconds() 
                                       for i in range(1, len(times))) / (len(times) - 1)
                metrics['avg_detection_time_seconds'] = avg_detection_time
        
        # Calcular taxas
        total_detections = (metrics['true_positives'] + metrics['false_positives'] + 
                          metrics['true_negatives'] + metrics['false_negatives'])
        
        if total_detections > 0:
            metrics['accuracy'] = ((metrics['true_positives'] + metrics['true_negatives']) / 
                                 total_detections)
            metrics['precision'] = (metrics['true_positives'] / 
                                  (metrics['true_positives'] + metrics['false_positives'])
                                  if (metrics['true_positives'] + metrics['false_positives']) > 0 else 0)
            metrics['recall'] = (metrics['true_positives'] / 
                               (metrics['true_positives'] + metrics['false_negatives'])
                               if (metrics['true_positives'] + metrics['false_negatives']) > 0 else 0)
        
        return metrics

# Funções utilitárias para integração

def download_malapi_dataset():
    """Baixar dataset MALAPI2019 do GitHub"""
    import urllib.request
    
    url = "https://github.com/ocatak/malware_api_class/raw/main/data/malapi2019.csv"
    filename = "malapi2019.csv"
    
    try:
        urllib.request.urlretrieve(url, filename)
        print(f"Dataset baixado: {filename}")
        return filename
    except Exception as e:
        print(f"Erro ao baixar dataset: {e}")
        return None

def setup_sysmon():
    """Configurar Sysmon para coleta de dados"""
    print("Para configurar o Sysmon:")
    print("1. Baixe o Sysmon do Microsoft Sysinternals")
    print("2. Execute: sysmon64.exe -accepteula -i sysmonconfig.xml")
    print("3. Configure os eventos necessários no XML")
    
    # Configuração XML básica para Sysmon
    sysmon_config = """<?xml version="1.0" encoding="UTF-8"?>
<Sysmon schemaversion="4.82">
  <EventFiltering>
    <ProcessCreate onmatch="include">
      <Image condition="end with">exe</Image>
    </ProcessCreate>
    <NetworkConnect onmatch="include">
      <DestinationPort condition="is">443</DestinationPort>
      <DestinationPort condition="is">80</DestinationPort>
    </NetworkConnect>
    <ImageLoaded onmatch="include">
      <ImageLoaded condition="end with">dll</ImageLoaded>
    </ImageLoaded>
  </EventFiltering>
</Sysmon>"""
    
    with open("sysmonconfig.xml", "w") as f:
        f.write(sysmon_config)
    
    print("Arquivo sysmonconfig.xml criado")

# Exemplo de uso principal
def main():
    """Exemplo de uso do sistema"""
    
    # Inicializar sistema
    detector = MalwareDetectionSystem()
    
    # Baixar dataset se necessário
    dataset_path = "malapi2019.csv"
    if not Path(dataset_path).exists():
        dataset_path = download_malapi_dataset()
        if dataset_path is None:
            print("Não foi possível baixar o dataset")
            return
    
    # Carregar e processar dados
    df = detector.load_malapi_dataset(dataset_path)
    X, y = detector.preprocess_data(df)
    
    # Treinar modelo
    model = detector.train_model(X, y)
    
    # Salvar modelo
    detector.save_model("malware_detector_model.joblib")
    
    # Iniciar monitoramento em tempo real
    print("Iniciando monitoramento em tempo real...")
    detector.start_realtime_monitoring()
    
    try:
        # Manter monitoramento ativo
        while True:
            time.sleep(60)
            metrics = detector.get_performance_metrics()
            print(f"Detecções: TP={metrics['true_positives']}, FP={metrics['false_positives']}")
            
    except KeyboardInterrupt:
        print("Parando monitoramento...")
        detector.stop_realtime_monitoring()

if __name__ == "__main__":
    main()


# ========================================
# PIPELINE COMPLETO DE DESENVOLVIMENTO
# ========================================

class MalwareDetectionPipeline:
    """
    Pipeline completo para desenvolvimento e deploy do sistema
    """
    
    def __init__(self):
        self.detector = MalwareDetectionSystem()
        self.pipeline_steps = [
            "data_collection",
            "data_preprocessing", 
            "feature_engineering",
            "model_training",
            "model_validation",
            "model_optimization",
            "deployment_preparation",
            "real_time_deployment"
        ]
        
    def run_complete_pipeline(self, dataset_path=None, target_column='class'):
        """Executar pipeline completo"""
        
        print("="*60)
        print("PIPELINE DE DESENVOLVIMENTO - DETECTOR DE MALWARE POLIMÓRFICO")
        print("="*60)
        
        # 1. COLETA DE DADOS
        print("\n1. COLETA DE DADOS")
        print("-" * 30)
        
        if dataset_path is None:
            print("Baixando dataset MALAPI2019...")
            dataset_path = download_malapi_dataset()
            if dataset_path is None:
                print("ERRO: Não foi possível obter o dataset")
                return False
        
        # 2. CARREGAMENTO E ANÁLISE EXPLORATÓRIA
        print("\n2. ANÁLISE EXPLORATÓRIA")
        print("-" * 30)
        
        df = self.detector.load_malapi_dataset(dataset_path)
        self._detailed_eda(df)
        
        # 3. PRÉ-PROCESSAMENTO
        print("\n3. PRÉ-PROCESSAMENTO")
        print("-" * 30)
        
        X, y = self.detector.preprocess_data(df, target_column)
        
        # 4. ENGENHARIA DE CARACTERÍSTICAS
        print("\n4. ENGENHARIA DE CARACTERÍSTICAS")
        print("-" * 30)
        
        X_engineered = self._advanced_feature_engineering(X, y)
        
        # 5. TREINAMENTO DO MODELO
        print("\n5. TREINAMENTO DO MODELO")
        print("-" * 30)
        
        model = self.detector.train_model(X_engineered, y, validation=True)
        
        # 6. OTIMIZAÇÃO DE HIPERPARÂMETROS
        print("\n6. OTIMIZAÇÃO DE HIPERPARÂMETROS")
        print("-" * 30)
        
        optimized_model = self._hyperparameter_optimization(X_engineered, y)
        
        # 7. VALIDAÇÃO AVANÇADA
        print("\n7. VALIDAÇÃO AVANÇADA")
        print("-" * 30)
        
        self._advanced_validation(X_engineered, y, optimized_model)
        
        # 8. PREPARAÇÃO PARA DEPLOY
        print("\n8. PREPARAÇÃO PARA DEPLOY")
        print("-" * 30)
        
        self._prepare_deployment()
        
        # 9. TESTES DE INTEGRAÇÃO
        print("\n9. TESTES DE INTEGRAÇÃO")
        print("-" * 30)
        
        self._integration_tests()
        
        print("\n" + "="*60)
        print("PIPELINE CONCLUÍDO COM SUCESSO!")
        print("="*60)
        
        return True
    
    def _detailed_eda(self, df):
        """Análise exploratória detalhada"""
        
        # Estatísticas básicas
        print(f"Dimensões do dataset: {df.shape}")
        print(f"Memória utilizada: {df.memory_usage().sum() / 1024**2:.2f} MB")
        
        # Análise de classes
        if 'class' in df.columns:
            class_dist = df['class'].value_counts()
            print(f"\nDistribuição de classes:")
            for cls, count in class_dist.items():
                print(f"  {cls}: {count} ({count/len(df)*100:.2f}%)")
            
            # Verificar desbalanceamento
            imbalance_ratio = class_dist.max() / class_dist.min()
            if imbalance_ratio > 5:
                print(f"⚠️  Dataset desbalanceado! Razão: {imbalance_ratio:.2f}")
        
        # Análise de features
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        print(f"\nFeatures numéricas: {len(numeric_cols)}")
        
        categorical_cols = df.select_dtypes(include=['object']).columns
        print(f"Features categóricas: {len(categorical_cols)}")
        
        # Valores ausentes
        missing_analysis = df.isnull().sum()
        missing_cols = missing_analysis[missing_analysis > 0]
        if len(missing_cols) > 0:
            print(f"\nColunas com valores ausentes:")
            for col, missing_count in missing_cols.items():
                print(f"  {col}: {missing_count} ({missing_count/len(df)*100:.2f}%)")
    
    def _advanced_feature_engineering(self, X, y):
        """Engenharia de características avançada"""
        
        # Adicionar características temporais se disponível
        X_engineered = X.copy()
        
        # Características estatísticas se for dados numéricos
        if isinstance(X, np.ndarray) and X.dtype in [np.float64, np.float32]:
            # Adicionar estatísticas por linha
            X_stats = np.column_stack([
                np.mean(X, axis=1),      # Média
                np.std(X, axis=1),       # Desvio padrão
                np.max(X, axis=1),       # Máximo
                np.min(X, axis=1),       # Mínimo
                np.median(X, axis=1),    # Mediana
            ])
            
            X_engineered = np.column_stack([X, X_stats])
            print(f"Características estatísticas adicionadas: {X_stats.shape[1]}")
        
        # Detecção de anomalias usando Isolation Forest
        from sklearn.ensemble import IsolationForest
        
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_scores = iso_forest.fit_transform(X_engineered)
        
        if isinstance(X_engineered, np.ndarray):
            X_engineered = np.column_stack([X_engineered, anomaly_scores])
        
        print(f"Score de anomalia adicionado")
        print(f"Dimensões finais: {X_engineered.shape}")
        
        return X_engineered
    
    def _hyperparameter_optimization(self, X, y):
        """Otimização de hiperparâmetros usando GridSearch"""
        
        from sklearn.model_selection import GridSearchCV
        
        # Definir grid de parâmetros
        param_grid = {
            'rf__n_estimators': [200, 300, 500],
            'rf__max_depth': [10, 20, None],
            'rf__min_samples_split': [2, 5, 10],
            'rf__min_samples_leaf': [1, 2, 4],
            'xgb__n_estimators': [100, 200, 300],
            'xgb__max_depth': [3, 6, 9],
            'xgb__learning_rate': [0.01, 0.1, 0.2]
        }
        
        # Grid search com validação cruzada
        grid_search = GridSearchCV(
            self.detector.model,
            param_grid,
            cv=5,
            scoring='f1_weighted',
            n_jobs=-1,
            verbose=1
        )
        
        print("Executando otimização de hiperparâmetros...")
        grid_search.fit(X, y)
        
        print(f"Melhores parâmetros: {grid_search.best_params_}")
        print(f"Melhor score: {grid_search.best_score_:.4f}")
        
        # Atualizar modelo do detector
        self.detector.model = grid_search.best_estimator_
        
        return grid_search.best_estimator_
    
    def _advanced_validation(self, X, y, model):
        """Validação avançada com múltiplas métricas"""
        
        from sklearn.model_selection import StratifiedKFold
        from sklearn.metrics import precision_recall_curve, roc_curve
        
        # Validação cruzada estratificada
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        metrics_summary = {
            'accuracy': [],
            'precision': [],
            'recall': [],
            'f1': [],
            'auc': []
        }
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            # Treinar no fold
            model.fit(X_train_fold, y_train_fold)
            
            # Predições
            y_pred = model.predict(X_val_fold)
            y_pred_proba = model.predict_proba(X_val_fold)
            
            # Calcular métricas
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            
            acc = accuracy_score(y_val_fold, y_pred)
            prec = precision_score(y_val_fold, y_pred, average='weighted')
            rec = recall_score(y_val_fold, y_pred, average='weighted')
            f1 = f1_score(y_val_fold, y_pred, average='weighted')
            auc = roc_auc_score(y_val_fold, y_pred_proba, multi_class='ovr')
            
            metrics_summary['accuracy'].append(acc)
            metrics_summary['precision'].append(prec)
            metrics_summary['recall'].append(rec)
            metrics_summary['f1'].append(f1)
            metrics_summary['auc'].append(auc)
            
            print(f"Fold {fold+1}: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}")
        
        # Resumo final
        print("\n=== RESUMO DA VALIDAÇÃO ===")
        for metric, scores in metrics_summary.items():
            mean_score = np.mean(scores)
            std_score = np.std(scores)
            print(f"{metric.upper()}: {mean_score:.4f} (±{std_score:.4f})")
    
    def _prepare_deployment(self):
        """Preparar arquivos para deployment"""
        
        # Salvar modelo
        model_path = "production_model.joblib"
        self.detector.save_model(model_path)
        
        # Criar arquivo de configuração
        config = {
            "model_path": model_path,
            "detection_threshold": 0.7,
            "temporal_window": 60,
            "log_level": "INFO",
            "sysmon_events": [1, 3, 7, 8, 10],
            "quarantine_enabled": True
        }
        
        with open("deployment_config.json", "w") as f:
            json.dump(config, f, indent=2)
        
        # Criar script de instalação
        install_script = """#!/bin/bash
# Script de instalação do detector de malware

echo "Instalando dependências..."
pip install -r requirements.txt

echo "Configurando Sysmon..."
# Baixar e configurar Sysmon aqui

echo "Criando serviço systemd..."
# Configurar como serviço do sistema

echo "Instalação concluída!"
"""
        
        with open("install.sh", "w") as f:
            f.write(install_script)
        
        # Criar requirements.txt
        requirements = """pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
xgboost>=1.5.0
shap>=0.40.0
psutil>=5.8.0
pywin32>=227
matplotlib>=3.3.0
seaborn>=0.11.0
joblib>=1.1.0
"""
        
        with open("requirements.txt", "w") as f:
            f.write(requirements)
        
        print("Arquivos de deployment criados:")
        print("- production_model.joblib")
        print("- deployment_config.json") 
        print("- install.sh")
        print("- requirements.txt")
    
    def _integration_tests(self):
        """Testes de integração do sistema"""
        
        print("Executando testes de integração...")
        
        # Teste 1: Carregamento do modelo
        try:
            test_detector = MalwareDetectionSystem()
            test_detector.load_model("production_model.joblib")
            print("✓ Carregamento do modelo: OK")
        except Exception as e:
            print(f"✗ Carregamento do modelo: FALHOU ({e})")
        
        # Teste 2: Predição de exemplo
        try:
            sample_api_calls = ["CreateFileA", "WriteFile", "CreateProcess", "OpenProcess"]
            result = test_detector.predict_realtime(sample_api_calls)
            if result is not None:
                print("✓ Predição em tempo real: OK")
            else:
                print("✗ Predição em tempo real: FALHOU")
        except Exception as e:
            print(f"✗ Predição em tempo real: FALHOU ({e})")
        
        # Teste 3: Verificação de dependências
        try:
            import psutil
            import win32evtlog
            print("✓ Dependências do sistema: OK")
        except ImportError as e:
            print(f"✗ Dependências do sistema: FALHOU ({e})")
        
        print("Testes de integração concluídos")


# ========================================
# COLETORES DE DADOS ADICIONAIS
# ========================================

class AdditionalDataCollector:
    """
    Coletor de dados adicionais para melhorar o treinamento
    """
    
    def __init__(self):
        self.collectors = {
            'process_behavior': self._collect_process_behavior,
            'network_patterns': self._collect_network_patterns,
            'file_operations': self._collect_file_operations,
            'registry_changes': self._collect_registry_changes,
            'memory_patterns': self._collect_memory_patterns
        }
    
    def collect_all_data(self, duration_minutes=60):
        """Coletar todos os tipos de dados por um período"""
        
        print(f"Coletando dados por {duration_minutes} minutos...")
        
        collected_data = {
            'timestamp': datetime.now(),
            'duration_minutes': duration_minutes,
            'data': {}
        }
        
        end_time = datetime.now() + timedelta(minutes=duration_minutes)
        
        while datetime.now() < end_time:
            for data_type, collector_func in self.collectors.items():
                try:
                    data = collector_func()
                    if data_type not in collected_data['data']:
                        collected_data['data'][data_type] = []
                    collected_data['data'][data_type].append(data)
                except Exception as e:
                    print(f"Erro ao coletar {data_type}: {e}")
            
            time.sleep(10)  # Coletar a cada 10 segundos
        
        # Salvar dados coletados
        filename = f"additional_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w') as f:
            json.dump(collected_data, f, default=str, indent=2)
        
        print(f"Dados coletados salvos em: {filename}")
        return filename
    
    def _collect_process_behavior(self):
        """Coletar comportamento de processos"""
        processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent', 'create_time']):
            try:
                proc_info = proc.info
                proc_info['connections'] = len(proc.connections())
                proc_info['threads'] = proc.num_threads()
                processes.append(proc_info)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        
        return {
            'timestamp': datetime.now(),
            'processes': processes[:50]  # Top 50 processos
        }
    
    def _collect_network_patterns(self):
        """Coletar padrões de rede"""
        connections = []
        
        for conn in psutil.net_connections():
            if conn.status == 'ESTABLISHED':
                connections.append({
                    'local_addr': f"{conn.laddr.ip}:{conn.laddr.port}" if conn.laddr else None,
                    'remote_addr': f"{conn.raddr.ip}:{conn.raddr.port}" if conn.raddr else None,
                    'pid': conn.pid
                })
        
        return {
            'timestamp': datetime.now(),
            'active_connections': connections,
            'network_stats': dict(psutil.net_io_counters()._asdict())
        }
    
    def _collect_file_operations(self):
        """Coletar operações de arquivo"""
        # Simulação de coleta de operações de arquivo
        # Em implementação real, usaria APIs de monitoramento de arquivo
        
        return {
            'timestamp': datetime.now(),
            'disk_usage': dict(psutil.disk_usage('C:')._asdict()),
            'disk_io': dict(psutil.disk_io_counters()._asdict())
        }
    
    def _collect_registry_changes(self):
        """Coletar mudanças no registro (Windows)"""
        # Placeholder para coleta de mudanças no registro
        # Implementação completa requereria APIs específicas do Windows
        
        return {
            'timestamp': datetime.now(),
            'registry_monitoring': 'placeholder_data'
        }
    
    def _collect_memory_patterns(self):
        """Coletar padrões de uso de memória"""
        memory_info = psutil.virtual_memory()
        
        return {
            'timestamp': datetime.now(),
            'memory_total': memory_info.total,
            'memory_available': memory_info.available,
            'memory_percent': memory_info.percent,
            'memory_used': memory_info.used
        }


# ========================================
# EXEMPLO DE USO COMPLETO
# ========================================

def run_complete_system():
    """Executar sistema completo de detecção"""
    
    print("SISTEMA DE DETECÇÃO DE MALWARE POLIMÓRFICO CONTROLADO POR LLM")
    print("=" * 70)
    
    # 1. Executar pipeline completo
    pipeline = MalwareDetectionPipeline()
    success = pipeline.run_complete_pipeline()
    
    if not success:
        print("ERRO: Pipeline falhou")
        return
    
    # 2. Coletar dados adicionais (opcional)
    print("\nDeseja coletar dados adicionais para melhorar o modelo? (y/n): ", end="")
    if input().lower() == 'y':
        collector = AdditionalDataCollector()
        collector.collect_all_data(duration_minutes=30)
    
    # 3. Configurar Sysmon
    print("\nConfigurando Sysmon...")
    setup_sysmon()
    
    # 4. Iniciar monitoramento
    print("\nIniciando sistema de monitoramento...")
    detector = MalwareDetectionSystem()
    detector.load_model("production_model.joblib")
    detector.start_realtime_monitoring()
    
    print("\n🛡️  SISTEMA ATIVO - Monitorando ameaças em tempo real")
    print("Pressione Ctrl+C para parar")
    
    try:
        while True:
            time.sleep(60)
            metrics = detector.get_performance_metrics()
            print(f"[{datetime.now().strftime('%H:%M:%S')}] Detecções: {metrics['true_positives']}")
    except KeyboardInterrupt:
        detector.stop_realtime_monitoring()
        print("\nSistema parado pelo usuário")

if __name__ == "__main__":
    run_complete_system()
            