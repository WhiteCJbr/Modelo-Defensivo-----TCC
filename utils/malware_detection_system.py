# Sistema de Detec√ß√£o de Malware Polim√≥rfico Controlado por LLM
# Baseado no Framework Te√≥rico-Pr√°tico com Random Forest e MALAPI2019

import pandas as pd
import numpy as np
import joblib
import warnings
from datetime import datetime, timedelta
import threading
import time
import json
import xml.etree.ElementTree as ET
from pathlib import Path
import logging
from collections import defaultdict, Counter
import subprocess
import psutil
import win32evtlog
import win32con

# Bibliotecas de ML
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score
import xgboost as xgb
from sklearn.ensemble import VotingClassifier

# Interpretabilidade
import shap

# Visualiza√ß√£o
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

class MalwareDetectionSystem:
    """
    Sistema de Detec√ß√£o de Malware Polim√≥rfico Controlado por LLM
    Implementa Random Forest otimizado com an√°lise em tempo real
    """
    
    def __init__(self, config_path=None):
        self.config = self._load_config(config_path)
        self.model = None
        self.tfidf_vectorizer = None
        self.pca = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_selector = None
        self.shap_explainer = None
        
        # Configura√ß√µes para detec√ß√£o em tempo real
        self.api_calls_buffer = defaultdict(list)
        self.detection_threshold = 0.7
        self.monitoring_active = False
        self.temporal_window = 60  # segundos
        
        # M√©tricas de performance
        self.detection_metrics = {
            'true_positives': 0,
            'false_positives': 0,
            'true_negatives': 0,
            'false_negatives': 0,
            'detection_times': []
        }
        
        self._setup_logging()
        
    def _load_config(self, config_path):
        """Carrega configura√ß√µes baseadas no framework te√≥rico"""
        default_config = {
            'random_forest': {
                'n_estimators': 300,  # Baseado na literatura: 100-500
                'max_depth': None,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'criterion': 'gini',
                'random_state': 42,
                'n_jobs': -1
            },
            'tfidf': {
                'max_features': 10000,
                'ngram_range': (1, 2),
                'min_df': 2,
                'max_df': 0.95
            },
            'pca': {
                'n_components': 0.95,  # Preservar 95% da vari√¢ncia
                'random_state': 42
            },
            'feature_selection': {
                'k_best': 2500  # 25-50% das caracter√≠sticas originais
            },
            'xgboost': {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'random_state': 42
            }
        }
        
        if config_path and Path(config_path).exists():
            with open(config_path, 'r') as f:
                user_config = json.load(f)
            default_config.update(user_config)
            
        return default_config
    
    def _setup_logging(self):
        """Configurar sistema de logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('malware_detection.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_malapi_dataset(self, dataset_path):
        """
        Carrega e processa o dataset MALAPI2019
        """
        self.logger.info("Carregando dataset MALAPI2019...")
        
        try:
            # Carregar dataset
            if dataset_path.endswith('.csv'):
                df = pd.read_csv(dataset_path)
            elif dataset_path.endswith('.json'):
                df = pd.read_json(dataset_path)
            else:
                raise ValueError("Formato de dataset n√£o suportado")
            
            self.logger.info(f"Dataset carregado: {df.shape}")
            
            # An√°lise explorat√≥ria
            self._exploratory_analysis(df)
            
            return df
            
        except Exception as e:
            self.logger.error(f"Erro ao carregar dataset: {e}")
            raise
    
    def _exploratory_analysis(self, df):
        """An√°lise explorat√≥ria do dataset"""
        self.logger.info("=== AN√ÅLISE EXPLORAT√ìRIA ===")
        self.logger.info(f"Formato do dataset: {df.shape}")
        self.logger.info(f"Colunas: {list(df.columns)}")
        
        if 'label' in df.columns or 'class' in df.columns:
            target_col = 'label' if 'label' in df.columns else 'class'
            self.logger.info(f"Distribui√ß√£o de classes:")
            self.logger.info(f"{df[target_col].value_counts()}")
        
        # Verificar valores ausentes
        missing_values = df.isnull().sum()
        if missing_values.any():
            self.logger.warning(f"Valores ausentes encontrados: {missing_values.sum()}")
    
    def preprocess_data(self, df, target_column='class'):
        """
        Pr√©-processamento completo baseado no framework te√≥rico
        """
        self.logger.info("Iniciando pr√©-processamento...")
        
        try:
            # Separar features e target
            if target_column not in df.columns:
                # Tentar encontrar coluna de target automaticamente
                possible_targets = ['class', 'label', 'malware_type', 'family']
                target_column = next((col for col in possible_targets if col in df.columns), None)
                
                if target_column is None:
                    raise ValueError("Coluna de target n√£o encontrada")
            
            y = df[target_column]
            X = df.drop(columns=[target_column])
            
            # Codificar labels
            y_encoded = self.label_encoder.fit_transform(y)
            
            # Processar chamadas de API
            if 'api_calls' in X.columns:
                # Assumindo que as chamadas de API est√£o em formato de texto/lista
                api_sequences = self._process_api_calls(X['api_calls'])
            else:
                # Se n√£o houver coluna espec√≠fica, usar todas as features num√©ricas
                api_sequences = X.select_dtypes(include=[np.number])
            
            # Aplicar TF-IDF para an√°lise de padr√µes de API
            if isinstance(api_sequences, pd.Series):
                X_tfidf = self._apply_tfidf(api_sequences)
            else:
                # Para features num√©ricas, normalizar
                X_tfidf = self.scaler.fit_transform(api_sequences)
            
            # Sele√ß√£o de caracter√≠sticas usando Mutual Information
            X_selected = self._feature_selection(X_tfidf, y_encoded)
            
            # Aplicar PCA se necess√°rio
            if X_selected.shape[1] > 1000:
                X_final = self._apply_pca(X_selected)
            else:
                X_final = X_selected
            
            self.logger.info(f"Pr√©-processamento conclu√≠do: {X_final.shape}")
            
            return X_final, y_encoded
            
        except Exception as e:
            self.logger.error(f"Erro no pr√©-processamento: {e}")
            raise
    
    def _process_api_calls(self, api_calls_series):
        """Processar sequ√™ncias de chamadas de API"""
        processed_calls = []
        
        for calls in api_calls_series:
            if isinstance(calls, str):
                # Se for string, assumir que s√£o chamadas separadas por v√≠rgula/espa√ßo
                call_sequence = ' '.join(calls.split())
            elif isinstance(calls, list):
                # Se for lista, juntar em string
                call_sequence = ' '.join(str(call) for call in calls)
            else:
                call_sequence = str(calls)
            
            processed_calls.append(call_sequence)
        
        return pd.Series(processed_calls)
    
    def _apply_tfidf(self, text_series):
        """Aplicar TF-IDF conforme framework te√≥rico"""
        self.logger.info("Aplicando TF-IDF...")
        
        self.tfidf_vectorizer = TfidfVectorizer(**self.config['tfidf'])
        X_tfidf = self.tfidf_vectorizer.fit_transform(text_series)
        
        return X_tfidf.toarray()
    
    def _feature_selection(self, X, y):
        """Sele√ß√£o de caracter√≠sticas usando Mutual Information"""
        self.logger.info("Aplicando sele√ß√£o de caracter√≠sticas...")
        
        k_best = min(self.config['feature_selection']['k_best'], X.shape[1])
        self.feature_selector = SelectKBest(score_func=mutual_info_classif, k=k_best)
        X_selected = self.feature_selector.fit_transform(X, y)
        
        self.logger.info(f"Caracter√≠sticas selecionadas: {X_selected.shape[1]}")
        
        return X_selected
    
    def _apply_pca(self, X):
        """Aplicar PCA para redu√ß√£o de dimensionalidade"""
        self.logger.info("Aplicando PCA...")
        
        self.pca = PCA(**self.config['pca'])
        X_pca = self.pca.fit_transform(X)
        
        explained_variance = sum(self.pca.explained_variance_ratio_)
        self.logger.info(f"Vari√¢ncia explicada pelo PCA: {explained_variance:.3f}")
        
        return X_pca
    
    def train_model(self, X, y, test_size=0.2, validation=True):
        """
        Treinar modelo Random Forest otimizado com ensemble
        """
        self.logger.info("Iniciando treinamento do modelo...")
        
        # Dividir dados
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # Configurar Random Forest
        rf_model = RandomForestClassifier(**self.config['random_forest'])
        
        # Configurar XGBoost para ensemble
        xgb_model = xgb.XGBClassifier(**self.config['xgboost'])
        
        # Criar ensemble com Voting Classifier
        self.model = VotingClassifier(
            estimators=[
                ('rf', rf_model),
                ('xgb', xgb_model)
            ],
            voting='soft'  # Usar probabilidades
        )
        
        # Treinar modelo
        self.logger.info("Treinando ensemble...")
        self.model.fit(X_train, y_train)
        
        # Valida√ß√£o
        if validation:
            self._validate_model(X_train, y_train, X_test, y_test)
        
        # Configurar SHAP para interpretabilidade
        self._setup_shap_explainer(X_train)
        
        self.logger.info("Treinamento conclu√≠do!")
        
        return self.model
    
    def _validate_model(self, X_train, y_train, X_test, y_test):
        """Valida√ß√£o abrangente do modelo"""
        self.logger.info("=== VALIDA√á√ÉO DO MODELO ===")
        
        # Predi√ß√µes
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)
        
        # M√©tricas b√°sicas
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
        
        self.logger.info(f"Accuracy: {accuracy:.4f}")
        self.logger.info(f"AUC: {auc:.4f}")
        
        # Relat√≥rio detalhado
        report = classification_report(y_test, y_pred, 
                                     target_names=self.label_encoder.classes_)
        self.logger.info(f"Relat√≥rio de Classifica√ß√£o:\n{report}")
        
        # Valida√ß√£o cruzada
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=5)
        self.logger.info(f"Cross-validation scores: {cv_scores}")
        self.logger.info(f"CV Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    def _setup_shap_explainer(self, X_train):
        """Configurar SHAP para interpretabilidade"""
        try:
            # Usar uma amostra menor para SHAP devido √† complexidade computacional
            sample_size = min(100, len(X_train))
            X_sample = X_train[:sample_size]
            
            self.shap_explainer = shap.Explainer(self.model.predict, X_sample)
            self.logger.info("SHAP explainer configurado")
        except Exception as e:
            self.logger.warning(f"N√£o foi poss√≠vel configurar SHAP: {e}")
    
    def explain_prediction(self, X_sample):
        """Explicar predi√ß√µes usando SHAP"""
        if self.shap_explainer is None:
            return "SHAP explainer n√£o dispon√≠vel"
        
        try:
            shap_values = self.shap_explainer(X_sample[:1])
            return shap_values
        except Exception as e:
            self.logger.error(f"Erro ao explicar predi√ß√£o: {e}")
            return None
    
    def predict_realtime(self, api_calls, process_info=None):
        """
        Predi√ß√£o em tempo real para detec√ß√£o de malware
        """
        if self.model is None:
            raise ValueError("Modelo n√£o foi treinado")
        
        try:
            # Processar chamadas de API
            processed_calls = self._process_single_api_sequence(api_calls)
            
            # Aplicar mesmo pr√©-processamento
            X_processed = self._preprocess_single_sample(processed_calls)
            
            # Predi√ß√£o
            prediction = self.model.predict([X_processed])[0]
            probability = self.model.predict_proba([X_processed])[0]
            
            # Converter predi√ß√£o para r√≥tulo original
            predicted_label = self.label_encoder.inverse_transform([prediction])[0]
            
            # Calcular confian√ßa
            confidence = np.max(probability)
            
            # Determinar se √© malware
            is_malware = confidence > self.detection_threshold and predicted_label != 'benign'
            
            result = {
                'prediction': predicted_label,
                'confidence': confidence,
                'is_malware': is_malware,
                'probabilities': dict(zip(self.label_encoder.classes_, probability)),
                'timestamp': datetime.now(),
                'process_info': process_info
            }
            
            if is_malware:
                self.logger.warning(f"MALWARE DETECTADO: {predicted_label} (confian√ßa: {confidence:.3f})")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erro na predi√ß√£o em tempo real: {e}")
            return None
    
    def _process_single_api_sequence(self, api_calls):
        """Processar uma √∫nica sequ√™ncia de API calls"""
        if isinstance(api_calls, list):
            return ' '.join(str(call) for call in api_calls)
        elif isinstance(api_calls, str):
            return api_calls
        else:
            return str(api_calls)
    
    def _preprocess_single_sample(self, api_sequence):
        """Pr√©-processar uma √∫nica amostra para predi√ß√£o"""
        # Aplicar TF-IDF
        if self.tfidf_vectorizer:
            X_tfidf = self.tfidf_vectorizer.transform([api_sequence]).toarray()
        else:
            # Se n√£o houver TF-IDF, assumir que √© num√©rico
            X_tfidf = np.array([api_sequence])
        
        # Sele√ß√£o de caracter√≠sticas
        if self.feature_selector:
            X_selected = self.feature_selector.transform(X_tfidf)
        else:
            X_selected = X_tfidf
        
        # PCA
        if self.pca:
            X_final = self.pca.transform(X_selected)
        else:
            X_final = X_selected
        
        return X_final[0]
    
    def start_realtime_monitoring(self):
        """Iniciar monitoramento em tempo real com Sysmon"""
        self.monitoring_active = True
        self.logger.info("Iniciando monitoramento em tempo real...")
        
        # Thread para monitorar eventos do Sysmon
        sysmon_thread = threading.Thread(target=self._monitor_sysmon_events)
        sysmon_thread.daemon = True
        sysmon_thread.start()
        
        # Thread para an√°lise peri√≥dica
        analysis_thread = threading.Thread(target=self._periodic_analysis)
        analysis_thread.daemon = True
        analysis_thread.start()
    
    def stop_realtime_monitoring(self):
        """Parar monitoramento em tempo real"""
        self.monitoring_active = False
        self.logger.info("Monitoramento em tempo real parado")
    
    def _monitor_sysmon_events(self):
        """Monitorar eventos do Sysmon"""
        try:
            # Conectar ao log do Sysmon
            hand = win32evtlog.OpenEventLog(None, "Microsoft-Windows-Sysmon/Operational")
            
            while self.monitoring_active:
                # Ler eventos recentes
                events = win32evtlog.ReadEventLog(
                    hand,
                    win32evtlog.EVENTLOG_BACKWARDS_READ | win32evtlog.EVENTLOG_SEQUENTIAL_READ,
                    0
                )
                
                for event in events:
                    if event.EventID in [1, 3, 7, 8, 10]:  # Process creation, Network, Image load, etc.
                        self._process_sysmon_event(event)
                
                time.sleep(1)  # Verificar a cada segundo
                
        except Exception as e:
            self.logger.error(f"Erro no monitoramento Sysmon: {e}")
    
    def _process_sysmon_event(self, event):
        """Processar evento individual do Sysmon"""
        try:
            # Extrair informa√ß√µes do evento
            event_data = self._parse_sysmon_event(event)
            
            if event_data:
                process_id = event_data.get('ProcessId')
                if process_id:
                    # Coletar chamadas de API para este processo
                    api_calls = self._collect_api_calls(process_id)
                    
                    if api_calls:
                        # Adicionar ao buffer para an√°lise
                        self.api_calls_buffer[process_id].extend(api_calls)
                        
        except Exception as e:
            self.logger.error(f"Erro ao processar evento Sysmon: {e}")
    
    def _parse_sysmon_event(self, event):
        """Parsear evento do Sysmon"""
        try:
            # Converter evento para dicion√°rio
            event_data = {}
            
            if hasattr(event, 'StringInserts'):
                # Extrair dados b√°sicos
                inserts = event.StringInserts
                if len(inserts) > 0:
                    event_data['ProcessId'] = inserts[0] if len(inserts) > 0 else None
                    event_data['Image'] = inserts[4] if len(inserts) > 4 else None
                    event_data['CommandLine'] = inserts[5] if len(inserts) > 5 else None
            
            return event_data
            
        except Exception as e:
            self.logger.error(f"Erro ao parsear evento: {e}")
            return None
    
    def _collect_api_calls(self, process_id):
        """Coletar chamadas de API para um processo espec√≠fico"""
        try:
            # Esta fun√ß√£o deveria integrar com uma ferramenta de API monitoring
            # Por simplicidade, vamos simular com informa√ß√µes do processo
            
            process = psutil.Process(int(process_id))
            
            # Coletar informa√ß√µes b√°sicas do processo
            api_calls = [
                f"CreateProcess:{process.name()}",
                f"OpenProcess:{process.pid}",
                f"GetProcAddress:{process.exe()}",
            ]
            
            # Adicionar informa√ß√µes de rede se dispon√≠vel
            try:
                connections = process.connections()
                for conn in connections:
                    api_calls.append(f"WSAConnect:{conn.raddr}")
            except:
                pass
            
            return api_calls
            
        except Exception as e:
            self.logger.debug(f"Erro ao coletar API calls para processo {process_id}: {e}")
            return []
    
    def _periodic_analysis(self):
        """An√°lise peri√≥dica dos dados coletados"""
        while self.monitoring_active:
            try:
                current_time = datetime.now()
                
                # Analisar processos no buffer
                for process_id, api_calls in list(self.api_calls_buffer.items()):
                    if len(api_calls) >= 10:  # M√≠nimo de API calls para an√°lise
                        # Realizar predi√ß√£o
                        result = self.predict_realtime(api_calls, {'pid': process_id})
                        
                        if result and result['is_malware']:
                            self._handle_malware_detection(process_id, result)
                        
                        # Limpar buffer antigo
                        self.api_calls_buffer[process_id] = api_calls[-50:]  # Manter √∫ltimas 50
                
                # Limpar buffers muito antigos
                self._cleanup_old_buffers()
                
                time.sleep(self.temporal_window)  # Analisar a cada minuto
                
            except Exception as e:
                self.logger.error(f"Erro na an√°lise peri√≥dica: {e}")
    
    def _handle_malware_detection(self, process_id, detection_result):
        """Lidar com detec√ß√£o de malware"""
        self.logger.critical(f"MALWARE DETECTADO - PID: {process_id}")
        self.logger.critical(f"Tipo: {detection_result['prediction']}")
        self.logger.critical(f"Confian√ßa: {detection_result['confidence']:.3f}")
        
        # Atualizar m√©tricas
        self.detection_metrics['true_positives'] += 1
        self.detection_metrics['detection_times'].append(datetime.now())
        
        # A√ß√µes adicionais (quarentena, alerta, etc.)
        self._quarantine_process(process_id)
    
    def _quarantine_process(self, process_id):
        """Colocar processo em quarentena"""
        try:
            process = psutil.Process(int(process_id))
            self.logger.warning(f"Terminando processo malicioso: {process.name()} (PID: {process_id})")
            process.terminate()
        except Exception as e:
            self.logger.error(f"Erro ao terminar processo {process_id}: {e}")
    
    def _cleanup_old_buffers(self):
        """Limpar buffers antigos"""
        # Remover entradas de processos que n√£o existem mais
        for process_id in list(self.api_calls_buffer.keys()):
            try:
                psutil.Process(int(process_id))
            except psutil.NoSuchProcess:
                del self.api_calls_buffer[process_id]
    
    def save_model(self, filepath):
        """Salvar modelo treinado"""
        model_data = {
            'model': self.model,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'pca': self.pca,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_selector': self.feature_selector,
            'config': self.config
        }
        
        joblib.dump(model_data, filepath)
        self.logger.info(f"Modelo salvo em: {filepath}")
    
    def load_model(self, filepath):
        """Carregar modelo treinado"""
        model_data = joblib.load(filepath)
        
        self.model = model_data['model']
        self.tfidf_vectorizer = model_data['tfidf_vectorizer']
        self.pca = model_data['pca']
        self.scaler = model_data['scaler']
        self.label_encoder = model_data['label_encoder']
        self.feature_selector = model_data['feature_selector']
        self.config = model_data['config']
        
        self.logger.info(f"Modelo carregado de: {filepath}")
    
    def get_performance_metrics(self):
        """Obter m√©tricas de performance"""
        metrics = self.detection_metrics.copy()
        
        if metrics['detection_times']:
            # Calcular tempo m√©dio de detec√ß√£o
            times = metrics['detection_times']
            if len(times) > 1:
                avg_detection_time = sum((times[i] - times[i-1]).total_seconds() 
                                       for i in range(1, len(times))) / (len(times) - 1)
                metrics['avg_detection_time_seconds'] = avg_detection_time
        
        # Calcular taxas
        total_detections = (metrics['true_positives'] + metrics['false_positives'] + 
                          metrics['true_negatives'] + metrics['false_negatives'])
        
        if total_detections > 0:
            metrics['accuracy'] = ((metrics['true_positives'] + metrics['true_negatives']) / 
                                 total_detections)
            metrics['precision'] = (metrics['true_positives'] / 
                                  (metrics['true_positives'] + metrics['false_positives'])
                                  if (metrics['true_positives'] + metrics['false_positives']) > 0 else 0)
            metrics['recall'] = (metrics['true_positives'] / 
                               (metrics['true_positives'] + metrics['false_negatives'])
                               if (metrics['true_positives'] + metrics['false_negatives']) > 0 else 0)
        
        return metrics

# Fun√ß√µes utilit√°rias para integra√ß√£o

def download_malapi_dataset():
    """Baixar dataset MALAPI2019 do GitHub"""
    import urllib.request
    
    url = "https://github.com/ocatak/malware_api_class/raw/main/data/malapi2019.csv"
    filename = "malapi2019.csv"
    
    try:
        urllib.request.urlretrieve(url, filename)
        print(f"Dataset baixado: {filename}")
        return filename
    except Exception as e:
        print(f"Erro ao baixar dataset: {e}")
        return None

def setup_sysmon():
    """Configurar Sysmon para coleta de dados"""
    print("Para configurar o Sysmon:")
    print("1. Baixe o Sysmon do Microsoft Sysinternals")
    print("2. Execute: sysmon64.exe -accepteula -i sysmonconfig.xml")
    print("3. Configure os eventos necess√°rios no XML")
    
    # Configura√ß√£o XML b√°sica para Sysmon
    sysmon_config = """<?xml version="1.0" encoding="UTF-8"?>
<Sysmon schemaversion="4.82">
  <EventFiltering>
    <ProcessCreate onmatch="include">
      <Image condition="end with">exe</Image>
    </ProcessCreate>
    <NetworkConnect onmatch="include">
      <DestinationPort condition="is">443</DestinationPort>
      <DestinationPort condition="is">80</DestinationPort>
    </NetworkConnect>
    <ImageLoaded onmatch="include">
      <ImageLoaded condition="end with">dll</ImageLoaded>
    </ImageLoaded>
  </EventFiltering>
</Sysmon>"""
    
    with open("sysmonconfig.xml", "w") as f:
        f.write(sysmon_config)
    
    print("Arquivo sysmonconfig.xml criado")

# Exemplo de uso principal
def main():
    """Exemplo de uso do sistema"""
    
    # Inicializar sistema
    detector = MalwareDetectionSystem()
    
    # Baixar dataset se necess√°rio
    dataset_path = "malapi2019.csv"
    if not Path(dataset_path).exists():
        dataset_path = download_malapi_dataset()
        if dataset_path is None:
            print("N√£o foi poss√≠vel baixar o dataset")
            return
    
    # Carregar e processar dados
    df = detector.load_malapi_dataset(dataset_path)
    X, y = detector.preprocess_data(df)
    
    # Treinar modelo
    model = detector.train_model(X, y)
    
    # Salvar modelo
    detector.save_model("malware_detector_model.joblib")
    
    # Iniciar monitoramento em tempo real
    print("Iniciando monitoramento em tempo real...")
    detector.start_realtime_monitoring()
    
    try:
        # Manter monitoramento ativo
        while True:
            time.sleep(60)
            metrics = detector.get_performance_metrics()
            print(f"Detec√ß√µes: TP={metrics['true_positives']}, FP={metrics['false_positives']}")
            
    except KeyboardInterrupt:
        print("Parando monitoramento...")
        detector.stop_realtime_monitoring()

if __name__ == "__main__":
    main()


# ========================================
# PIPELINE COMPLETO DE DESENVOLVIMENTO
# ========================================

class MalwareDetectionPipeline:
    """
    Pipeline completo para desenvolvimento e deploy do sistema
    """
    
    def __init__(self):
        self.detector = MalwareDetectionSystem()
        self.pipeline_steps = [
            "data_collection",
            "data_preprocessing", 
            "feature_engineering",
            "model_training",
            "model_validation",
            "model_optimization",
            "deployment_preparation",
            "real_time_deployment"
        ]
        
    def run_complete_pipeline(self, dataset_path=None, target_column='class'):
        """Executar pipeline completo"""
        
        print("="*60)
        print("PIPELINE DE DESENVOLVIMENTO - DETECTOR DE MALWARE POLIM√ìRFICO")
        print("="*60)
        
        # 1. COLETA DE DADOS
        print("\n1. COLETA DE DADOS")
        print("-" * 30)
        
        if dataset_path is None:
            print("Baixando dataset MALAPI2019...")
            dataset_path = download_malapi_dataset()
            if dataset_path is None:
                print("ERRO: N√£o foi poss√≠vel obter o dataset")
                return False
        
        # 2. CARREGAMENTO E AN√ÅLISE EXPLORAT√ìRIA
        print("\n2. AN√ÅLISE EXPLORAT√ìRIA")
        print("-" * 30)
        
        df = self.detector.load_malapi_dataset(dataset_path)
        self._detailed_eda(df)
        
        # 3. PR√â-PROCESSAMENTO
        print("\n3. PR√â-PROCESSAMENTO")
        print("-" * 30)
        
        X, y = self.detector.preprocess_data(df, target_column)
        
        # 4. ENGENHARIA DE CARACTER√çSTICAS
        print("\n4. ENGENHARIA DE CARACTER√çSTICAS")
        print("-" * 30)
        
        X_engineered = self._advanced_feature_engineering(X, y)
        
        # 5. TREINAMENTO DO MODELO
        print("\n5. TREINAMENTO DO MODELO")
        print("-" * 30)
        
        model = self.detector.train_model(X_engineered, y, validation=True)
        
        # 6. OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS
        print("\n6. OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS")
        print("-" * 30)
        
        optimized_model = self._hyperparameter_optimization(X_engineered, y)
        
        # 7. VALIDA√á√ÉO AVAN√áADA
        print("\n7. VALIDA√á√ÉO AVAN√áADA")
        print("-" * 30)
        
        self._advanced_validation(X_engineered, y, optimized_model)
        
        # 8. PREPARA√á√ÉO PARA DEPLOY
        print("\n8. PREPARA√á√ÉO PARA DEPLOY")
        print("-" * 30)
        
        self._prepare_deployment()
        
        # 9. TESTES DE INTEGRA√á√ÉO
        print("\n9. TESTES DE INTEGRA√á√ÉO")
        print("-" * 30)
        
        self._integration_tests()
        
        print("\n" + "="*60)
        print("PIPELINE CONCLU√çDO COM SUCESSO!")
        print("="*60)
        
        return True
    
    def _detailed_eda(self, df):
        """An√°lise explorat√≥ria detalhada"""
        
        # Estat√≠sticas b√°sicas
        print(f"Dimens√µes do dataset: {df.shape}")
        print(f"Mem√≥ria utilizada: {df.memory_usage().sum() / 1024**2:.2f} MB")
        
        # An√°lise de classes
        if 'class' in df.columns:
            class_dist = df['class'].value_counts()
            print(f"\nDistribui√ß√£o de classes:")
            for cls, count in class_dist.items():
                print(f"  {cls}: {count} ({count/len(df)*100:.2f}%)")
            
            # Verificar desbalanceamento
            imbalance_ratio = class_dist.max() / class_dist.min()
            if imbalance_ratio > 5:
                print(f"‚ö†Ô∏è  Dataset desbalanceado! Raz√£o: {imbalance_ratio:.2f}")
        
        # An√°lise de features
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        print(f"\nFeatures num√©ricas: {len(numeric_cols)}")
        
        categorical_cols = df.select_dtypes(include=['object']).columns
        print(f"Features categ√≥ricas: {len(categorical_cols)}")
        
        # Valores ausentes
        missing_analysis = df.isnull().sum()
        missing_cols = missing_analysis[missing_analysis > 0]
        if len(missing_cols) > 0:
            print(f"\nColunas com valores ausentes:")
            for col, missing_count in missing_cols.items():
                print(f"  {col}: {missing_count} ({missing_count/len(df)*100:.2f}%)")
    
    def _advanced_feature_engineering(self, X, y):
        """Engenharia de caracter√≠sticas avan√ßada"""
        
        # Adicionar caracter√≠sticas temporais se dispon√≠vel
        X_engineered = X.copy()
        
        # Caracter√≠sticas estat√≠sticas se for dados num√©ricos
        if isinstance(X, np.ndarray) and X.dtype in [np.float64, np.float32]:
            # Adicionar estat√≠sticas por linha
            X_stats = np.column_stack([
                np.mean(X, axis=1),      # M√©dia
                np.std(X, axis=1),       # Desvio padr√£o
                np.max(X, axis=1),       # M√°ximo
                np.min(X, axis=1),       # M√≠nimo
                np.median(X, axis=1),    # Mediana
            ])
            
            X_engineered = np.column_stack([X, X_stats])
            print(f"Caracter√≠sticas estat√≠sticas adicionadas: {X_stats.shape[1]}")
        
        # Detec√ß√£o de anomalias usando Isolation Forest
        from sklearn.ensemble import IsolationForest
        
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_scores = iso_forest.fit_transform(X_engineered)
        
        if isinstance(X_engineered, np.ndarray):
            X_engineered = np.column_stack([X_engineered, anomaly_scores])
        
        print(f"Score de anomalia adicionado")
        print(f"Dimens√µes finais: {X_engineered.shape}")
        
        return X_engineered
    
    def _hyperparameter_optimization(self, X, y):
        """Otimiza√ß√£o de hiperpar√¢metros usando GridSearch"""
        
        from sklearn.model_selection import GridSearchCV
        
        # Definir grid de par√¢metros
        param_grid = {
            'rf__n_estimators': [200, 300, 500],
            'rf__max_depth': [10, 20, None],
            'rf__min_samples_split': [2, 5, 10],
            'rf__min_samples_leaf': [1, 2, 4],
            'xgb__n_estimators': [100, 200, 300],
            'xgb__max_depth': [3, 6, 9],
            'xgb__learning_rate': [0.01, 0.1, 0.2]
        }
        
        # Grid search com valida√ß√£o cruzada
        grid_search = GridSearchCV(
            self.detector.model,
            param_grid,
            cv=5,
            scoring='f1_weighted',
            n_jobs=-1,
            verbose=1
        )
        
        print("Executando otimiza√ß√£o de hiperpar√¢metros...")
        grid_search.fit(X, y)
        
        print(f"Melhores par√¢metros: {grid_search.best_params_}")
        print(f"Melhor score: {grid_search.best_score_:.4f}")
        
        # Atualizar modelo do detector
        self.detector.model = grid_search.best_estimator_
        
        return grid_search.best_estimator_
    
    def _advanced_validation(self, X, y, model):
        """Valida√ß√£o avan√ßada com m√∫ltiplas m√©tricas"""
        
        from sklearn.model_selection import StratifiedKFold
        from sklearn.metrics import precision_recall_curve, roc_curve
        
        # Valida√ß√£o cruzada estratificada
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        metrics_summary = {
            'accuracy': [],
            'precision': [],
            'recall': [],
            'f1': [],
            'auc': []
        }
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            # Treinar no fold
            model.fit(X_train_fold, y_train_fold)
            
            # Predi√ß√µes
            y_pred = model.predict(X_val_fold)
            y_pred_proba = model.predict_proba(X_val_fold)
            
            # Calcular m√©tricas
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            
            acc = accuracy_score(y_val_fold, y_pred)
            prec = precision_score(y_val_fold, y_pred, average='weighted')
            rec = recall_score(y_val_fold, y_pred, average='weighted')
            f1 = f1_score(y_val_fold, y_pred, average='weighted')
            auc = roc_auc_score(y_val_fold, y_pred_proba, multi_class='ovr')
            
            metrics_summary['accuracy'].append(acc)
            metrics_summary['precision'].append(prec)
            metrics_summary['recall'].append(rec)
            metrics_summary['f1'].append(f1)
            metrics_summary['auc'].append(auc)
            
            print(f"Fold {fold+1}: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}")
        
        # Resumo final
        print("\n=== RESUMO DA VALIDA√á√ÉO ===")
        for metric, scores in metrics_summary.items():
            mean_score = np.mean(scores)
            std_score = np.std(scores)
            print(f"{metric.upper()}: {mean_score:.4f} (¬±{std_score:.4f})")
    
    def _prepare_deployment(self):
        """Preparar arquivos para deployment"""
        
        # Salvar modelo
        model_path = "production_model.joblib"
        self.detector.save_model(model_path)
        
        # Criar arquivo de configura√ß√£o
        config = {
            "model_path": model_path,
            "detection_threshold": 0.7,
            "temporal_window": 60,
            "log_level": "INFO",
            "sysmon_events": [1, 3, 7, 8, 10],
            "quarantine_enabled": True
        }
        
        with open("deployment_config.json", "w") as f:
            json.dump(config, f, indent=2)
        
        # Criar script de instala√ß√£o
        install_script = """#!/bin/bash
# Script de instala√ß√£o do detector de malware

echo "Instalando depend√™ncias..."
pip install -r requirements.txt

echo "Configurando Sysmon..."
# Baixar e configurar Sysmon aqui

echo "Criando servi√ßo systemd..."
# Configurar como servi√ßo do sistema

echo "Instala√ß√£o conclu√≠da!"
"""
        
        with open("install.sh", "w") as f:
            f.write(install_script)
        
        # Criar requirements.txt
        requirements = """pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
xgboost>=1.5.0
shap>=0.40.0
psutil>=5.8.0
pywin32>=227
matplotlib>=3.3.0
seaborn>=0.11.0
joblib>=1.1.0
"""
        
        with open("requirements.txt", "w") as f:
            f.write(requirements)
        
        print("Arquivos de deployment criados:")
        print("- production_model.joblib")
        print("- deployment_config.json") 
        print("- install.sh")
        print("- requirements.txt")
    
    def _integration_tests(self):
        """Testes de integra√ß√£o do sistema"""
        
        print("Executando testes de integra√ß√£o...")
        
        # Teste 1: Carregamento do modelo
        try:
            test_detector = MalwareDetectionSystem()
            test_detector.load_model("production_model.joblib")
            print("‚úì Carregamento do modelo: OK")
        except Exception as e:
            print(f"‚úó Carregamento do modelo: FALHOU ({e})")
        
        # Teste 2: Predi√ß√£o de exemplo
        try:
            sample_api_calls = ["CreateFileA", "WriteFile", "CreateProcess", "OpenProcess"]
            result = test_detector.predict_realtime(sample_api_calls)
            if result is not None:
                print("‚úì Predi√ß√£o em tempo real: OK")
            else:
                print("‚úó Predi√ß√£o em tempo real: FALHOU")
        except Exception as e:
            print(f"‚úó Predi√ß√£o em tempo real: FALHOU ({e})")
        
        # Teste 3: Verifica√ß√£o de depend√™ncias
        try:
            import psutil
            import win32evtlog
            print("‚úì Depend√™ncias do sistema: OK")
        except ImportError as e:
            print(f"‚úó Depend√™ncias do sistema: FALHOU ({e})")
        
        print("Testes de integra√ß√£o conclu√≠dos")


# ========================================
# COLETORES DE DADOS ADICIONAIS
# ========================================

class AdditionalDataCollector:
    """
    Coletor de dados adicionais para melhorar o treinamento
    """
    
    def __init__(self):
        self.collectors = {
            'process_behavior': self._collect_process_behavior,
            'network_patterns': self._collect_network_patterns,
            'file_operations': self._collect_file_operations,
            'registry_changes': self._collect_registry_changes,
            'memory_patterns': self._collect_memory_patterns
        }
    
    def collect_all_data(self, duration_minutes=60):
        """Coletar todos os tipos de dados por um per√≠odo"""
        
        print(f"Coletando dados por {duration_minutes} minutos...")
        
        collected_data = {
            'timestamp': datetime.now(),
            'duration_minutes': duration_minutes,
            'data': {}
        }
        
        end_time = datetime.now() + timedelta(minutes=duration_minutes)
        
        while datetime.now() < end_time:
            for data_type, collector_func in self.collectors.items():
                try:
                    data = collector_func()
                    if data_type not in collected_data['data']:
                        collected_data['data'][data_type] = []
                    collected_data['data'][data_type].append(data)
                except Exception as e:
                    print(f"Erro ao coletar {data_type}: {e}")
            
            time.sleep(10)  # Coletar a cada 10 segundos
        
        # Salvar dados coletados
        filename = f"additional_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w') as f:
            json.dump(collected_data, f, default=str, indent=2)
        
        print(f"Dados coletados salvos em: {filename}")
        return filename
    
    def _collect_process_behavior(self):
        """Coletar comportamento de processos"""
        processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent', 'create_time']):
            try:
                proc_info = proc.info
                proc_info['connections'] = len(proc.connections())
                proc_info['threads'] = proc.num_threads()
                processes.append(proc_info)
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        
        return {
            'timestamp': datetime.now(),
            'processes': processes[:50]  # Top 50 processos
        }
    
    def _collect_network_patterns(self):
        """Coletar padr√µes de rede"""
        connections = []
        
        for conn in psutil.net_connections():
            if conn.status == 'ESTABLISHED':
                connections.append({
                    'local_addr': f"{conn.laddr.ip}:{conn.laddr.port}" if conn.laddr else None,
                    'remote_addr': f"{conn.raddr.ip}:{conn.raddr.port}" if conn.raddr else None,
                    'pid': conn.pid
                })
        
        return {
            'timestamp': datetime.now(),
            'active_connections': connections,
            'network_stats': dict(psutil.net_io_counters()._asdict())
        }
    
    def _collect_file_operations(self):
        """Coletar opera√ß√µes de arquivo"""
        # Simula√ß√£o de coleta de opera√ß√µes de arquivo
        # Em implementa√ß√£o real, usaria APIs de monitoramento de arquivo
        
        return {
            'timestamp': datetime.now(),
            'disk_usage': dict(psutil.disk_usage('C:')._asdict()),
            'disk_io': dict(psutil.disk_io_counters()._asdict())
        }
    
    def _collect_registry_changes(self):
        """Coletar mudan√ßas no registro (Windows)"""
        # Placeholder para coleta de mudan√ßas no registro
        # Implementa√ß√£o completa requereria APIs espec√≠ficas do Windows
        
        return {
            'timestamp': datetime.now(),
            'registry_monitoring': 'placeholder_data'
        }
    
    def _collect_memory_patterns(self):
        """Coletar padr√µes de uso de mem√≥ria"""
        memory_info = psutil.virtual_memory()
        
        return {
            'timestamp': datetime.now(),
            'memory_total': memory_info.total,
            'memory_available': memory_info.available,
            'memory_percent': memory_info.percent,
            'memory_used': memory_info.used
        }


# ========================================
# EXEMPLO DE USO COMPLETO
# ========================================

def run_complete_system():
    """Executar sistema completo de detec√ß√£o"""
    
    print("SISTEMA DE DETEC√á√ÉO DE MALWARE POLIM√ìRFICO CONTROLADO POR LLM")
    print("=" * 70)
    
    # 1. Executar pipeline completo
    pipeline = MalwareDetectionPipeline()
    success = pipeline.run_complete_pipeline()
    
    if not success:
        print("ERRO: Pipeline falhou")
        return
    
    # 2. Coletar dados adicionais (opcional)
    print("\nDeseja coletar dados adicionais para melhorar o modelo? (y/n): ", end="")
    if input().lower() == 'y':
        collector = AdditionalDataCollector()
        collector.collect_all_data(duration_minutes=30)
    
    # 3. Configurar Sysmon
    print("\nConfigurando Sysmon...")
    setup_sysmon()
    
    # 4. Iniciar monitoramento
    print("\nIniciando sistema de monitoramento...")
    detector = MalwareDetectionSystem()
    detector.load_model("production_model.joblib")
    detector.start_realtime_monitoring()
    
    print("\nüõ°Ô∏è  SISTEMA ATIVO - Monitorando amea√ßas em tempo real")
    print("Pressione Ctrl+C para parar")
    
    try:
        while True:
            time.sleep(60)
            metrics = detector.get_performance_metrics()
            print(f"[{datetime.now().strftime('%H:%M:%S')}] Detec√ß√µes: {metrics['true_positives']}")
    except KeyboardInterrupt:
        detector.stop_realtime_monitoring()
        print("\nSistema parado pelo usu√°rio")

if __name__ == "__main__":
    run_complete_system()
            