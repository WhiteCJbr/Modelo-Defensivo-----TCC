{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd8697e",
   "metadata": {},
   "source": [
    "# Modelo Defensivo com Random Forest (Colab) — Foco em Spyware\n",
    "\n",
    "Treinamento de um classificador para detectar Spyware (positiva) versus Benigno (negativa) usando o MALAPI2019 (all_analysis_data.txt + labels.csv, filtrando apenas a família Spyware) e, opcionalmente, dados benignos coletados localmente.\n",
    "\n",
    "- Base científica: framework teórico (Random Forest + n-grams/TF‑IDF; balanceamento; validação cruzada).\n",
    "- Direção: voltamos à abordagem inicial (apenas Spyware), evitando outras famílias que vinham induzindo overfitting na análise binária geral.\n",
    "- Saídas: métricas (Accuracy, F1, ROC‑AUC), matriz de confusão, validação cruzada e artefatos (.joblib)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0803bf",
   "metadata": {},
   "source": [
    "## Como fornecer os dados no Google Colab\n",
    "\n",
    "Você pode usar um dos caminhos abaixo (edite as variáveis no próximo bloco):\n",
    "\n",
    "1) Upload direto (recomendado para início):\n",
    "   - Faça upload de `all_analysis_data.txt` e `labels.csv` (MALAPI2019). Usaremos APENAS as amostras rotuladas como `Spyware`.\n",
    "   - Faça upload de um CSV benigno (ex.: `benign_api_dataset_*.csv`) para compor a classe negativa (Benign).\n",
    "\n",
    "2) Google Drive:\n",
    "   - Monte o Drive e aponte `MALAPI_DIR` para a pasta que contém os arquivos.\n",
    "   - Defina `BENIGN_CSV_PATH` para incluir benignos adicionais.\n",
    "\n",
    "Observação: mapeamos `Spyware` -> 1 (malicioso) e dados benignos -> 0. Amostras de outras famílias do MALAPI serão descartadas para este treino focado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar ambiente Colab e preparar instalações opcionais\n",
    "import sys, os, math, json, random\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print('IN_COLAB =', IN_COLAB)\n",
    "\n",
    "# Instalações leves (apenas se necessário)\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        import sklearn, pandas, numpy  # noqa: F401\n",
    "    except Exception:\n",
    "        %pip -q install scikit-learn pandas numpy\n",
    "    try:\n",
    "        import imblearn  # noqa: F401\n",
    "    except Exception:\n",
    "        %pip -q install imbalanced-learn\n",
    "    try:\n",
    "        import seaborn  # noqa: F401\n",
    "    except Exception:\n",
    "        %pip -q install seaborn\n",
    "    try:\n",
    "        import joblib  # noqa: F401\n",
    "    except Exception:\n",
    "        %pip -q install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04317029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações do experimento (edite conforme seu caso)\n",
    "RANDOM_STATE = 42\n",
    "DATA_SOURCE = 'upload'  # 'upload' ou 'drive'\n",
    "\n",
    "# Se usar Google Drive, defina a pasta dos dados (contendo 'all_analysis_data.txt' e 'labels.csv')\n",
    "MALAPI_DIR = ''  # ex.: '/content/drive/MyDrive/Modelo_Defensivo_TCC/mal-api-2019'\n",
    "MALAPI_ALL_PATH = ''  # se quiser apontar o arquivo diretamente\n",
    "MALAPI_LABELS_PATH = ''  # se quiser apontar o arquivo diretamente\n",
    "\n",
    "# (Obrigatório) caminho para CSV de benignos coletados para classe negativa.\n",
    "BENIGN_CSV_PATH = ''  # ex.: '/content/drive/MyDrive/benign_api_dataset_2025XXXX.csv'\n",
    "\n",
    "# Hiperparâmetros e toggles (foco em generalização)\n",
    "MAX_TFIDF_FEATURES = 15000\n",
    "NGRAM_RANGE = (1, 2)\n",
    "MIN_DF = 2\n",
    "MAX_DF = 0.98\n",
    "USE_FEATURE_SELECTION = True\n",
    "K_BEST = 4000  # reduz dimensionalidade antes do RF\n",
    "DO_HYPERPARAM_TUNING = False  # deixe False para treinos rápidos\n",
    "TEST_SIZE = 0.25\n",
    "N_JOBS = -1\n",
    "\n",
    "# Parâmetros base do Random Forest (um pouco mais conservadores para reduzir overfitting)\n",
    "RF_PARAMS_BASE = dict(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    min_samples_split=6,\n",
    "    min_samples_leaf=3,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    class_weight='balanced_subsample',\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=RANDOM_STATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilitários: carregamento e preparação dos dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "def _read_lines(filepath: str) -> List[str]:\n",
    "    lines = []\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def load_malapi_dataset_spyware(all_path: str, labels_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lê MALAPI2019 e mantém apenas amostras com label 'Spyware'.\n",
    "    Retorna DataFrame: ['api_calls', 'family'] (somente Spyware)\n",
    "    \"\"\"\n",
    "    api_lines = _read_lines(all_path)\n",
    "    labels = _read_lines(labels_path)\n",
    "    if len(api_lines) != len(labels):\n",
    "        raise ValueError(f'Inconsistência: {len(api_lines)} linhas em all_analysis_data.txt vs {len(labels)} em labels.csv')\n",
    "\n",
    "    def normalize_seq(s: str) -> str:\n",
    "        s = s.replace(',', ' ')\n",
    "        return ' '.join(s.split())\n",
    "\n",
    "    api_calls = [normalize_seq(s) for s in api_lines]\n",
    "    df_all = pd.DataFrame({'api_calls': api_calls, 'family': labels})\n",
    "    df_spy = df_all[df_all['family'].str.strip().str.lower() == 'spyware'].copy()\n",
    "    if df_spy.empty:\n",
    "        raise ValueError('Nenhuma amostra Spyware encontrada em MALAPI2019.')\n",
    "    return df_spy.reset_index(drop=True)\n",
    "\n",
    "def load_benign_csv(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lê CSV benigno com coluna 'api_calls'. Outras colunas são ignoradas para padronizar.\n",
    "    Retorna DataFrame com colunas: ['api_calls']\n",
    "    \"\"\"\n",
    "    dfb = pd.read_csv(csv_path)\n",
    "    if 'api_calls' not in dfb.columns:\n",
    "        possible = [c for c in dfb.columns if 'api' in c.lower() and 'call' in c.lower()]\n",
    "        if not possible:\n",
    "            raise ValueError('CSV benigno não possui coluna api_calls')\n",
    "        dfb = dfb.rename(columns={possible[0]: 'api_calls'})\n",
    "    dfb['api_calls'] = dfb['api_calls'].astype(str).fillna('')\n",
    "    dfb['api_calls'] = dfb['api_calls'].str.replace(',', ' ', regex=False).str.replace('\\n', ' ', regex=False)\n",
    "    dfb['api_calls'] = dfb['api_calls'].apply(lambda s: ' '.join(s.split()))\n",
    "    dfb = dfb[['api_calls']].copy()\n",
    "    return dfb\n",
    "\n",
    "def build_spyware_vs_benign_dataset(spy_df: pd.DataFrame, benign_df: pd.DataFrame, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constrói dataset binário Spyware (1) vs Benign (0).\n",
    "    - Remove duplicatas e sequências vazias\n",
    "    - Balanceia levemente por downsampling da classe majoritária (até 1.5x da minoritária)\n",
    "    \"\"\"\n",
    "    spy = spy_df[['api_calls']].copy()\n",
    "    spy['label_bin'] = 1\n",
    "    ben = benign_df[['api_calls']].copy()\n",
    "    ben['label_bin'] = 0\n",
    "\n",
    "    data = pd.concat([spy, ben], axis=0, ignore_index=True)\n",
    "    data = data.dropna(subset=['api_calls'])\n",
    "    data = data[data['api_calls'].str.len() > 0]\n",
    "    data = data.drop_duplicates(subset=['api_calls', 'label_bin'])\n",
    "\n",
    "    # Balanceamento leve por downsampling\n",
    "    n_spy = (data['label_bin'] == 1).sum()\n",
    "    n_ben = (data['label_bin'] == 0).sum()\n",
    "    if n_spy == 0 or n_ben == 0:\n",
    "        raise ValueError('É necessário ter exemplos de ambas as classes (Spyware e Benign).')\n",
    "    maj_label = 1 if n_spy > n_ben else 0\n",
    "    min_label = 1 - maj_label\n",
    "    n_min = min(n_spy, n_ben)\n",
    "    n_maj_cap = int(1.5 * n_min)\n",
    "\n",
    "    df_min = data[data['label_bin'] == min_label]\n",
    "    df_maj = data[data['label_bin'] == maj_label]\n",
    "    if len(df_maj) > n_maj_cap:\n",
    "        df_maj = df_maj.sample(n=n_maj_cap, random_state=random_state)\n",
    "    data_bal = pd.concat([df_min, df_maj], axis=0, ignore_index=True)\n",
    "\n",
    "    return data_bal.sample(frac=1.0, random_state=random_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrada dos dados (upload ou drive)\n",
    "malapi_all, malapi_labels, benign_csv = None, None, None\n",
    "\n",
    "if IN_COLAB and DATA_SOURCE == 'upload':\n",
    "    from google.colab import files\n",
    "    print('Selecione: all_analysis_data.txt, labels.csv e (opcional) benign_csv...')\n",
    "    up = files.upload()  # abre seletor\n",
    "    # mapear nomes comuns\n",
    "    for k in up.keys():\n",
    "        lk = k.lower()\n",
    "        if 'all_analysis_data' in lk and lk.endswith('.txt'):\n",
    "            malapi_all = k\n",
    "        elif 'labels' in lk and lk.endswith('.csv'):\n",
    "            malapi_labels = k\n",
    "        elif 'benign' in lk and lk.endswith('.csv'):\n",
    "            benign_csv = k\n",
    "    print('Detectado:', malapi_all, malapi_labels, benign_csv)\n",
    "\n",
    "elif IN_COLAB and DATA_SOURCE == 'drive':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    if MALAPI_ALL_PATH and MALAPI_LABELS_PATH:\n",
    "        malapi_all, malapi_labels = MALAPI_ALL_PATH, MALAPI_LABELS_PATH\n",
    "    elif MALAPI_DIR:\n",
    "        malapi_all = str(Path(MALAPI_DIR) / 'all_analysis_data.txt')\n",
    "        malapi_labels = str(Path(MALAPI_DIR) / 'labels.csv')\n",
    "    else:\n",
    "        raise ValueError('Defina MALAPI_DIR ou os caminhos MALAPI_ALL_PATH/MALAPI_LABELS_PATH')\n",
    "    if BENIGN_CSV_PATH:\n",
    "        benign_csv = BENIGN_CSV_PATH\n",
    "else:\n",
    "    # Ambiente local (fora do Colab): ajuste caminhos conforme necessário\n",
    "    # Exemplo (comente/edite se executar localmente com os arquivos ao lado do notebook):\n",
    "    malapi_all = 'all_analysis_data.txt'\n",
    "    malapi_labels = 'labels.csv'\n",
    "    # benign_csv = 'benign_api_dataset_YYYYMMDD.csv'  # opcional\n",
    "\n",
    "print('Paths finais:')\n",
    "print('malapi_all     =', malapi_all)\n",
    "print('malapi_labels  =', malapi_labels)\n",
    "print('benign_csv     =', benign_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7426c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar datasets (Spyware de MALAPI + Benign CSV) e unificar\n",
    "spy_df = load_malapi_dataset_spyware(malapi_all, malapi_labels)\n",
    "print('Spyware (MALAPI) ->', spy_df.shape)\n",
    "\n",
    "if not BENIGN_CSV_PATH and not benign_csv:\n",
    "    print('ATENÇÃO: Forneça um CSV de benignos (BENIGN_CSV_PATH ou upload) para formar a classe negativa!')\n",
    "\n",
    "ben_df = None\n",
    "if benign_csv:\n",
    "    ben_df = load_benign_csv(benign_csv)\n",
    "elif BENIGN_CSV_PATH:\n",
    "    ben_df = load_benign_csv(BENIGN_CSV_PATH)\n",
    "\n",
    "if ben_df is None or ben_df.empty:\n",
    "    raise ValueError('Benign CSV ausente ou vazio. É necessário para treinar Spyware vs Benign.')\n",
    "print('Benign ->', ben_df.shape)\n",
    "\n",
    "data = build_spyware_vs_benign_dataset(spy_df, ben_df, random_state=RANDOM_STATE)\n",
    "print('Dataset final (Spyware vs Benign) ->', data.shape)\n",
    "data['label_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd57f8a",
   "metadata": {},
   "source": [
    "## Vetorização e divisão estratificada (Spyware vs Benign)\n",
    "Usamos TF‑IDF com n-grams para capturar padrões de sequência em chamadas de API. Em seguida, dividimos em treino/teste de forma estratificada para avaliar generalização no cenário Spyware (1) vs Benign (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50020fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_text = data['api_calls'].values\n",
    "y = data['label_bin'].values\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print('Treino/Teste:', len(X_train_text), len(X_test_text))\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=MAX_TFIDF_FEATURES,\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    min_df=MIN_DF,\n",
    "    max_df=MAX_DF,\n",
    "    token_pattern=r'[^\\s]+'  # tokens por separação de espaços\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "X_train_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) seleção de características para reduzir dimensionalidade e ruído\n",
    "from scipy import sparse\n",
    "Xtr, Xte = X_train_tfidf, X_test_tfidf\n",
    "selector = None\n",
    "if USE_FEATURE_SELECTION:\n",
    "    k = min(K_BEST, X_train_tfidf.shape[1] - 1) if X_train_tfidf.shape[1] > 1 else 1\n",
    "    selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    # Para MI, precisamos de arrays densos quando necessário; tentamos conversão \n",
    "    # Apenas converte um subconjunto para estimar MI se memória estiver limitada (heurística simples)\n",
    "    try:\n",
    "        Xtr_dense = X_train_tfidf.toarray() if sparse.issparse(X_train_tfidf) else X_train_tfidf\n",
    "    except MemoryError:\n",
    "        # fallback: reduzir features antes de MI, p.ex. pelos termos mais frequentes\n",
    "        print('Memória insuficiente para densificar. Reduzindo max_features pela metade e refazendo TF-IDF...')\n",
    "        MAX_TFIDF_FEATURES = max(5000, MAX_TFIDF_FEATURES // 2)\n",
    "        tfidf = TfidfVectorizer(\n",
    "            max_features=MAX_TFIDF_FEATURES, ngram_range=NGRAM_RANGE, min_df=MIN_DF, max_df=MAX_DF, token_pattern=r'[^\\s]+'\n",
    "        )\n",
    "        X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "        X_test_tfidf = tfidf.transform(X_test_text)\n",
    "        Xtr_dense = X_train_tfidf.toarray()\n",
    "    Xtr_sel = selector.fit_transform(Xtr_dense, y_train)\n",
    "    # aplicar ao teste\n",
    "    Xte_dense = X_test_tfidf.toarray() if sparse.issparse(X_test_tfidf) else X_test_tfidf\n",
    "    Xte_sel = selector.transform(Xte_dense)\n",
    "    Xtr, Xte = Xtr_sel, Xte_sel\n",
    "    print('Seleção k-best ->', Xtr.shape)\n",
    "else:\n",
    "    print('Sem seleção de características.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25834e9e",
   "metadata": {},
   "source": [
    "## Treinamento do Random Forest e validação (Spyware vs Benign)\n",
    "Usamos parâmetros conservadores e `class_weight` para mitigar desbalanceamento. Validamos com holdout e CV estratificado, monitorando F1 da classe positiva (Spyware)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "rf = RandomForestClassifier(**RF_PARAMS_BASE)\n",
    "rf.fit(Xtr, y_train)\n",
    "\n",
    "y_pred = rf.predict(Xte)\n",
    "y_proba = None\n",
    "try:\n",
    "    y_proba = rf.predict_proba(Xte)[:, 1]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba) if y_proba is not None else float('nan')\n",
    "\n",
    "print(json.dumps({\n",
    "    'accuracy': acc, 'f1': f1, 'precision': prec, 'recall': rec, 'roc_auc': auc\n",
    "}, indent=2))\n",
    "\n",
    "print('Classification report:\\n', classification_report(y_test, y_pred, target_names=['Benign', 'Malware']))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(4.5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign','Malware'], yticklabels=['Benign','Malware'])\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.show()\n",
    "\n",
    "# Validação cruzada rápida (F1) no treino\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_scores = cross_val_score(rf, Xtr, y_train, cv=cv, scoring='f1', n_jobs=N_JOBS)\n",
    "print('CV F1 (5-fold):', cv_scores, ' | média=%.4f +/- %.4f' % (cv_scores.mean(), cv_scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dc7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) Busca de hiperparâmetros leve com RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "if DO_HYPERPARAM_TUNING:\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(200, 600),\n",
    "        'max_depth': [None] + list(range(6, 21, 2)),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    base = RandomForestClassifier(**{**RF_PARAMS_BASE, 'n_jobs': N_JOBS})\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    rs = RandomizedSearchCV(base, param_distributions=param_dist, n_iter=20,\n",
    "                           scoring='f1', n_jobs=N_JOBS, cv=cv, verbose=1,\n",
    "                           random_state=RANDOM_STATE)\n",
    "    rs.fit(Xtr, y_train)\n",
    "    print('Melhores params:', rs.best_params_)\n",
    "    rf = rs.best_estimator_\n",
    "    y_pred = rf.predict(Xte)\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = rf.predict_proba(Xte)[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    acc = accuracy_score(y_test, y_pred); f1 = f1_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred); rec = recall_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else float('nan')\n",
    "    print(json.dumps({'accuracy': acc, 'f1': f1, 'precision': prec, 'recall': rec, 'roc_auc': auc}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ef0d4",
   "metadata": {},
   "source": [
    "## Palavras/ngrams mais importantes\n",
    "Mapeamos as importâncias do Random Forest para os termos do TF‑IDF. Útil para análise e entendimento dos sinais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top_features(rf_model, tfidf_vectorizer, top_k=30):\n",
    "    feats = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "    importances = rf_model.feature_importances_\n",
    "    idx = np.argsort(importances)[::-1][:top_k]\n",
    "    return pd.DataFrame({'term': feats[idx], 'importance': importances[idx]})\n",
    "\n",
    "# Se houve seleção de características, o mapeamento fica inconsistente.\n",
    "# Neste caso, mostramos aviso e pulamos.\n",
    "if selector is None:\n",
    "    tf_top = top_features(rf, tfidf, 30)\n",
    "    print('Top termos por importância (foco no classificador Spyware=1):')\n",
    "    print(tf_top.head(30).to_string(index=False))\n",
    "else:\n",
    "    print('Aviso: seleção de características ativa — importâncias diretas no espaço TF-IDF ficam desalinhadas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc0486",
   "metadata": {},
   "source": [
    "## Salvar artefatos do modelo\n",
    "Salvamos o modelo Random Forest, o vetorizar TF‑IDF e (se houver) o seletor de características para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, os\n",
    "outdir = '/content/models' if IN_COLAB else './models'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "model_path = os.path.join(outdir, 'rf_malware_detector.joblib')\n",
    "tfidf_path = os.path.join(outdir, 'tfidf_vectorizer.joblib')\n",
    "selector_path = os.path.join(outdir, 'feature_selector.joblib')\n",
    "info_path = os.path.join(outdir, 'training_info.json')\n",
    "\n",
    "joblib.dump(rf, model_path)\n",
    "joblib.dump(tfidf, tfidf_path)\n",
    "if selector is not None:\n",
    "    joblib.dump(selector, selector_path)\n",
    "\n",
    "info = {\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'test_size': TEST_SIZE,\n",
    "    'n_samples_total': int(len(data)),\n",
    "    'n_samples_train': int(len(X_train_text)),\n",
    "    'n_samples_test': int(len(X_test_text)),\n",
    "    'class_balance': {\n",
    "        'train': {\n",
    "            'benign': int((y_train==0).sum()),\n",
    "            'malware': int((y_train==1).sum())\n",
    "        },\n",
    "        'test': {\n",
    "            'benign': int((y_test==0).sum()),\n",
    "            'malware': int((y_test==1).sum())\n",
    "        }\n",
    "    },\n",
    "    'metrics': {\n",
    "        'accuracy': float(acc), 'f1': float(f1), 'precision': float(prec), 'recall': float(rec), 'roc_auc': float(auc) if not math.isnan(auc) else None\n",
    "    },\n",
    "    'tfidf': {\n",
    "        'max_features': MAX_TFIDF_FEATURES, 'ngram_range': NGRAM_RANGE, 'min_df': MIN_DF, 'max_df': MAX_DF\n",
    "    },\n",
    "    'feature_selection': {\n",
    "        'enabled': bool(selector is not None), 'k_best': K_BEST if selector is not None else None\n",
    "    },\n",
    "    'rf_params': RF_PARAMS_BASE\n",
    "}\n",
    "with open(info_path, 'w') as f:\n",
    "    json.dump(info, f, indent=2)\n",
    "\n",
    "print('Artefatos salvos em:', outdir)\n",
    "[model_path, tfidf_path, selector_path if selector is not None else None, info_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc989bce",
   "metadata": {},
   "source": [
    "## Notas e alinhamento com tentativas anteriores\n",
    "- Voltamos ao foco original: detectar exclusivamente Spyware, usando amostras `Spyware` do MALAPI como classe positiva e dados coletados (CSV) como classe negativa.\n",
    "- Para reduzir overfitting: n-grams moderados, seleção opcional de características (K-best), leve balanceamento por downsampling e `class_weight` no RF.\n",
    "- Se ainda observar overfitting, experimente: reduzir `MAX_TFIDF_FEATURES`, diminuir `K_BEST`, aumentar `min_samples_leaf` ou ativar `DO_HYPERPARAM_TUNING` para buscar parâmetros mais conservadores."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
