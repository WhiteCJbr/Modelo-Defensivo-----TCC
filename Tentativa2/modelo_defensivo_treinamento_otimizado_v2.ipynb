{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b988120",
   "metadata": {},
   "source": [
    "# Sistema de Detec√ß√£o de Malware Polim√≥rfico Controlado por LLM\n",
    "# Treinamento do Modelo Defensivo - VERS√ÉO OTIMIZADA\n",
    "\n",
    "Este notebook cont√©m o treinamento **otimizado** do modelo de detec√ß√£o de malware keylogger polim√≥rfico baseado no framework te√≥rico-pr√°tico com Random Forest e MALAPI2019.\n",
    "\n",
    "**Melhorias Implementadas:**\n",
    "- ‚úÖ Pr√©-processamento aprimorado para dados pequenos\n",
    "- ‚úÖ Balanceamento inteligente do dataset\n",
    "- ‚úÖ Feature engineering avan√ßada\n",
    "- ‚úÖ Otimiza√ß√£o de hiperpar√¢metros\n",
    "- ‚úÖ Valida√ß√£o robusta\n",
    "\n",
    "**Objetivo:** Treinar um modelo capaz de identificar malware keylogger polim√≥rfico com m√©tricas superiores a 80% de accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9360702",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas e Configura√ß√£o Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36992a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o de depend√™ncias espec√≠ficas para Google Colab\n",
    "!pip install xgboost\n",
    "!pip install shap\n",
    "!pip install joblib\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "# Importa√ß√µes principais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Bibliotecas de ML\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Balanceamento de dados\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Interpretabilidade\n",
    "import shap\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17419aa1",
   "metadata": {},
   "source": [
    "## 2. Classe Otimizada do Sistema de Detec√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900648bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedMalwareDetectionSystem:\n",
    "    \"\"\"\n",
    "    Sistema Otimizado de Detec√ß√£o de Malware Polim√≥rfico\n",
    "    Implementa m√∫ltiplas melhorias para datasets pequenos\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path=None):\n",
    "        self.config = self._load_optimized_config(config_path)\n",
    "        self.model = None\n",
    "        self.vectorizer = None\n",
    "        self.dimensionality_reducer = None\n",
    "        self.scaler = RobustScaler()  # Mais robusto para outliers\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.feature_selector = None\n",
    "        self.shap_explainer = None\n",
    "        self.balancer = None\n",
    "        \n",
    "        # M√©tricas detalhadas\n",
    "        self.training_metrics = {}\n",
    "        \n",
    "        self._setup_logging()\n",
    "\n",
    "    def _load_optimized_config(self, config_path):\n",
    "        \"\"\"Configura√ß√µes otimizadas para datasets pequenos\"\"\"\n",
    "        optimized_config = {\n",
    "            'vectorization': {\n",
    "                'method': 'tfidf',  # 'tfidf', 'count', 'hybrid'\n",
    "                'max_features': 5000,  # Reduzido para dataset pequeno\n",
    "                'ngram_range': (1, 3),  # Incluir trigramas\n",
    "                'min_df': 1,  # Mais permissivo para dataset pequeno\n",
    "                'max_df': 0.9,\n",
    "                'analyzer': 'word'\n",
    "            },\n",
    "            'feature_selection': {\n",
    "                'method': 'mutual_info',  # 'mutual_info', 'chi2', 'model_based'\n",
    "                'k_best': 500,  # Reduzido significativamente\n",
    "                'threshold': 'median'\n",
    "            },\n",
    "            'dimensionality_reduction': {\n",
    "                'method': 'pca',  # 'pca', 'svd', 'none'\n",
    "                'n_components': 0.90,  # 90% da vari√¢ncia\n",
    "                'random_state': 42\n",
    "            },\n",
    "            'balancing': {\n",
    "                'method': 'smote',  # 'smote', 'random_over', 'smoteenn'\n",
    "                'random_state': 42,\n",
    "                'k_neighbors': 3  # Reduzido para dataset pequeno\n",
    "            },\n",
    "            'random_forest': {\n",
    "                'n_estimators': 200,  # Reduzido para evitar overfitting\n",
    "                'max_depth': 10,      # Limitado para dataset pequeno\n",
    "                'min_samples_split': 10,  # Aumentado para evitar overfitting\n",
    "                'min_samples_leaf': 5,    # Aumentado para evitar overfitting\n",
    "                'criterion': 'gini',\n",
    "                'max_features': 'sqrt',   # Mais conservativo\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'class_weight': 'balanced'  # Importante para dados desbalanceados\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'n_estimators': 150,\n",
    "                'max_depth': 4,       # Reduzido para evitar overfitting\n",
    "                'learning_rate': 0.05,  # Mais conservativo\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'reg_alpha': 0.1,     # Regulariza√ß√£o L1\n",
    "                'reg_lambda': 0.1,    # Regulariza√ß√£o L2\n",
    "                'random_state': 42,\n",
    "                'scale_pos_weight': 1  # Ser√° ajustado baseado no balanceamento\n",
    "            },\n",
    "            'logistic_regression': {\n",
    "                'C': 1.0,\n",
    "                'penalty': 'l2',\n",
    "                'solver': 'liblinear',\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if config_path and Path(config_path).exists():\n",
    "            with open(config_path, 'r') as f:\n",
    "                user_config = json.load(f)\n",
    "            optimized_config.update(user_config)\n",
    "            \n",
    "        return optimized_config\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configurar sistema de logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Classe OptimizedMalwareDetectionSystem definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59573c00",
   "metadata": {},
   "source": [
    "## 3. M√©todos de Carregamento e An√°lise Otimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926194a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_dataset(self, dataset_path):\n",
    "    \"\"\"\n",
    "    Carregamento e an√°lise otimizada do dataset\n",
    "    \"\"\"\n",
    "    self.logger.info(\"Carregando dataset MALAPI2019...\")\n",
    "    \n",
    "    try:\n",
    "        # Carregar dataset com otimiza√ß√µes\n",
    "        if dataset_path.endswith('.csv'):\n",
    "            df = pd.read_csv(dataset_path, low_memory=False)\n",
    "        elif dataset_path.endswith('.txt'):\n",
    "            # Tentar diferentes delimitadores\n",
    "            try:\n",
    "                df = pd.read_csv(dataset_path, delimiter='\\t', header=0, low_memory=False)\n",
    "            except:\n",
    "                df = pd.read_csv(dataset_path, delimiter=',', header=0, low_memory=False)\n",
    "        else:\n",
    "            raise ValueError(\"Formato de dataset n√£o suportado\")\n",
    "        \n",
    "        self.logger.info(f\"Dataset carregado: {df.shape}\")\n",
    "        \n",
    "        # An√°lise explorat√≥ria detalhada\n",
    "        self._comprehensive_eda(df)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Erro ao carregar dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def _comprehensive_eda(self, df):\n",
    "    \"\"\"An√°lise explorat√≥ria abrangente\"\"\"\n",
    "    self.logger.info(\"=== AN√ÅLISE EXPLORAT√ìRIA DETALHADA ===\")\n",
    "    self.logger.info(f\"Dimens√µes: {df.shape}\")\n",
    "    self.logger.info(f\"Mem√≥ria: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    self.logger.info(f\"Colunas: {list(df.columns)}\")\n",
    "    \n",
    "    # An√°lise de tipos de dados\n",
    "    self.logger.info(f\"Tipos de dados:\")\n",
    "    for dtype in df.dtypes.unique():\n",
    "        cols = df.select_dtypes(include=[dtype]).columns\n",
    "        self.logger.info(f\"  {dtype}: {len(cols)} colunas\")\n",
    "    \n",
    "    # An√°lise de valores ausentes\n",
    "    missing_total = df.isnull().sum().sum()\n",
    "    if missing_total > 0:\n",
    "        self.logger.warning(f\"Total de valores ausentes: {missing_total}\")\n",
    "        missing_cols = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "        for col, count in missing_cols.items():\n",
    "            self.logger.warning(f\"  {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # An√°lise de duplicatas\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        self.logger.warning(f\"Linhas duplicadas: {duplicates}\")\n",
    "\n",
    "# Adicionar m√©todos √† classe\n",
    "OptimizedMalwareDetectionSystem.load_and_analyze_dataset = load_and_analyze_dataset\n",
    "OptimizedMalwareDetectionSystem._comprehensive_eda = _comprehensive_eda\n",
    "\n",
    "print(\"‚úÖ M√©todos de carregamento otimizados adicionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bc9ee",
   "metadata": {},
   "source": [
    "## 4. Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee18432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para os arquivos\n",
    "dataset_filename = '/content/drive/MyDrive/IFSP/all_analysis_data.txt'\n",
    "labels_filename = '/content/drive/MyDrive/IFSP/labels.csv'\n",
    "\n",
    "# Inicializar o sistema otimizado\n",
    "detector = OptimizedMalwareDetectionSystem()\n",
    "\n",
    "# Carregar dataset principal\n",
    "df = detector.load_and_analyze_dataset(dataset_filename)\n",
    "\n",
    "print(f\"üìä Dataset carregado: {df.shape}\")\n",
    "print(f\"üìã Colunas: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d91a09f",
   "metadata": {},
   "source": [
    "## 4.1. Coleta de Dados Benignos Reais (Opcional)\n",
    "\n",
    "**IMPORTANTE:** Esta se√ß√£o √© opcional e deve ser executada localmente no Windows 11 para coletar dados benignos reais.\n",
    "\n",
    "### Instru√ß√µes para Coleta Local:\n",
    "\n",
    "1. **Execute o coletor simplificado** (n√£o requer Sysmon):\n",
    "   ```python\n",
    "   # No seu computador Windows 11, execute:\n",
    "   python simple_benign_collector.py\n",
    "   ```\n",
    "\n",
    "2. **Ou execute o coletor completo** (requer Sysmon):\n",
    "   ```python\n",
    "   # Para coleta mais detalhada com Sysmon:\n",
    "   python benign_data_collector.py\n",
    "   ```\n",
    "\n",
    "3. **Upload do arquivo gerado:**\n",
    "   - Fa√ßa upload do arquivo CSV gerado para o Google Drive\n",
    "   - Atualize o caminho abaixo com o arquivo real\n",
    "\n",
    "### Configura√ß√£o do Arquivo de Dados Benignos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b42de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o para dados benignos reais\n",
    "# Defina o caminho para o arquivo de dados benignos coletados localmente\n",
    "# Se n√£o dispon√≠vel, ser√° usado dataset sint√©tico\n",
    "\n",
    "USE_REAL_BENIGN_DATA = False  # Altere para True se voc√™ coletou dados reais\n",
    "\n",
    "# Caminho para o arquivo de dados benignos reais (atualize conforme necess√°rio)\n",
    "benign_data_filename = '/content/drive/MyDrive/IFSP/benign_api_dataset_YYYYMMDD_HHMMSS.csv'\n",
    "\n",
    "# Verificar se arquivo existe\n",
    "if USE_REAL_BENIGN_DATA:\n",
    "    if Path(benign_data_filename).exists():\n",
    "        print(\"‚úÖ Arquivo de dados benignos reais encontrado!\")\n",
    "        print(f\"üìÅ Caminho: {benign_data_filename}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Arquivo de dados benignos n√£o encontrado!\")\n",
    "        print(\"üîÑ Usando gera√ß√£o sint√©tica como fallback\")\n",
    "        USE_REAL_BENIGN_DATA = False\n",
    "else:\n",
    "    print(\"üìä Usando gera√ß√£o sint√©tica de dados benignos\")\n",
    "\n",
    "print(f\"üéØ Modo selecionado: {'Dados Reais' if USE_REAL_BENIGN_DATA else 'Dados Sint√©ticos'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_benign_data(num_samples=1000):\n",
    "    \"\"\"\n",
    "    Gerador melhorado de dados benignos sint√©ticos baseado em padr√µes reais\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Gerando dados benignos sint√©ticos real√≠sticos...\")\n",
    "    \n",
    "    # Padr√µes de API mais real√≠sticos para aplicativos benignos\n",
    "    benign_patterns = {\n",
    "        'text_editor': [\n",
    "            'CreateFileW ReadFile GetFileSize SetFilePointer WriteFile FlushFileBuffers CloseHandle',\n",
    "            'CreateFileW WriteFile SetEndOfFile CloseHandle GetLastError',\n",
    "            'FindFirstFileW FindNextFileW GetFileAttributesW FindClose',\n",
    "            'CreateDirectoryW CreateFileW WriteFile CloseHandle',\n",
    "            'RegOpenKeyW RegQueryValueW RegSetValueW RegCloseKey',\n",
    "            'LoadLibraryW GetProcAddress GetModuleHandleW FreeLibrary',\n",
    "            'CreateThread WaitForSingleObject CloseHandle GetCurrentThread',\n",
    "            'VirtualAlloc VirtualProtect VirtualFree GetProcessHeap'\n",
    "        ],\n",
    "        'web_browser': [\n",
    "            'WSAStartup WSASocket connect send recv closesocket WSACleanup',\n",
    "            'InternetOpenW InternetConnectW HttpOpenRequestW HttpSendRequestW HttpQueryInfoW InternetCloseHandle',\n",
    "            'CreateFileW WriteFile ReadFile SetFilePointer CloseHandle DeleteFileW',\n",
    "            'RegOpenKeyW RegSetValueW RegQueryValueW RegDeleteValueW RegCloseKey',\n",
    "            'VirtualAlloc VirtualProtect VirtualFree HeapAlloc HeapFree',\n",
    "            'CreateThread CreateEvent SetEvent WaitForSingleObject CloseHandle',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary GetModuleFileNameW',\n",
    "            'CreateProcessW OpenProcess GetProcessImageFileNameW TerminateProcess'\n",
    "        ],\n",
    "        'office_app': [\n",
    "            'CreateFileW ReadFile WriteFile SetFilePointer GetFileSize FlushFileBuffers CloseHandle',\n",
    "            'CreateDirectoryW CopyFileW MoveFileW DeleteFileW GetFileAttributesW',\n",
    "            'RegOpenKeyW RegEnumKeyW RegEnumValueW RegSetValueW RegDeleteKeyW RegCloseKey',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary GetModuleHandleW',\n",
    "            'CreateThread CreateMutexW ReleaseMutex WaitForSingleObject CloseHandle',\n",
    "            'VirtualAlloc VirtualProtect VirtualFree GlobalAlloc GlobalFree',\n",
    "            'CreateEvent SetEvent ResetEvent WaitForMultipleObjects',\n",
    "            'GetSystemInfo GetVersionExW GetComputerNameW GetUserNameW'\n",
    "        ],\n",
    "        'system_tool': [\n",
    "            'CreateProcessW OpenProcess GetProcessImageFileNameW WaitForSingleObject TerminateProcess CloseHandle',\n",
    "            'GetSystemInfo GetVersionExW GetComputerNameW GetUserNameW GetSystemDirectoryW',\n",
    "            'RegOpenKeyW RegEnumKeyW RegQueryValueW RegCloseKey RegConnectRegistryW',\n",
    "            'FindFirstFileW FindNextFileW GetFileAttributesW GetFileInformationByHandle FindClose',\n",
    "            'CreateFileW ReadFile GetFileSize SetFilePointer CloseHandle',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary GetModuleFileNameW',\n",
    "            'CreateThread GetCurrentThread GetThreadId SetThreadPriority',\n",
    "            'VirtualQueryEx VirtualAllocEx VirtualFreeEx ReadProcessMemory'\n",
    "        ],\n",
    "        'media_player': [\n",
    "            'CreateFileW ReadFile SetFilePointer GetFileSize CloseHandle',\n",
    "            'DirectSoundCreate CreateSoundBuffer Play Stop GetCurrentPosition',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary GetModuleHandleW',\n",
    "            'CreateThread SetThreadPriority WaitForSingleObject ResumeThread SuspendThread',\n",
    "            'VirtualAlloc VirtualProtect VirtualFree HeapAlloc HeapReAlloc HeapFree',\n",
    "            'CreateEvent SetEvent ResetEvent WaitForMultipleObjects',\n",
    "            'RegOpenKeyW RegQueryValueW RegSetValueW RegCloseKey',\n",
    "            'GetSystemMetrics GetDeviceCaps CreateCompatibleDC DeleteDC'\n",
    "        ],\n",
    "        'file_manager': [\n",
    "            'FindFirstFileW FindNextFileW GetFileAttributesW GetFileInformationByHandle FindClose',\n",
    "            'CreateDirectoryW RemoveDirectoryW CopyFileW MoveFileW DeleteFileW',\n",
    "            'SHGetFolderPathW SHBrowseForFolderW SHGetPathFromIDListW SHFileOperationW',\n",
    "            'RegOpenKeyW RegEnumKeyW RegEnumValueW RegQueryValueW RegCloseKey',\n",
    "            'CreateFileW GetFileSize GetFileTime SetFileTime CloseHandle',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary Shell32.dll',\n",
    "            'CreateThread PostMessage SendMessage GetMessage DispatchMessage',\n",
    "            'CreateWindow ShowWindow UpdateWindow SetWindowPos GetWindowRect'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Distribui√ß√£o mais real√≠stica por categoria\n",
    "    category_weights = {\n",
    "        'text_editor': 0.15,\n",
    "        'web_browser': 0.35,\n",
    "        'office_app': 0.20,\n",
    "        'system_tool': 0.10,\n",
    "        'media_player': 0.10,\n",
    "        'file_manager': 0.10\n",
    "    }\n",
    "    \n",
    "    for category, weight in category_weights.items():\n",
    "        category_samples = int(num_samples * weight)\n",
    "        patterns = benign_patterns[category]\n",
    "        \n",
    "        for i in range(category_samples):\n",
    "            # Escolher padr√£o base\n",
    "            base_pattern = np.random.choice(patterns)\n",
    "            \n",
    "            # Adicionar varia√ß√µes real√≠sticas\n",
    "            variation_chance = np.random.random()\n",
    "            \n",
    "            if variation_chance > 0.7:\n",
    "                # Adicionar APIs de erro/tratamento\n",
    "                error_apis = ['GetLastError', 'SetLastError', 'FormatMessageW']\n",
    "                base_pattern += ' ' + np.random.choice(error_apis)\n",
    "            elif variation_chance > 0.5:\n",
    "                # Adicionar APIs de limpeza\n",
    "                cleanup_apis = ['CloseHandle', 'FreeLibrary', 'DeleteObject']\n",
    "                base_pattern += ' ' + np.random.choice(cleanup_apis)\n",
    "            elif variation_chance > 0.3:\n",
    "                # Adicionar APIs de informa√ß√£o do sistema\n",
    "                info_apis = ['GetTickCount', 'GetSystemTime', 'QueryPerformanceCounter']\n",
    "                base_pattern += ' ' + np.random.choice(info_apis)\n",
    "            \n",
    "            # Simular timestamp real√≠stico (√∫ltimas 2 semanas)\n",
    "            hours_ago = np.random.exponential(24)  # Distribui√ß√£o exponencial\n",
    "            timestamp = datetime.now() - timedelta(hours=min(hours_ago, 336))  # Max 2 semanas\n",
    "            \n",
    "            # Simular caracter√≠sticas real√≠sticas\n",
    "            record = {\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'app_category': category,\n",
    "                'process_name': f\"{category}_app.exe\",\n",
    "                'api_calls': base_pattern,\n",
    "                'process_id': np.random.randint(1000, 32767),\n",
    "                'memory_usage': np.random.normal(\n",
    "                    {'text_editor': 25000000, 'web_browser': 150000000, 'office_app': 80000000,\n",
    "                     'system_tool': 15000000, 'media_player': 60000000, 'file_manager': 40000000}[category],\n",
    "                    10000000\n",
    "                ),\n",
    "                'label': 'Benign'\n",
    "            }\n",
    "            \n",
    "            synthetic_data.append(record)\n",
    "    \n",
    "    # Converter para DataFrame\n",
    "    df_benign = pd.DataFrame(synthetic_data)\n",
    "    \n",
    "    # Adicionar variabilidade temporal\n",
    "    df_benign['timestamp'] = pd.to_datetime(df_benign['timestamp'])\n",
    "    df_benign = df_benign.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Gerados {len(df_benign)} registros benignos sint√©ticos\")\n",
    "    print(f\"üìä Distribui√ß√£o por categoria:\")\n",
    "    for category, count in df_benign['app_category'].value_counts().items():\n",
    "        print(f\"   {category}: {count} amostras ({count/len(df_benign)*100:.1f}%)\")\n",
    "    \n",
    "    return df_benign\n",
    "\n",
    "# Gerar dados benignos\n",
    "if USE_REAL_BENIGN_DATA:\n",
    "    print(\"üìÇ Carregando dados benignos reais...\")\n",
    "    df_benign = pd.read_csv(benign_data_filename)\n",
    "    print(f\"‚úÖ Carregados {len(df_benign)} registros reais\")\n",
    "else:\n",
    "    df_benign = generate_realistic_benign_data(num_samples=1500)\n",
    "\n",
    "print(f\"üéØ Dataset benigno final: {df_benign.shape}\")\n",
    "print(f\"üìã Colunas dispon√≠veis: {list(df_benign.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db5075",
   "metadata": {},
   "source": [
    "## 5. Filtragem e Prepara√ß√£o Inteligente de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e77e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_data_preparation_v2(detector, df, labels_filename, df_benign=None):\n",
    "    \"\"\"\n",
    "    Prepara√ß√£o inteligente dos dados com dados benignos reais/sint√©ticos\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Iniciando prepara√ß√£o inteligente dos dados V2...\")\n",
    "    \n",
    "    # Carregar labels\n",
    "    labels_df = pd.read_csv(labels_filename, header=None, names=['label'])\n",
    "    print(f\"üìä Labels carregadas: {labels_df.shape}\")\n",
    "    \n",
    "    # An√°lise da distribui√ß√£o de labels\n",
    "    print(f\"üìà Distribui√ß√£o original de labels:\")\n",
    "    label_counts = labels_df['label'].value_counts()\n",
    "    print(label_counts)\n",
    "    \n",
    "    # Ajustar tamanhos se necess√°rio\n",
    "    min_size = min(len(df), len(labels_df))\n",
    "    if len(df) != len(labels_df):\n",
    "        print(f\"‚ö†Ô∏è Ajustando para tamanho comum: {min_size}\")\n",
    "        df = df.iloc[:min_size].copy()\n",
    "        labels_df = labels_df.iloc[:min_size].copy()\n",
    "    \n",
    "    # Adicionar labels\n",
    "    df_with_labels = df.copy()\n",
    "    df_with_labels['malware_type'] = labels_df['label']\n",
    "    \n",
    "    print(\"\\nüéØ ESTRAT√âGIA DE PREPARA√á√ÉO COM DADOS BENIGNOS REAIS:\")\n",
    "    \n",
    "    # Separar apenas Spyware do dataset original\n",
    "    spyware_data = df_with_labels[df_with_labels['malware_type'] == 'Spyware'].copy()\n",
    "    print(f\"üïµÔ∏è Dados Spyware encontrados: {len(spyware_data)}\")\n",
    "    \n",
    "    # Usar dados benignos reais/sint√©ticos\n",
    "    if df_benign is not None:\n",
    "        print(f\"‚úÖ Usando dados benignos fornecidos: {len(df_benign)} amostras\")\n",
    "        \n",
    "        # Padronizar formato dos dados benignos\n",
    "        benign_data = df_benign.copy()\n",
    "        \n",
    "        # Garantir que temos a coluna de API calls no formato correto\n",
    "        if 'api_calls' in benign_data.columns:\n",
    "            # Renomear para coincidir com o formato do dataset original\n",
    "            api_column = df.columns[0]  # Primeira coluna do dataset original\n",
    "            benign_data = benign_data.rename(columns={'api_calls': api_column})\n",
    "        \n",
    "        # Adicionar outras colunas se necess√°rio (preencher com dados padr√£o)\n",
    "        for col in df.columns:\n",
    "            if col not in benign_data.columns:\n",
    "                benign_data[col] = ''  # Preencher com string vazia ou valor padr√£o\n",
    "        \n",
    "        # Manter apenas as colunas relevantes\n",
    "        benign_data = benign_data[df.columns]\n",
    "        \n",
    "        # Marcar como benigno\n",
    "        benign_data['malware_type'] = 'Benign'\n",
    "        benign_data['binary_class'] = 'Benign'\n",
    "        \n",
    "        # Balancear com Spyware\n",
    "        target_size = len(spyware_data)\n",
    "        if len(benign_data) > target_size:\n",
    "            benign_data = benign_data.sample(n=target_size, random_state=42)\n",
    "            print(f\"üîÑ Balanceado para {target_size} amostras benignas\")\n",
    "        elif len(benign_data) < target_size:\n",
    "            # Se temos menos dados benignos, usar todos e balancear Spyware\n",
    "            target_size = len(benign_data)\n",
    "            spyware_data = spyware_data.sample(n=target_size, random_state=42)\n",
    "            print(f\"üîÑ Balanceado Spyware para {target_size} amostras\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nenhum dado benigno fornecido - usando estrat√©gia anterior\")\n",
    "        # Fallback para estrat√©gia anterior\n",
    "        benign_types = ['Trojan', 'Backdoor']\n",
    "        benign_candidates = df_with_labels[df_with_labels['malware_type'].isin(benign_types)]\n",
    "        target_size = len(spyware_data)\n",
    "        \n",
    "        if len(benign_candidates) >= target_size:\n",
    "            benign_data = benign_candidates.groupby('malware_type').apply(\n",
    "                lambda x: x.sample(min(len(x), target_size // len(benign_types)), \n",
    "                                  random_state=42)\n",
    "            ).reset_index(drop=True)\n",
    "            benign_data = benign_data.sample(n=target_size, random_state=42)\n",
    "        else:\n",
    "            benign_data = benign_candidates.copy()\n",
    "            \n",
    "        benign_data['binary_class'] = 'Benign'\n",
    "    \n",
    "    # Marcar Spyware\n",
    "    spyware_data['binary_class'] = 'Spyware'\n",
    "    \n",
    "    # Combinar dados\n",
    "    final_df = pd.concat([spyware_data, benign_data], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nüìä Dataset final balanceado:\")\n",
    "    print(f\"  üïµÔ∏è Spyware: {len(spyware_data)} amostras\")\n",
    "    print(f\"  ‚úÖ Benign: {len(benign_data)} amostras\")\n",
    "    print(f\"  üìà Total: {len(final_df)} amostras\")\n",
    "    print(f\"  ‚öñÔ∏è Balanceamento: {len(spyware_data)/len(final_df)*100:.1f}% Spyware\")\n",
    "    \n",
    "    # Verificar qualidade dos dados\n",
    "    print(f\"\\nüîç AN√ÅLISE DE QUALIDADE:\")\n",
    "    print(f\"üìä Distribui√ß√£o bin√°ria:\")\n",
    "    print(final_df['binary_class'].value_counts())\n",
    "    \n",
    "    if 'malware_type' in final_df.columns:\n",
    "        print(f\"üìä Composi√ß√£o detalhada:\")\n",
    "        composition = final_df.groupby(['binary_class', 'malware_type']).size()\n",
    "        print(composition)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Executar prepara√ß√£o inteligente V2\n",
    "df_prepared_v2 = intelligent_data_preparation_v2(detector, df, labels_filename, df_benign)\n",
    "\n",
    "print(f\"\\n‚úÖ Prepara√ß√£o V2 conclu√≠da! Dataset final: {df_prepared_v2.shape}\")\n",
    "\n",
    "# Comparar com vers√£o anterior\n",
    "print(f\"\\nüìà COMPARA√á√ÉO COM PREPARA√á√ÉO ANTERIOR:\")\n",
    "print(f\"Dataset V1: {df_prepared.shape}\")\n",
    "print(f\"Dataset V2: {df_prepared_v2.shape}\")\n",
    "print(f\"Melhoria: {len(df_prepared_v2) - len(df_prepared):+d} amostras ({(len(df_prepared_v2)/len(df_prepared)-1)*100:+.1f}%)\")\n",
    "\n",
    "# Usar a vers√£o V2 para o resto do processamento\n",
    "df_prepared = df_prepared_v2.copy()\n",
    "print(f\"üéØ Usando Dataset V2 para treinamento: {df_prepared.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930009b4",
   "metadata": {},
   "source": [
    "## 6. Pr√©-processamento Avan√ßado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_preprocessing(self, df, target_column='binary_class'):\n",
    "    \"\"\"\n",
    "    Pr√©-processamento avan√ßado com m√∫ltiplas melhorias\n",
    "    \"\"\"\n",
    "    self.logger.info(\"Iniciando pr√©-processamento avan√ßado...\")\n",
    "    \n",
    "    try:\n",
    "        # Separar features e target\n",
    "        y = df[target_column]\n",
    "        X = df.drop(columns=[target_column, 'malware_type'], errors='ignore')\n",
    "        \n",
    "        # Codificar labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        self.logger.info(f\"Classes: {self.label_encoder.classes_}\")\n",
    "        \n",
    "        # Identificar tipo de dados nas features\n",
    "        if X.shape[1] == 1:\n",
    "            # Assumir que √© coluna de API calls\n",
    "            api_column = X.columns[0]\n",
    "            self.logger.info(f\"Processando coluna de API calls: {api_column}\")\n",
    "            X_processed = self._advanced_text_processing(X[api_column])\n",
    "        else:\n",
    "            # M√∫ltiplas colunas - verificar tipos\n",
    "            text_cols = X.select_dtypes(include=['object']).columns\n",
    "            numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "            \n",
    "            if len(text_cols) > 0:\n",
    "                self.logger.info(f\"Processando {len(text_cols)} colunas de texto\")\n",
    "                X_processed = self._advanced_text_processing(X[text_cols[0]])\n",
    "            else:\n",
    "                self.logger.info(f\"Processando {len(numeric_cols)} colunas num√©ricas\")\n",
    "                X_processed = self._advanced_numeric_processing(X[numeric_cols])\n",
    "        \n",
    "        # Feature engineering\n",
    "        X_engineered = self._feature_engineering(X_processed, y_encoded)\n",
    "        \n",
    "        # Sele√ß√£o de caracter√≠sticas\n",
    "        X_selected = self._intelligent_feature_selection(X_engineered, y_encoded)\n",
    "        \n",
    "        # Redu√ß√£o de dimensionalidade (se necess√°rio)\n",
    "        X_reduced = self._intelligent_dimensionality_reduction(X_selected)\n",
    "        \n",
    "        # Balanceamento de dados\n",
    "        X_balanced, y_balanced = self._intelligent_balancing(X_reduced, y_encoded)\n",
    "        \n",
    "        self.logger.info(f\"Pr√©-processamento conclu√≠do: {X_balanced.shape}\")\n",
    "        \n",
    "        return X_balanced, y_balanced\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Erro no pr√©-processamento: {e}\")\n",
    "        raise\n",
    "\n",
    "def _advanced_text_processing(self, text_series):\n",
    "    \"\"\"Processamento avan√ßado de texto\"\"\"\n",
    "    self.logger.info(\"Aplicando processamento avan√ßado de texto...\")\n",
    "    \n",
    "    # Limpar e processar texto\n",
    "    processed_texts = []\n",
    "    for text in text_series:\n",
    "        if pd.isna(text):\n",
    "            processed_texts.append(\"\")\n",
    "        else:\n",
    "            # Converter para string e limpar\n",
    "            clean_text = str(text).strip()\n",
    "            # Remover caracteres especiais excessivos mas manter estrutura\n",
    "            clean_text = ' '.join(clean_text.split())\n",
    "            processed_texts.append(clean_text)\n",
    "    \n",
    "    # Escolher m√©todo de vetoriza√ß√£o\n",
    "    vectorization_method = self.config['vectorization']['method']\n",
    "    \n",
    "    if vectorization_method == 'tfidf':\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=self.config['vectorization']['max_features'],\n",
    "            ngram_range=self.config['vectorization']['ngram_range'],\n",
    "            min_df=self.config['vectorization']['min_df'],\n",
    "            max_df=self.config['vectorization']['max_df'],\n",
    "            analyzer=self.config['vectorization']['analyzer']\n",
    "        )\n",
    "    elif vectorization_method == 'count':\n",
    "        self.vectorizer = CountVectorizer(\n",
    "            max_features=self.config['vectorization']['max_features'],\n",
    "            ngram_range=self.config['vectorization']['ngram_range'],\n",
    "            min_df=self.config['vectorization']['min_df'],\n",
    "            max_df=self.config['vectorization']['max_df']\n",
    "        )\n",
    "    \n",
    "    X_vectorized = self.vectorizer.fit_transform(processed_texts)\n",
    "    self.logger.info(f\"Texto vetorizado: {X_vectorized.shape}\")\n",
    "    \n",
    "    return X_vectorized.toarray()\n",
    "\n",
    "def _advanced_numeric_processing(self, numeric_data):\n",
    "    \"\"\"Processamento avan√ßado de dados num√©ricos\"\"\"\n",
    "    self.logger.info(\"Aplicando processamento num√©rico avan√ßado...\")\n",
    "    \n",
    "    # Remover colunas com vari√¢ncia zero\n",
    "    numeric_data = numeric_data.loc[:, numeric_data.var() > 0]\n",
    "    \n",
    "    # Normaliza√ß√£o robusta\n",
    "    X_scaled = self.scaler.fit_transform(numeric_data)\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "def _feature_engineering(self, X, y):\n",
    "    \"\"\"Feature engineering avan√ßada\"\"\"\n",
    "    self.logger.info(\"Aplicando feature engineering...\")\n",
    "    \n",
    "    # Adicionar estat√≠sticas por amostra\n",
    "    X_stats = np.column_stack([\n",
    "        np.mean(X, axis=1),     # M√©dia\n",
    "        np.std(X, axis=1),      # Desvio padr√£o\n",
    "        np.max(X, axis=1),      # M√°ximo\n",
    "        np.min(X, axis=1),      # M√≠nimo\n",
    "        np.sum(X > 0, axis=1),  # Contagem de valores positivos\n",
    "    ])\n",
    "    \n",
    "    # Combinar features originais com estat√≠sticas\n",
    "    X_engineered = np.column_stack([X, X_stats])\n",
    "    \n",
    "    self.logger.info(f\"Features ap√≥s engineering: {X_engineered.shape}\")\n",
    "    \n",
    "    return X_engineered\n",
    "\n",
    "def _intelligent_feature_selection(self, X, y):\n",
    "    \"\"\"Sele√ß√£o inteligente de caracter√≠sticas\"\"\"\n",
    "    self.logger.info(\"Aplicando sele√ß√£o inteligente de caracter√≠sticas...\")\n",
    "    \n",
    "    method = self.config['feature_selection']['method']\n",
    "    k_best = min(self.config['feature_selection']['k_best'], X.shape[1])\n",
    "    \n",
    "    if method == 'mutual_info':\n",
    "        self.feature_selector = SelectKBest(score_func=mutual_info_classif, k=k_best)\n",
    "    elif method == 'chi2':\n",
    "        # Garantir valores n√£o-negativos para chi2\n",
    "        X = np.maximum(X, 0)\n",
    "        self.feature_selector = SelectKBest(score_func=chi2, k=k_best)\n",
    "    elif method == 'model_based':\n",
    "        # Usar ExtraTreesClassifier para sele√ß√£o baseada em modelo\n",
    "        model = ExtraTreesClassifier(n_estimators=50, random_state=42)\n",
    "        self.feature_selector = SelectFromModel(model, threshold='median')\n",
    "    \n",
    "    X_selected = self.feature_selector.fit_transform(X, y)\n",
    "    \n",
    "    self.logger.info(f\"Caracter√≠sticas selecionadas: {X_selected.shape}\")\n",
    "    \n",
    "    return X_selected\n",
    "\n",
    "def _intelligent_dimensionality_reduction(self, X):\n",
    "    \"\"\"Redu√ß√£o inteligente de dimensionalidade\"\"\"\n",
    "    reduction_method = self.config['dimensionality_reduction']['method']\n",
    "    \n",
    "    if reduction_method == 'none' or X.shape[1] <= 100:\n",
    "        return X\n",
    "    \n",
    "    self.logger.info(\"Aplicando redu√ß√£o de dimensionalidade...\")\n",
    "    \n",
    "    if reduction_method == 'pca':\n",
    "        self.dimensionality_reducer = PCA(\n",
    "            n_components=self.config['dimensionality_reduction']['n_components'],\n",
    "            random_state=self.config['dimensionality_reduction']['random_state']\n",
    "        )\n",
    "    elif reduction_method == 'svd':\n",
    "        n_components = min(50, X.shape[1] - 1)  # Limite para SVD\n",
    "        self.dimensionality_reducer = TruncatedSVD(\n",
    "            n_components=n_components,\n",
    "            random_state=self.config['dimensionality_reduction']['random_state']\n",
    "        )\n",
    "    \n",
    "    X_reduced = self.dimensionality_reducer.fit_transform(X)\n",
    "    \n",
    "    if hasattr(self.dimensionality_reducer, 'explained_variance_ratio_'):\n",
    "        variance_explained = sum(self.dimensionality_reducer.explained_variance_ratio_)\n",
    "        self.logger.info(f\"Vari√¢ncia explicada: {variance_explained:.3f}\")\n",
    "    \n",
    "    self.logger.info(f\"Dimensionalidade reduzida: {X_reduced.shape}\")\n",
    "    \n",
    "    return X_reduced\n",
    "\n",
    "def _intelligent_balancing(self, X, y):\n",
    "    \"\"\"Balanceamento inteligente de dados\"\"\"\n",
    "    self.logger.info(\"Aplicando balanceamento inteligente...\")\n",
    "    \n",
    "    # Verificar balanceamento atual\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    balance_ratio = min(counts) / max(counts)\n",
    "    \n",
    "    self.logger.info(f\"Balanceamento atual: {dict(zip(unique, counts))}\")\n",
    "    self.logger.info(f\"Raz√£o de balanceamento: {balance_ratio:.3f}\")\n",
    "    \n",
    "    if balance_ratio < 0.5:  # Se muito desbalanceado\n",
    "        method = self.config['balancing']['method']\n",
    "        \n",
    "        if method == 'smote':\n",
    "            # Ajustar k_neighbors baseado no tamanho do dataset\n",
    "            k_neighbors = min(self.config['balancing']['k_neighbors'], min(counts) - 1)\n",
    "            if k_neighbors < 1:\n",
    "                k_neighbors = 1\n",
    "            \n",
    "            self.balancer = SMOTE(\n",
    "                random_state=self.config['balancing']['random_state'],\n",
    "                k_neighbors=k_neighbors\n",
    "            )\n",
    "        elif method == 'random_over':\n",
    "            self.balancer = RandomOverSampler(\n",
    "                random_state=self.config['balancing']['random_state']\n",
    "            )\n",
    "        elif method == 'smoteenn':\n",
    "            self.balancer = SMOTEENN(\n",
    "                random_state=self.config['balancing']['random_state']\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            X_balanced, y_balanced = self.balancer.fit_resample(X, y)\n",
    "            self.logger.info(f\"Dados balanceados: {X_balanced.shape}\")\n",
    "            \n",
    "            # Verificar novo balanceamento\n",
    "            unique_new, counts_new = np.unique(y_balanced, return_counts=True)\n",
    "            self.logger.info(f\"Novo balanceamento: {dict(zip(unique_new, counts_new))}\")\n",
    "            \n",
    "            return X_balanced, y_balanced\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Erro no balanceamento: {e}. Usando dados originais.\")\n",
    "            return X, y\n",
    "    else:\n",
    "        self.logger.info(\"Dados j√° bem balanceados\")\n",
    "        return X, y\n",
    "\n",
    "# Adicionar m√©todos √† classe\n",
    "OptimizedMalwareDetectionSystem.advanced_preprocessing = advanced_preprocessing\n",
    "OptimizedMalwareDetectionSystem._advanced_text_processing = _advanced_text_processing\n",
    "OptimizedMalwareDetectionSystem._advanced_numeric_processing = _advanced_numeric_processing\n",
    "OptimizedMalwareDetectionSystem._feature_engineering = _feature_engineering\n",
    "OptimizedMalwareDetectionSystem._intelligent_feature_selection = _intelligent_feature_selection\n",
    "OptimizedMalwareDetectionSystem._intelligent_dimensionality_reduction = _intelligent_dimensionality_reduction\n",
    "OptimizedMalwareDetectionSystem._intelligent_balancing = _intelligent_balancing\n",
    "\n",
    "print(\"‚úÖ M√©todos de pr√©-processamento avan√ßado adicionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a1351",
   "metadata": {},
   "source": [
    "## 7. Executar Pr√©-processamento Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar pr√©-processamento avan√ßado\n",
    "print(\"üîÑ Iniciando pr√©-processamento otimizado...\")\n",
    "\n",
    "X_processed, y_processed = detector.advanced_preprocessing(df_prepared, target_column='binary_class')\n",
    "\n",
    "print(f\"‚úÖ Pr√©-processamento conclu√≠do!\")\n",
    "print(f\"üìä Formato final: {X_processed.shape}\")\n",
    "print(f\"üè∑Ô∏è Classes: {detector.label_encoder.classes_}\")\n",
    "\n",
    "# An√°lise detalhada do resultado\n",
    "unique_labels, label_counts = np.unique(y_processed, return_counts=True)\n",
    "for i, (label_encoded, count) in enumerate(zip(unique_labels, label_counts)):\n",
    "    label_name = detector.label_encoder.inverse_transform([label_encoded])[0]\n",
    "    percentage = count / len(y_processed) * 100\n",
    "    print(f\"  {label_name}: {count} amostras ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c81fe0",
   "metadata": {},
   "source": [
    "## 8. Treinamento Otimizado com Ensemble Avan√ßado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_train_model(self, X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Treinamento otimizado com ensemble avan√ßado\n",
    "    \"\"\"\n",
    "    self.logger.info(\"Iniciando treinamento otimizado...\")\n",
    "    \n",
    "    # Divis√£o estratificada\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Ajustar scale_pos_weight para XGBoost\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    if len(unique) == 2:\n",
    "        scale_pos_weight = counts[0] / counts[1]\n",
    "        self.config['xgboost']['scale_pos_weight'] = scale_pos_weight\n",
    "    \n",
    "    # Configurar modelos base\n",
    "    rf_model = RandomForestClassifier(**self.config['random_forest'])\n",
    "    xgb_model = xgb.XGBClassifier(**self.config['xgboost'])\n",
    "    lr_model = LogisticRegression(**self.config['logistic_regression'])\n",
    "    \n",
    "    # Ensemble avan√ßado com m√∫ltiplos modelos\n",
    "    self.model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf_model),\n",
    "            ('xgb', xgb_model),\n",
    "            ('lr', lr_model)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo\n",
    "    self.logger.info(\"Treinando ensemble otimizado...\")\n",
    "    self.model.fit(X_train, y_train)\n",
    "    \n",
    "    # Valida√ß√£o detalhada\n",
    "    validation_results = self._comprehensive_validation(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Salvar m√©tricas\n",
    "    self.training_metrics = validation_results\n",
    "    \n",
    "    self.logger.info(\"Treinamento otimizado conclu√≠do!\")\n",
    "    \n",
    "    return self.model\n",
    "\n",
    "def _comprehensive_validation(self, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Valida√ß√£o abrangente do modelo\"\"\"\n",
    "    self.logger.info(\"=== VALIDA√á√ÉO ABRANGENTE ===\")\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = self.model.predict(X_test)\n",
    "    y_pred_proba = self.model.predict_proba(X_test)\n",
    "    \n",
    "    # M√©tricas b√°sicas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # AUC espec√≠fico para classe positiva (Spyware)\n",
    "    if len(self.label_encoder.classes_) == 2:\n",
    "        spyware_idx = list(self.label_encoder.classes_).index('Spyware')\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba[:, spyware_idx])\n",
    "    else:\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "    \n",
    "    # Valida√ß√£o cruzada estratificada\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(self.model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    cv_f1_scores = cross_val_score(self.model, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "    \n",
    "    # Log das m√©tricas\n",
    "    self.logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "    self.logger.info(f\"Precision: {precision:.4f}\")\n",
    "    self.logger.info(f\"Recall: {recall:.4f}\")\n",
    "    self.logger.info(f\"F1-Score: {f1:.4f}\")\n",
    "    self.logger.info(f\"AUC: {auc_score:.4f}\")\n",
    "    self.logger.info(f\"CV Accuracy: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "    self.logger.info(f\"CV F1: {cv_f1_scores.mean():.4f} (¬±{cv_f1_scores.std():.4f})\")\n",
    "    \n",
    "    # Relat√≥rio detalhado\n",
    "    report = classification_report(y_test, y_pred, \n",
    "                                 target_names=self.label_encoder.classes_)\n",
    "    self.logger.info(f\"Relat√≥rio de Classifica√ß√£o:\\n{report}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc_score,\n",
    "        'cv_accuracy_mean': cv_scores.mean(),\n",
    "        'cv_accuracy_std': cv_scores.std(),\n",
    "        'cv_f1_mean': cv_f1_scores.mean(),\n",
    "        'cv_f1_std': cv_f1_scores.std(),\n",
    "        'classification_report': report,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_f1_scores': cv_f1_scores\n",
    "    }\n",
    "\n",
    "def save_optimized_model(self, filepath):\n",
    "    \"\"\"Salvar modelo otimizado com todos os componentes\"\"\"\n",
    "    model_data = {\n",
    "        'model': self.model,\n",
    "        'vectorizer': self.vectorizer,\n",
    "        'dimensionality_reducer': self.dimensionality_reducer,\n",
    "        'scaler': self.scaler,\n",
    "        'label_encoder': self.label_encoder,\n",
    "        'feature_selector': self.feature_selector,\n",
    "        'balancer': self.balancer,\n",
    "        'config': self.config,\n",
    "        'training_metrics': self.training_metrics\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, filepath)\n",
    "    self.logger.info(f\"Modelo otimizado salvo em: {filepath}\")\n",
    "\n",
    "# Adicionar m√©todos √† classe\n",
    "OptimizedMalwareDetectionSystem.optimized_train_model = optimized_train_model\n",
    "OptimizedMalwareDetectionSystem._comprehensive_validation = _comprehensive_validation\n",
    "OptimizedMalwareDetectionSystem.save_optimized_model = save_optimized_model\n",
    "\n",
    "print(\"‚úÖ M√©todos de treinamento otimizado adicionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3414600",
   "metadata": {},
   "source": [
    "## 9. Treinar Modelo Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43cb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo otimizado\n",
    "print(\"üöÄ Iniciando treinamento do modelo otimizado...\")\n",
    "print(\"‚è±Ô∏è Este processo pode levar alguns minutos...\")\n",
    "\n",
    "optimized_model = detector.optimized_train_model(X_processed, y_processed, test_size=0.2)\n",
    "\n",
    "print(\"‚úÖ Treinamento otimizado conclu√≠do!\")\n",
    "print(f\"üéØ Modelo: {type(optimized_model).__name__}\")\n",
    "\n",
    "# Acessar m√©tricas salvas\n",
    "metrics = detector.training_metrics\n",
    "print(f\"\\nüìä M√âTRICAS OTIMIZADAS:\")\n",
    "print(f\"üéØ Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"üìä Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"üìà Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"üî• F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"üöÄ AUC: {metrics['auc']:.4f}\")\n",
    "print(f\"üîÑ CV Accuracy: {metrics['cv_accuracy_mean']:.4f} (¬±{metrics['cv_accuracy_std']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfb006",
   "metadata": {},
   "source": [
    "## 10. Visualiza√ß√µes Avan√ßadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes avan√ßadas dos resultados\n",
    "metrics = detector.training_metrics\n",
    "\n",
    "# 1. Matriz de Confus√£o Melhorada\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cm = confusion_matrix(metrics['y_test'], metrics['y_pred'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=detector.label_encoder.classes_,\n",
    "            yticklabels=detector.label_encoder.classes_,\n",
    "            cbar_kws={'label': 'N√∫mero de Amostras'})\n",
    "plt.title('Matriz de Confus√£o - Modelo Otimizado')\n",
    "plt.xlabel('Predi√ß√£o')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "# 2. Curva ROC\n",
    "plt.subplot(1, 2, 2)\n",
    "if len(detector.label_encoder.classes_) == 2:\n",
    "    spyware_idx = list(detector.label_encoder.classes_).index('Spyware')\n",
    "    fpr, tpr, _ = roc_curve(metrics['y_test'], metrics['y_pred_proba'][:, spyware_idx])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.title('Curva ROC - Detec√ß√£o de Spyware')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Compara√ß√£o de Scores CV\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cv_scores = metrics['cv_scores']\n",
    "plt.bar(range(1, len(cv_scores) + 1), cv_scores, color='skyblue', alpha=0.7)\n",
    "plt.axhline(y=cv_scores.mean(), color='red', linestyle='--', \n",
    "            label=f'M√©dia: {cv_scores.mean():.4f}')\n",
    "plt.axhline(y=cv_scores.mean() + cv_scores.std(), color='orange', linestyle=':', alpha=0.7,\n",
    "            label=f'+1œÉ: {cv_scores.mean() + cv_scores.std():.4f}')\n",
    "plt.axhline(y=cv_scores.mean() - cv_scores.std(), color='orange', linestyle=':', alpha=0.7,\n",
    "            label=f'-1œÉ: {cv_scores.mean() - cv_scores.std():.4f}')\n",
    "plt.title('Valida√ß√£o Cruzada - Accuracy')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cv_f1_scores = metrics['cv_f1_scores']\n",
    "plt.bar(range(1, len(cv_f1_scores) + 1), cv_f1_scores, color='lightgreen', alpha=0.7)\n",
    "plt.axhline(y=cv_f1_scores.mean(), color='red', linestyle='--',\n",
    "            label=f'M√©dia: {cv_f1_scores.mean():.4f}')\n",
    "plt.title('Valida√ß√£o Cruzada - F1-Score')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Resumo de M√©tricas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RESUMO DETALHADO DAS M√âTRICAS OTIMIZADAS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üéØ Accuracy:     {metrics['accuracy']:.4f}\")\n",
    "print(f\"üìä Precision:    {metrics['precision']:.4f}\")\n",
    "print(f\"üìà Recall:       {metrics['recall']:.4f}\")\n",
    "print(f\"üî• F1-Score:     {metrics['f1_score']:.4f}\")\n",
    "print(f\"üöÄ AUC:          {metrics['auc']:.4f}\")\n",
    "print(f\"üîÑ CV Accuracy:  {metrics['cv_accuracy_mean']:.4f} (¬±{metrics['cv_accuracy_std']:.4f})\")\n",
    "print(f\"üîÑ CV F1:        {metrics['cv_f1_mean']:.4f} (¬±{metrics['cv_f1_std']:.4f})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compara√ß√£o com resultados anteriores\n",
    "print(f\"\\nüìà COMPARA√á√ÉO COM VERS√ÉO ANTERIOR:\")\n",
    "print(f\"Accuracy:    {0.5946:.4f} ‚Üí {metrics['accuracy']:.4f} (Œî{metrics['accuracy']-0.5946:+.4f})\")\n",
    "print(f\"AUC:         {0.6244:.4f} ‚Üí {metrics['auc']:.4f} (Œî{metrics['auc']-0.6244:+.4f})\")\n",
    "\n",
    "if metrics['accuracy'] > 0.75:\n",
    "    print(\"üéâ EXCELENTE! M√©tricas superiores a 75%\")\n",
    "elif metrics['accuracy'] > 0.65:\n",
    "    print(\"‚úÖ BOM! Melhoria significativa alcan√ßada\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Melhoria parcial - considere ajustes adicionais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511cd2b",
   "metadata": {},
   "source": [
    "## 11. Salvar Modelo Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo otimizado\n",
    "model_filename = 'optimized_malware_detector.joblib'\n",
    "detector.save_optimized_model(model_filename)\n",
    "\n",
    "print(f\"üíæ Modelo otimizado salvo: {model_filename}\")\n",
    "\n",
    "# Salvar informa√ß√µes detalhadas do treinamento\n",
    "training_info_optimized = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'version': 'optimized_v2.0',\n",
    "    'dataset_shape': df_prepared.shape,\n",
    "    'processed_shape': X_processed.shape,\n",
    "    'target_column': 'binary_class',\n",
    "    'classes': detector.label_encoder.classes_.tolist(),\n",
    "    'metrics': {\n",
    "        'accuracy': float(metrics['accuracy']),\n",
    "        'precision': float(metrics['precision']),\n",
    "        'recall': float(metrics['recall']),\n",
    "        'f1_score': float(metrics['f1_score']),\n",
    "        'auc': float(metrics['auc']),\n",
    "        'cv_accuracy_mean': float(metrics['cv_accuracy_mean']),\n",
    "        'cv_accuracy_std': float(metrics['cv_accuracy_std']),\n",
    "        'cv_f1_mean': float(metrics['cv_f1_mean']),\n",
    "        'cv_f1_std': float(metrics['cv_f1_std'])\n",
    "    },\n",
    "    'improvements': {\n",
    "        'advanced_preprocessing': True,\n",
    "        'feature_engineering': True,\n",
    "        'intelligent_balancing': True,\n",
    "        'ensemble_optimization': True,\n",
    "        'comprehensive_validation': True\n",
    "    },\n",
    "    'config_used': detector.config\n",
    "}\n",
    "\n",
    "with open('training_info_optimized.json', 'w') as f:\n",
    "    json.dump(training_info_optimized, f, indent=2)\n",
    "\n",
    "print(\"üìã Informa√ß√µes detalhadas salvas: training_info_optimized.json\")\n",
    "\n",
    "# Download dos arquivos\n",
    "print(\"\\nüì• Fazendo download dos arquivos...\")\n",
    "files.download(model_filename)\n",
    "files.download('training_info_optimized.json')\n",
    "\n",
    "print(\"‚úÖ Download conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdaf1f1",
   "metadata": {},
   "source": [
    "## 12. Teste Final e An√°lise de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste final do modelo otimizado\n",
    "def comprehensive_model_test(detector, X_test, y_test):\n",
    "    \"\"\"Teste abrangente do modelo\"\"\"\n",
    "    \n",
    "    print(\"üß™ TESTE ABRANGENTE DO MODELO OTIMIZADO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Predi√ß√µes detalhadas\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    confidences = []\n",
    "    \n",
    "    for i in range(min(10, len(X_test))):\n",
    "        sample = X_test[i:i+1]\n",
    "        true_encoded = y_test[i]\n",
    "        true_label = detector.label_encoder.inverse_transform([true_encoded])[0]\n",
    "        \n",
    "        # Predi√ß√£o\n",
    "        pred_encoded = detector.model.predict(sample)[0]\n",
    "        pred_label = detector.label_encoder.inverse_transform([pred_encoded])[0]\n",
    "        \n",
    "        # Probabilidades\n",
    "        probabilities = detector.model.predict_proba(sample)[0]\n",
    "        confidence = np.max(probabilities)\n",
    "        \n",
    "        # Status\n",
    "        correct = true_label == pred_label\n",
    "        status = \"‚úÖ CORRETO\" if correct else \"‚ùå INCORRETO\"\n",
    "        \n",
    "        print(f\"Teste {i+1:2d}: {true_label:>8} ‚Üí {pred_label:>8} ({confidence:.3f}) {status}\")\n",
    "        \n",
    "        predictions.append(pred_label)\n",
    "        true_labels.append(true_label)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    # Estat√≠sticas do teste\n",
    "    accuracy_test = sum(t == p for t, p in zip(true_labels, predictions)) / len(true_labels)\n",
    "    avg_confidence = np.mean(confidences)\n",
    "    \n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DO TESTE:\")\n",
    "    print(f\"Accuracy no teste: {accuracy_test:.3f}\")\n",
    "    print(f\"Confian√ßa m√©dia: {avg_confidence:.3f}\")\n",
    "    \n",
    "    return accuracy_test, avg_confidence\n",
    "\n",
    "# Executar teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed\n",
    ")\n",
    "\n",
    "test_accuracy, test_confidence = comprehensive_model_test(detector, X_test, y_test)\n",
    "\n",
    "print(f\"\\nüéâ CONCLUS√ÉO FINAL:\")\n",
    "print(f\"üéØ Modelo otimizado atingiu {metrics['accuracy']:.1%} de accuracy\")\n",
    "print(f\"üöÄ AUC de {metrics['auc']:.3f} indica excelente capacidade de discrimina√ß√£o\")\n",
    "print(f\"üíØ Confian√ßa m√©dia de {test_confidence:.1%} nas predi√ß√µes\")\n",
    "\n",
    "if metrics['accuracy'] > 0.80:\n",
    "    print(\"üèÜ EXCELENTE! Modelo pronto para produ√ß√£o\")\n",
    "elif metrics['accuracy'] > 0.70:\n",
    "    print(\"‚úÖ MUITO BOM! Modelo adequado para uso\")\n",
    "elif metrics['accuracy'] > 0.60:\n",
    "    print(\"üëç BOM! Melhoria significativa alcan√ßada\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Considere otimiza√ß√µes adicionais\")\n",
    "\n",
    "print(\"\\nüõ°Ô∏è O modelo otimizado est√° pronto para detectar malware Spyware!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67745c",
   "metadata": {},
   "source": [
    "## 13. An√°lise Comparativa dos Resultados\n",
    "\n",
    "### üìä Evolu√ß√£o das M√©tricas:\n",
    "\n",
    "1. **Vers√£o Original**: 59.4% accuracy, 62.4% AUC\n",
    "2. **Vers√£o Otimizada**: ~62.8% accuracy, ~66.1% AUC  \n",
    "3. **Vers√£o com Dados Benignos Reais**: (Resultados atuais)\n",
    "\n",
    "### üîç An√°lise da Melhoria:\n",
    "\n",
    "O uso de dados benignos reais coletados do Windows 11 deve proporcionar:\n",
    "\n",
    "- ‚úÖ **Maior Realismo**: Padr√µes de API reais de aplicativos leg√≠timos\n",
    "- ‚úÖ **Melhor Discrimina√ß√£o**: Distin√ß√£o mais clara entre comportamento benigno e malicioso\n",
    "- ‚úÖ **Redu√ß√£o de Overfitting**: Dados mais variados e naturais\n",
    "- ‚úÖ **Generaliza√ß√£o**: Modelo mais robusto para detec√ß√£o em ambiente real\n",
    "\n",
    "### üéØ Pr√≥ximos Passos:\n",
    "\n",
    "1. **Coleta Cont√≠nua**: Execute o coletor periodicamente para ampliar o dataset\n",
    "2. **Refinamento**: Ajuste os padr√µes de API baseado nos resultados\n",
    "3. **Valida√ß√£o**: Teste com malware real em ambiente controlado\n",
    "4. **Deploy**: Implementa√ß√£o em sistema de monitoramento real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7bd31",
   "metadata": {},
   "source": [
    "## 13. Resumo das Melhorias Implementadas\n",
    "\n",
    "### üöÄ **Principais Otimiza√ß√µes:**\n",
    "\n",
    "1. **üìä Prepara√ß√£o Inteligente de Dados:**\n",
    "   - Filtragem estrat√©gica de tipos de malware\n",
    "   - Balanceamento baseado em tipos espec√≠ficos\n",
    "   - Valida√ß√£o de qualidade dos dados\n",
    "\n",
    "2. **üîß Pr√©-processamento Avan√ßado:**\n",
    "   - Processamento robusto de texto com m√∫ltiplas op√ß√µes\n",
    "   - Feature engineering com estat√≠sticas descritivas\n",
    "   - Sele√ß√£o inteligente de caracter√≠sticas\n",
    "   - Balanceamento autom√°tico com SMOTE\n",
    "\n",
    "3. **ü§ñ Modelo Ensemble Otimizado:**\n",
    "   - Tr√™s algoritmos complementares (RF, XGB, LR)\n",
    "   - Configura√ß√µes ajustadas para datasets pequenos\n",
    "   - Regulariza√ß√£o para evitar overfitting\n",
    "   - Class weighting autom√°tico\n",
    "\n",
    "4. **üìà Valida√ß√£o Abrangente:**\n",
    "   - M√∫ltiplas m√©tricas de avalia√ß√£o\n",
    "   - Valida√ß√£o cruzada estratificada\n",
    "   - An√°lise de curva ROC\n",
    "   - Testes de confian√ßa\n",
    "\n",
    "### üéØ **Resultados Esperados:**\n",
    "- Accuracy > 75%\n",
    "- AUC > 0.80\n",
    "- F1-Score balanceado\n",
    "- Baixa vari√¢ncia entre folds\n",
    "\n",
    "### üìÅ **Arquivos Gerados:**\n",
    "- `optimized_malware_detector.joblib` - Modelo completo\n",
    "- `training_info_optimized.json` - M√©tricas detalhadas"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
