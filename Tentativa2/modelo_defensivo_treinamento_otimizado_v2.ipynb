{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b988120",
   "metadata": {},
   "source": [
    "# Sistema de Detecção de Malware Polimórfico Controlado por LLM\n",
    "# Treinamento do Modelo Defensivo - VERSÃO OTIMIZADA\n",
    "\n",
    "Este notebook contém o treinamento **otimizado** do modelo de detecção de malware keylogger polimórfico baseado no framework teórico-prático com Random Forest e MALAPI2019.\n",
    "\n",
    "**Melhorias Implementadas:**\n",
    "- ✅ Pré-processamento aprimorado para dados pequenos\n",
    "- ✅ Balanceamento inteligente do dataset\n",
    "- ✅ Feature engineering avançada\n",
    "- ✅ Otimização de hiperparâmetros\n",
    "- ✅ Validação robusta\n",
    "\n",
    "**Objetivo:** Treinar um modelo capaz de identificar malware keylogger polimórfico com métricas superiores a 80% de accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9360702",
   "metadata": {},
   "source": [
    "## 1. Importação de Bibliotecas e Configuração Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36992a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de dependências específicas para Google Colab\n",
    "!pip install xgboost\n",
    "!pip install shap\n",
    "!pip install joblib\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "# Importações principais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Bibliotecas de ML\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Balanceamento de dados\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Interpretabilidade\n",
    "import shap\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17419aa1",
   "metadata": {},
   "source": [
    "## 2. Classe Otimizada do Sistema de Detecção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900648bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedMalwareDetectionSystem:\n",
    "    \"\"\"\n",
    "    Sistema Otimizado de Detecção de Malware Polimórfico\n",
    "    Implementa múltiplas melhorias para datasets pequenos\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path=None):\n",
    "        self.config = self._load_optimized_config(config_path)\n",
    "        self.model = None\n",
    "        self.vectorizer = None\n",
    "        self.dimensionality_reducer = None\n",
    "        self.scaler = RobustScaler()  # Mais robusto para outliers\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.feature_selector = None\n",
    "        self.shap_explainer = None\n",
    "        self.balancer = None\n",
    "        \n",
    "        # Métricas detalhadas\n",
    "        self.training_metrics = {}\n",
    "        \n",
    "        self._setup_logging()\n",
    "\n",
    "    def _load_optimized_config(self, config_path):\n",
    "        \"\"\"Configurações otimizadas para datasets pequenos\"\"\"\n",
    "        optimized_config = {\n",
    "            'vectorization': {\n",
    "                'method': 'tfidf',  # 'tfidf', 'count', 'hybrid'\n",
    "                'max_features': 5000,  # Reduzido para dataset pequeno\n",
    "                'ngram_range': (1, 3),  # Incluir trigramas\n",
    "                'min_df': 1,  # Mais permissivo para dataset pequeno\n",
    "                'max_df': 0.9,\n",
    "                'analyzer': 'word'\n",
    "            },\n",
    "            'feature_selection': {\n",
    "                'method': 'mutual_info',  # 'mutual_info', 'chi2', 'model_based'\n",
    "                'k_best': 500,  # Reduzido significativamente\n",
    "                'threshold': 'median'\n",
    "            },\n",
    "            'dimensionality_reduction': {\n",
    "                'method': 'pca',  # 'pca', 'svd', 'none'\n",
    "                'n_components': 0.90,  # 90% da variância\n",
    "                'random_state': 42\n",
    "            },\n",
    "            'balancing': {\n",
    "                'method': 'smote',  # 'smote', 'random_over', 'smoteenn'\n",
    "                'random_state': 42,\n",
    "                'k_neighbors': 3  # Reduzido para dataset pequeno\n",
    "            },\n",
    "            'random_forest': {\n",
    "                'n_estimators': 200,  # Reduzido para evitar overfitting\n",
    "                'max_depth': 10,      # Limitado para dataset pequeno\n",
    "                'min_samples_split': 10,  # Aumentado para evitar overfitting\n",
    "                'min_samples_leaf': 5,    # Aumentado para evitar overfitting\n",
    "                'criterion': 'gini',\n",
    "                'max_features': 'sqrt',   # Mais conservativo\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'class_weight': 'balanced'  # Importante para dados desbalanceados\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'n_estimators': 150,\n",
    "                'max_depth': 4,       # Reduzido para evitar overfitting\n",
    "                'learning_rate': 0.05,  # Mais conservativo\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'reg_alpha': 0.1,     # Regularização L1\n",
    "                'reg_lambda': 0.1,    # Regularização L2\n",
    "                'random_state': 42,\n",
    "                'scale_pos_weight': 1  # Será ajustado baseado no balanceamento\n",
    "            },\n",
    "            'logistic_regression': {\n",
    "                'C': 1.0,\n",
    "                'penalty': 'l2',\n",
    "                'solver': 'liblinear',\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if config_path and Path(config_path).exists():\n",
    "            with open(config_path, 'r') as f:\n",
    "                user_config = json.load(f)\n",
    "            optimized_config.update(user_config)\n",
    "            \n",
    "        return optimized_config\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configurar sistema de logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Classe OptimizedMalwareDetectionSystem definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59573c00",
   "metadata": {},
   "source": [
    "## 3. Métodos de Carregamento e Análise Otimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926194a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_dataset(self, dataset_path):\n",
    "    \"\"\"\n",
    "    Carregamento e análise otimizada do dataset\n",
    "    \"\"\"\n",
    "    self.logger.info(\"Carregando dataset MALAPI2019...\")\n",
    "    \n",
    "    try:\n",
    "        # Carregar dataset com otimizações\n",
    "        if dataset_path.endswith('.csv'):\n",
    "            df = pd.read_csv(dataset_path, low_memory=False)\n",
    "        elif dataset_path.endswith('.txt'):\n",
    "            # Tentar diferentes delimitadores\n",
    "            try:\n",
    "                df = pd.read_csv(dataset_path, delimiter='\\t', header=0, low_memory=False)\n",
    "            except:\n",
    "                df = pd.read_csv(dataset_path, delimiter=',', header=0, low_memory=False)\n",
    "        else:\n",
    "            raise ValueError(\"Formato de dataset não suportado\")\n",
    "        \n",
    "        self.logger.info(f\"Dataset carregado: {df.shape}\")\n",
    "        \n",
    "        # Análise exploratória detalhada\n",
    "        self._comprehensive_eda(df)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Erro ao carregar dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def _comprehensive_eda(self, df):\n",
    "    \"\"\"Análise exploratória abrangente\"\"\"\n",
    "    self.logger.info(\"=== ANÁLISE EXPLORATÓRIA DETALHADA ===\")\n",
    "    self.logger.info(f\"Dimensões: {df.shape}\")\n",
    "    self.logger.info(f\"Memória: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    self.logger.info(f\"Colunas: {list(df.columns)}\")\n",
    "    \n",
    "    # Análise de tipos de dados\n",
    "    self.logger.info(f\"Tipos de dados:\")\n",
    "    for dtype in df.dtypes.unique():\n",
    "        cols = df.select_dtypes(include=[dtype]).columns\n",
    "        self.logger.info(f\"  {dtype}: {len(cols)} colunas\")\n",
    "    \n",
    "    # Análise de valores ausentes\n",
    "    missing_total = df.isnull().sum().sum()\n",
    "    if missing_total > 0:\n",
    "        self.logger.warning(f\"Total de valores ausentes: {missing_total}\")\n",
    "        missing_cols = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "        for col, count in missing_cols.items():\n",
    "            self.logger.warning(f\"  {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Análise de duplicatas\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        self.logger.warning(f\"Linhas duplicadas: {duplicates}\")\n",
    "\n",
    "# Adicionar métodos à classe\n",
    "OptimizedMalwareDetectionSystem.load_and_analyze_dataset = load_and_analyze_dataset\n",
    "OptimizedMalwareDetectionSystem._comprehensive_eda = _comprehensive_eda\n",
    "\n",
    "print(\"✅ Métodos de carregamento otimizados adicionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bc9ee",
   "metadata": {},
   "source": [
    "## 4. Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee18432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para os arquivos\n",
    "dataset_filename = '/content/drive/MyDrive/IFSP/all_analysis_data.txt'\n",
    "labels_filename = '/content/drive/MyDrive/IFSP/labels.csv'\n",
    "\n",
    "# Inicializar o sistema otimizado\n",
    "detector = OptimizedMalwareDetectionSystem()\n",
    "\n",
    "# Carregar dataset principal\n",
    "df = detector.load_and_analyze_dataset(dataset_filename)\n",
    "\n",
    "print(f\"📊 Dataset carregado: {df.shape}\")\n",
    "print(f\"📋 Colunas: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d91a09f",
   "metadata": {},
   "source": [
    "## 4.1. Coleta de Dados Benignos Reais (Opcional)\n",
    "\n",
    "**IMPORTANTE:** Esta seção é opcional e deve ser executada localmente no Windows 11 para coletar dados benignos reais.\n",
    "\n",
    "### Instruções para Coleta Local:\n",
    "\n",
    "1. **Execute o coletor simplificado** (não requer Sysmon):\n",
    "   ```python\n",
    "   # No seu computador Windows 11, execute:\n",
    "   python simple_benign_collector.py\n",
    "   ```\n",
    "\n",
    "2. **Ou execute o coletor completo** (requer Sysmon):\n",
    "   ```python\n",
    "   # Para coleta mais detalhada com Sysmon:\n",
    "   python benign_data_collector.py\n",
    "   ```\n",
    "\n",
    "3. **Upload do arquivo gerado:**\n",
    "   - Faça upload do arquivo CSV gerado para o Google Drive\n",
    "   - Atualize o caminho abaixo com o arquivo real\n",
    "\n",
    "### Configuração do Arquivo de Dados Benignos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b42de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração para dados benignos reais\n",
    "# Defina o caminho para o arquivo de dados benignos coletados localmente\n",
    "# Se não disponível, será usado dataset sintético\n",
    "\n",
    "USE_REAL_BENIGN_DATA = False  # Altere para True se você coletou dados reais\n",
    "\n",
    "# Caminho para o arquivo de dados benignos reais (atualize conforme necessário)\n",
    "benign_data_filename = '/content/drive/MyDrive/IFSP/benign_api_dataset_YYYYMMDD_HHMMSS.csv'\n",
    "\n",
    "# Verificar se arquivo existe\n",
    "if USE_REAL_BENIGN_DATA:\n",
    "    if Path(benign_data_filename).exists():\n",
    "        print(\"✅ Arquivo de dados benignos reais encontrado!\")\n",
    "        print(f\"📁 Caminho: {benign_data_filename}\")\n",
    "    else:\n",
    "        print(\"⚠️ Arquivo de dados benignos não encontrado!\")\n",
    "        print(\"🔄 Usando geração sintética como fallback\")\n",
    "        USE_REAL_BENIGN_DATA = False\n",
    "else:\n",
    "    print(\"📊 Usando geração sintética de dados benignos\")\n",
    "\n",
    "print(f\"🎯 Modo selecionado: {'Dados Reais' if USE_REAL_BENIGN_DATA else 'Dados Sintéticos'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_benign_data(num_samples=1000):\n",
    "    \"\"\"\n",
    "    Gerador melhorado de dados benignos sintéticos baseado em padrões reais\n",
    "    \"\"\"\n",
    "    print(\"🔄 Gerando dados benignos sintéticos realísticos...\")\n",
    "    \n",
    "    # Padrões de API mais realísticos para aplicativos benignos\n",
    "    benign_patterns = {\n",
    "        'text_editor': [\n",
    "            'CreateFileW ReadFile GetFileSize SetFilePointer WriteFile FlushFileBuffers CloseHandle',\n",
    "            'CreateFileW WriteFile SetEndOfFile CloseHandle GetLastError',\n",
    "            'FindFirstFileW FindNextFileW GetFileAttributesW FindClose',\n",
    "            'CreateDirectoryW CreateFileW WriteFile CloseHandle',\n",
    "            'RegOpenKeyW RegQueryValueW RegSetValueW RegCloseKey',\n",
    "            'LoadLibraryW GetProcAddress GetModuleHandleW FreeLibrary',\n",
    "            'CreateThread WaitForSingleObject CloseHandle GetCurrentThread',\n",
    "            'VirtualAlloc VirtualProtect VirtualFree GetProcessHeap'\n",
    "        ],\n",
    "        'web_browser': [\n",
    "            'WSAStartup WSASocket connect send recv closesocket WSACleanup',\n",
    "            'InternetOpenW InternetConnectW HttpOpenRequestW HttpSendRequestW HttpQueryInfoW InternetCloseHandle',\n",
    "            'CreateFileW WriteFile ReadFile SetFilePointer CloseHandle DeleteFileW',\n",
    "            'RegOpenKeyW RegSetValueW RegQueryValueW RegDeleteValueW RegCloseKey',\n",
    "            'VirtualAlloc VirtualProtect VirtualFree HeapAlloc HeapFree',\n",
    "            'CreateThread CreateEvent SetEvent WaitForSingleObject CloseHandle',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary GetModuleFileNameW',\n",
    "            'CreateProcessW OpenProcess GetProcessImageFileNameW TerminateProcess'\n",
    "        ],\n",
    "        'office_app': [\n",
    "            'CreateFileW ReadFile WriteFile SetFilePointer GetFileSize FlushFileBuffers CloseHandle',\n",
    "            'CreateDirectoryW CopyFileW MoveFileW DeleteFileW GetFileAttributesW',\n",
    "            'RegOpenKeyW RegEnumKeyW RegEnumValueW RegSetValueW RegDeleteKeyW RegCloseKey',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary GetModuleHandleW',\n",
    "            'CreateThread CreateMutexW ReleaseMutex WaitForSingleObject CloseHandle',\n",
    "            'VirtualAlloc VirtualProtect VirtualFree GlobalAlloc GlobalFree',\n",
    "            'CreateEvent SetEvent ResetEvent WaitForMultipleObjects',\n",
    "            'GetSystemInfo GetVersionExW GetComputerNameW GetUserNameW'\n",
    "        ],\n",
    "        'system_tool': [\n",
    "            'CreateProcessW OpenProcess GetProcessImageFileNameW WaitForSingleObject TerminateProcess CloseHandle',\n",
    "            'GetSystemInfo GetVersionExW GetComputerNameW GetUserNameW GetSystemDirectoryW',\n",
    "            'RegOpenKeyW RegEnumKeyW RegQueryValueW RegCloseKey RegConnectRegistryW',\n",
    "            'FindFirstFileW FindNextFileW GetFileAttributesW GetFileInformationByHandle FindClose',\n",
    "            'CreateFileW ReadFile GetFileSize SetFilePointer CloseHandle',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary GetModuleFileNameW',\n",
    "            'CreateThread GetCurrentThread GetThreadId SetThreadPriority',\n",
    "            'VirtualQueryEx VirtualAllocEx VirtualFreeEx ReadProcessMemory'\n",
    "        ],\n",
    "        'media_player': [\n",
    "            'CreateFileW ReadFile SetFilePointer GetFileSize CloseHandle',\n",
    "            'DirectSoundCreate CreateSoundBuffer Play Stop GetCurrentPosition',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary GetModuleHandleW',\n",
    "            'CreateThread SetThreadPriority WaitForSingleObject ResumeThread SuspendThread',\n",
    "            'VirtualAlloc VirtualProtect VirtualFree HeapAlloc HeapReAlloc HeapFree',\n",
    "            'CreateEvent SetEvent ResetEvent WaitForMultipleObjects',\n",
    "            'RegOpenKeyW RegQueryValueW RegSetValueW RegCloseKey',\n",
    "            'GetSystemMetrics GetDeviceCaps CreateCompatibleDC DeleteDC'\n",
    "        ],\n",
    "        'file_manager': [\n",
    "            'FindFirstFileW FindNextFileW GetFileAttributesW GetFileInformationByHandle FindClose',\n",
    "            'CreateDirectoryW RemoveDirectoryW CopyFileW MoveFileW DeleteFileW',\n",
    "            'SHGetFolderPathW SHBrowseForFolderW SHGetPathFromIDListW SHFileOperationW',\n",
    "            'RegOpenKeyW RegEnumKeyW RegEnumValueW RegQueryValueW RegCloseKey',\n",
    "            'CreateFileW GetFileSize GetFileTime SetFileTime CloseHandle',\n",
    "            'LoadLibraryW GetProcAddress FreeLibrary Shell32.dll',\n",
    "            'CreateThread PostMessage SendMessage GetMessage DispatchMessage',\n",
    "            'CreateWindow ShowWindow UpdateWindow SetWindowPos GetWindowRect'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Distribuição mais realística por categoria\n",
    "    category_weights = {\n",
    "        'text_editor': 0.15,\n",
    "        'web_browser': 0.35,\n",
    "        'office_app': 0.20,\n",
    "        'system_tool': 0.10,\n",
    "        'media_player': 0.10,\n",
    "        'file_manager': 0.10\n",
    "    }\n",
    "    \n",
    "    for category, weight in category_weights.items():\n",
    "        category_samples = int(num_samples * weight)\n",
    "        patterns = benign_patterns[category]\n",
    "        \n",
    "        for i in range(category_samples):\n",
    "            # Escolher padrão base\n",
    "            base_pattern = np.random.choice(patterns)\n",
    "            \n",
    "            # Adicionar variações realísticas\n",
    "            variation_chance = np.random.random()\n",
    "            \n",
    "            if variation_chance > 0.7:\n",
    "                # Adicionar APIs de erro/tratamento\n",
    "                error_apis = ['GetLastError', 'SetLastError', 'FormatMessageW']\n",
    "                base_pattern += ' ' + np.random.choice(error_apis)\n",
    "            elif variation_chance > 0.5:\n",
    "                # Adicionar APIs de limpeza\n",
    "                cleanup_apis = ['CloseHandle', 'FreeLibrary', 'DeleteObject']\n",
    "                base_pattern += ' ' + np.random.choice(cleanup_apis)\n",
    "            elif variation_chance > 0.3:\n",
    "                # Adicionar APIs de informação do sistema\n",
    "                info_apis = ['GetTickCount', 'GetSystemTime', 'QueryPerformanceCounter']\n",
    "                base_pattern += ' ' + np.random.choice(info_apis)\n",
    "            \n",
    "            # Simular timestamp realístico (últimas 2 semanas)\n",
    "            hours_ago = np.random.exponential(24)  # Distribuição exponencial\n",
    "            timestamp = datetime.now() - timedelta(hours=min(hours_ago, 336))  # Max 2 semanas\n",
    "            \n",
    "            # Simular características realísticas\n",
    "            record = {\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'app_category': category,\n",
    "                'process_name': f\"{category}_app.exe\",\n",
    "                'api_calls': base_pattern,\n",
    "                'process_id': np.random.randint(1000, 32767),\n",
    "                'memory_usage': np.random.normal(\n",
    "                    {'text_editor': 25000000, 'web_browser': 150000000, 'office_app': 80000000,\n",
    "                     'system_tool': 15000000, 'media_player': 60000000, 'file_manager': 40000000}[category],\n",
    "                    10000000\n",
    "                ),\n",
    "                'label': 'Benign'\n",
    "            }\n",
    "            \n",
    "            synthetic_data.append(record)\n",
    "    \n",
    "    # Converter para DataFrame\n",
    "    df_benign = pd.DataFrame(synthetic_data)\n",
    "    \n",
    "    # Adicionar variabilidade temporal\n",
    "    df_benign['timestamp'] = pd.to_datetime(df_benign['timestamp'])\n",
    "    df_benign = df_benign.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✅ Gerados {len(df_benign)} registros benignos sintéticos\")\n",
    "    print(f\"📊 Distribuição por categoria:\")\n",
    "    for category, count in df_benign['app_category'].value_counts().items():\n",
    "        print(f\"   {category}: {count} amostras ({count/len(df_benign)*100:.1f}%)\")\n",
    "    \n",
    "    return df_benign\n",
    "\n",
    "# Gerar dados benignos\n",
    "if USE_REAL_BENIGN_DATA:\n",
    "    print(\"📂 Carregando dados benignos reais...\")\n",
    "    df_benign = pd.read_csv(benign_data_filename)\n",
    "    print(f\"✅ Carregados {len(df_benign)} registros reais\")\n",
    "else:\n",
    "    df_benign = generate_realistic_benign_data(num_samples=1500)\n",
    "\n",
    "print(f\"🎯 Dataset benigno final: {df_benign.shape}\")\n",
    "print(f\"📋 Colunas disponíveis: {list(df_benign.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db5075",
   "metadata": {},
   "source": [
    "## 5. Filtragem e Preparação Inteligente de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e77e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_data_preparation_v2(detector, df, labels_filename, df_benign=None):\n",
    "    \"\"\"\n",
    "    Preparação inteligente dos dados com dados benignos reais/sintéticos\n",
    "    \"\"\"\n",
    "    print(\"🔄 Iniciando preparação inteligente dos dados V2...\")\n",
    "    \n",
    "    # Carregar labels\n",
    "    labels_df = pd.read_csv(labels_filename, header=None, names=['label'])\n",
    "    print(f\"📊 Labels carregadas: {labels_df.shape}\")\n",
    "    \n",
    "    # Análise da distribuição de labels\n",
    "    print(f\"📈 Distribuição original de labels:\")\n",
    "    label_counts = labels_df['label'].value_counts()\n",
    "    print(label_counts)\n",
    "    \n",
    "    # Ajustar tamanhos se necessário\n",
    "    min_size = min(len(df), len(labels_df))\n",
    "    if len(df) != len(labels_df):\n",
    "        print(f\"⚠️ Ajustando para tamanho comum: {min_size}\")\n",
    "        df = df.iloc[:min_size].copy()\n",
    "        labels_df = labels_df.iloc[:min_size].copy()\n",
    "    \n",
    "    # Adicionar labels\n",
    "    df_with_labels = df.copy()\n",
    "    df_with_labels['malware_type'] = labels_df['label']\n",
    "    \n",
    "    print(\"\\n🎯 ESTRATÉGIA DE PREPARAÇÃO COM DADOS BENIGNOS REAIS:\")\n",
    "    \n",
    "    # Separar apenas Spyware do dataset original\n",
    "    spyware_data = df_with_labels[df_with_labels['malware_type'] == 'Spyware'].copy()\n",
    "    print(f\"🕵️ Dados Spyware encontrados: {len(spyware_data)}\")\n",
    "    \n",
    "    # Usar dados benignos reais/sintéticos\n",
    "    if df_benign is not None:\n",
    "        print(f\"✅ Usando dados benignos fornecidos: {len(df_benign)} amostras\")\n",
    "        \n",
    "        # Padronizar formato dos dados benignos\n",
    "        benign_data = df_benign.copy()\n",
    "        \n",
    "        # Garantir que temos a coluna de API calls no formato correto\n",
    "        if 'api_calls' in benign_data.columns:\n",
    "            # Renomear para coincidir com o formato do dataset original\n",
    "            api_column = df.columns[0]  # Primeira coluna do dataset original\n",
    "            benign_data = benign_data.rename(columns={'api_calls': api_column})\n",
    "        \n",
    "        # Adicionar outras colunas se necessário (preencher com dados padrão)\n",
    "        for col in df.columns:\n",
    "            if col not in benign_data.columns:\n",
    "                benign_data[col] = ''  # Preencher com string vazia ou valor padrão\n",
    "        \n",
    "        # Manter apenas as colunas relevantes\n",
    "        benign_data = benign_data[df.columns]\n",
    "        \n",
    "        # Marcar como benigno\n",
    "        benign_data['malware_type'] = 'Benign'\n",
    "        benign_data['binary_class'] = 'Benign'\n",
    "        \n",
    "        # Balancear com Spyware\n",
    "        target_size = len(spyware_data)\n",
    "        if len(benign_data) > target_size:\n",
    "            benign_data = benign_data.sample(n=target_size, random_state=42)\n",
    "            print(f\"🔄 Balanceado para {target_size} amostras benignas\")\n",
    "        elif len(benign_data) < target_size:\n",
    "            # Se temos menos dados benignos, usar todos e balancear Spyware\n",
    "            target_size = len(benign_data)\n",
    "            spyware_data = spyware_data.sample(n=target_size, random_state=42)\n",
    "            print(f\"🔄 Balanceado Spyware para {target_size} amostras\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️ Nenhum dado benigno fornecido - usando estratégia anterior\")\n",
    "        # Fallback para estratégia anterior\n",
    "        benign_types = ['Trojan', 'Backdoor']\n",
    "        benign_candidates = df_with_labels[df_with_labels['malware_type'].isin(benign_types)]\n",
    "        target_size = len(spyware_data)\n",
    "        \n",
    "        if len(benign_candidates) >= target_size:\n",
    "            benign_data = benign_candidates.groupby('malware_type').apply(\n",
    "                lambda x: x.sample(min(len(x), target_size // len(benign_types)), \n",
    "                                  random_state=42)\n",
    "            ).reset_index(drop=True)\n",
    "            benign_data = benign_data.sample(n=target_size, random_state=42)\n",
    "        else:\n",
    "            benign_data = benign_candidates.copy()\n",
    "            \n",
    "        benign_data['binary_class'] = 'Benign'\n",
    "    \n",
    "    # Marcar Spyware\n",
    "    spyware_data['binary_class'] = 'Spyware'\n",
    "    \n",
    "    # Combinar dados\n",
    "    final_df = pd.concat([spyware_data, benign_data], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n📊 Dataset final balanceado:\")\n",
    "    print(f\"  🕵️ Spyware: {len(spyware_data)} amostras\")\n",
    "    print(f\"  ✅ Benign: {len(benign_data)} amostras\")\n",
    "    print(f\"  📈 Total: {len(final_df)} amostras\")\n",
    "    print(f\"  ⚖️ Balanceamento: {len(spyware_data)/len(final_df)*100:.1f}% Spyware\")\n",
    "    \n",
    "    # Verificar qualidade dos dados\n",
    "    print(f\"\\n🔍 ANÁLISE DE QUALIDADE:\")\n",
    "    print(f\"📊 Distribuição binária:\")\n",
    "    print(final_df['binary_class'].value_counts())\n",
    "    \n",
    "    if 'malware_type' in final_df.columns:\n",
    "        print(f\"📊 Composição detalhada:\")\n",
    "        composition = final_df.groupby(['binary_class', 'malware_type']).size()\n",
    "        print(composition)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Executar preparação inteligente V2\n",
    "df_prepared_v2 = intelligent_data_preparation_v2(detector, df, labels_filename, df_benign)\n",
    "\n",
    "print(f\"\\n✅ Preparação V2 concluída! Dataset final: {df_prepared_v2.shape}\")\n",
    "\n",
    "# Comparar com versão anterior\n",
    "print(f\"\\n📈 COMPARAÇÃO COM PREPARAÇÃO ANTERIOR:\")\n",
    "print(f\"Dataset V1: {df_prepared.shape}\")\n",
    "print(f\"Dataset V2: {df_prepared_v2.shape}\")\n",
    "print(f\"Melhoria: {len(df_prepared_v2) - len(df_prepared):+d} amostras ({(len(df_prepared_v2)/len(df_prepared)-1)*100:+.1f}%)\")\n",
    "\n",
    "# Usar a versão V2 para o resto do processamento\n",
    "df_prepared = df_prepared_v2.copy()\n",
    "print(f\"🎯 Usando Dataset V2 para treinamento: {df_prepared.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930009b4",
   "metadata": {},
   "source": [
    "## 6. Pré-processamento Avançado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_preprocessing(self, df, target_column='binary_class'):\n",
    "    \"\"\"\n",
    "    Pré-processamento avançado com múltiplas melhorias\n",
    "    \"\"\"\n",
    "    self.logger.info(\"Iniciando pré-processamento avançado...\")\n",
    "    \n",
    "    try:\n",
    "        # Separar features e target\n",
    "        y = df[target_column]\n",
    "        X = df.drop(columns=[target_column, 'malware_type'], errors='ignore')\n",
    "        \n",
    "        # Codificar labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        self.logger.info(f\"Classes: {self.label_encoder.classes_}\")\n",
    "        \n",
    "        # Identificar tipo de dados nas features\n",
    "        if X.shape[1] == 1:\n",
    "            # Assumir que é coluna de API calls\n",
    "            api_column = X.columns[0]\n",
    "            self.logger.info(f\"Processando coluna de API calls: {api_column}\")\n",
    "            X_processed = self._advanced_text_processing(X[api_column])\n",
    "        else:\n",
    "            # Múltiplas colunas - verificar tipos\n",
    "            text_cols = X.select_dtypes(include=['object']).columns\n",
    "            numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "            \n",
    "            if len(text_cols) > 0:\n",
    "                self.logger.info(f\"Processando {len(text_cols)} colunas de texto\")\n",
    "                X_processed = self._advanced_text_processing(X[text_cols[0]])\n",
    "            else:\n",
    "                self.logger.info(f\"Processando {len(numeric_cols)} colunas numéricas\")\n",
    "                X_processed = self._advanced_numeric_processing(X[numeric_cols])\n",
    "        \n",
    "        # Feature engineering\n",
    "        X_engineered = self._feature_engineering(X_processed, y_encoded)\n",
    "        \n",
    "        # Seleção de características\n",
    "        X_selected = self._intelligent_feature_selection(X_engineered, y_encoded)\n",
    "        \n",
    "        # Redução de dimensionalidade (se necessário)\n",
    "        X_reduced = self._intelligent_dimensionality_reduction(X_selected)\n",
    "        \n",
    "        # Balanceamento de dados\n",
    "        X_balanced, y_balanced = self._intelligent_balancing(X_reduced, y_encoded)\n",
    "        \n",
    "        self.logger.info(f\"Pré-processamento concluído: {X_balanced.shape}\")\n",
    "        \n",
    "        return X_balanced, y_balanced\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Erro no pré-processamento: {e}\")\n",
    "        raise\n",
    "\n",
    "def _advanced_text_processing(self, text_series):\n",
    "    \"\"\"Processamento avançado de texto\"\"\"\n",
    "    self.logger.info(\"Aplicando processamento avançado de texto...\")\n",
    "    \n",
    "    # Limpar e processar texto\n",
    "    processed_texts = []\n",
    "    for text in text_series:\n",
    "        if pd.isna(text):\n",
    "            processed_texts.append(\"\")\n",
    "        else:\n",
    "            # Converter para string e limpar\n",
    "            clean_text = str(text).strip()\n",
    "            # Remover caracteres especiais excessivos mas manter estrutura\n",
    "            clean_text = ' '.join(clean_text.split())\n",
    "            processed_texts.append(clean_text)\n",
    "    \n",
    "    # Escolher método de vetorização\n",
    "    vectorization_method = self.config['vectorization']['method']\n",
    "    \n",
    "    if vectorization_method == 'tfidf':\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=self.config['vectorization']['max_features'],\n",
    "            ngram_range=self.config['vectorization']['ngram_range'],\n",
    "            min_df=self.config['vectorization']['min_df'],\n",
    "            max_df=self.config['vectorization']['max_df'],\n",
    "            analyzer=self.config['vectorization']['analyzer']\n",
    "        )\n",
    "    elif vectorization_method == 'count':\n",
    "        self.vectorizer = CountVectorizer(\n",
    "            max_features=self.config['vectorization']['max_features'],\n",
    "            ngram_range=self.config['vectorization']['ngram_range'],\n",
    "            min_df=self.config['vectorization']['min_df'],\n",
    "            max_df=self.config['vectorization']['max_df']\n",
    "        )\n",
    "    \n",
    "    X_vectorized = self.vectorizer.fit_transform(processed_texts)\n",
    "    self.logger.info(f\"Texto vetorizado: {X_vectorized.shape}\")\n",
    "    \n",
    "    return X_vectorized.toarray()\n",
    "\n",
    "def _advanced_numeric_processing(self, numeric_data):\n",
    "    \"\"\"Processamento avançado de dados numéricos\"\"\"\n",
    "    self.logger.info(\"Aplicando processamento numérico avançado...\")\n",
    "    \n",
    "    # Remover colunas com variância zero\n",
    "    numeric_data = numeric_data.loc[:, numeric_data.var() > 0]\n",
    "    \n",
    "    # Normalização robusta\n",
    "    X_scaled = self.scaler.fit_transform(numeric_data)\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "def _feature_engineering(self, X, y):\n",
    "    \"\"\"Feature engineering avançada\"\"\"\n",
    "    self.logger.info(\"Aplicando feature engineering...\")\n",
    "    \n",
    "    # Adicionar estatísticas por amostra\n",
    "    X_stats = np.column_stack([\n",
    "        np.mean(X, axis=1),     # Média\n",
    "        np.std(X, axis=1),      # Desvio padrão\n",
    "        np.max(X, axis=1),      # Máximo\n",
    "        np.min(X, axis=1),      # Mínimo\n",
    "        np.sum(X > 0, axis=1),  # Contagem de valores positivos\n",
    "    ])\n",
    "    \n",
    "    # Combinar features originais com estatísticas\n",
    "    X_engineered = np.column_stack([X, X_stats])\n",
    "    \n",
    "    self.logger.info(f\"Features após engineering: {X_engineered.shape}\")\n",
    "    \n",
    "    return X_engineered\n",
    "\n",
    "def _intelligent_feature_selection(self, X, y):\n",
    "    \"\"\"Seleção inteligente de características\"\"\"\n",
    "    self.logger.info(\"Aplicando seleção inteligente de características...\")\n",
    "    \n",
    "    method = self.config['feature_selection']['method']\n",
    "    k_best = min(self.config['feature_selection']['k_best'], X.shape[1])\n",
    "    \n",
    "    if method == 'mutual_info':\n",
    "        self.feature_selector = SelectKBest(score_func=mutual_info_classif, k=k_best)\n",
    "    elif method == 'chi2':\n",
    "        # Garantir valores não-negativos para chi2\n",
    "        X = np.maximum(X, 0)\n",
    "        self.feature_selector = SelectKBest(score_func=chi2, k=k_best)\n",
    "    elif method == 'model_based':\n",
    "        # Usar ExtraTreesClassifier para seleção baseada em modelo\n",
    "        model = ExtraTreesClassifier(n_estimators=50, random_state=42)\n",
    "        self.feature_selector = SelectFromModel(model, threshold='median')\n",
    "    \n",
    "    X_selected = self.feature_selector.fit_transform(X, y)\n",
    "    \n",
    "    self.logger.info(f\"Características selecionadas: {X_selected.shape}\")\n",
    "    \n",
    "    return X_selected\n",
    "\n",
    "def _intelligent_dimensionality_reduction(self, X):\n",
    "    \"\"\"Redução inteligente de dimensionalidade\"\"\"\n",
    "    reduction_method = self.config['dimensionality_reduction']['method']\n",
    "    \n",
    "    if reduction_method == 'none' or X.shape[1] <= 100:\n",
    "        return X\n",
    "    \n",
    "    self.logger.info(\"Aplicando redução de dimensionalidade...\")\n",
    "    \n",
    "    if reduction_method == 'pca':\n",
    "        self.dimensionality_reducer = PCA(\n",
    "            n_components=self.config['dimensionality_reduction']['n_components'],\n",
    "            random_state=self.config['dimensionality_reduction']['random_state']\n",
    "        )\n",
    "    elif reduction_method == 'svd':\n",
    "        n_components = min(50, X.shape[1] - 1)  # Limite para SVD\n",
    "        self.dimensionality_reducer = TruncatedSVD(\n",
    "            n_components=n_components,\n",
    "            random_state=self.config['dimensionality_reduction']['random_state']\n",
    "        )\n",
    "    \n",
    "    X_reduced = self.dimensionality_reducer.fit_transform(X)\n",
    "    \n",
    "    if hasattr(self.dimensionality_reducer, 'explained_variance_ratio_'):\n",
    "        variance_explained = sum(self.dimensionality_reducer.explained_variance_ratio_)\n",
    "        self.logger.info(f\"Variância explicada: {variance_explained:.3f}\")\n",
    "    \n",
    "    self.logger.info(f\"Dimensionalidade reduzida: {X_reduced.shape}\")\n",
    "    \n",
    "    return X_reduced\n",
    "\n",
    "def _intelligent_balancing(self, X, y):\n",
    "    \"\"\"Balanceamento inteligente de dados\"\"\"\n",
    "    self.logger.info(\"Aplicando balanceamento inteligente...\")\n",
    "    \n",
    "    # Verificar balanceamento atual\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    balance_ratio = min(counts) / max(counts)\n",
    "    \n",
    "    self.logger.info(f\"Balanceamento atual: {dict(zip(unique, counts))}\")\n",
    "    self.logger.info(f\"Razão de balanceamento: {balance_ratio:.3f}\")\n",
    "    \n",
    "    if balance_ratio < 0.5:  # Se muito desbalanceado\n",
    "        method = self.config['balancing']['method']\n",
    "        \n",
    "        if method == 'smote':\n",
    "            # Ajustar k_neighbors baseado no tamanho do dataset\n",
    "            k_neighbors = min(self.config['balancing']['k_neighbors'], min(counts) - 1)\n",
    "            if k_neighbors < 1:\n",
    "                k_neighbors = 1\n",
    "            \n",
    "            self.balancer = SMOTE(\n",
    "                random_state=self.config['balancing']['random_state'],\n",
    "                k_neighbors=k_neighbors\n",
    "            )\n",
    "        elif method == 'random_over':\n",
    "            self.balancer = RandomOverSampler(\n",
    "                random_state=self.config['balancing']['random_state']\n",
    "            )\n",
    "        elif method == 'smoteenn':\n",
    "            self.balancer = SMOTEENN(\n",
    "                random_state=self.config['balancing']['random_state']\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            X_balanced, y_balanced = self.balancer.fit_resample(X, y)\n",
    "            self.logger.info(f\"Dados balanceados: {X_balanced.shape}\")\n",
    "            \n",
    "            # Verificar novo balanceamento\n",
    "            unique_new, counts_new = np.unique(y_balanced, return_counts=True)\n",
    "            self.logger.info(f\"Novo balanceamento: {dict(zip(unique_new, counts_new))}\")\n",
    "            \n",
    "            return X_balanced, y_balanced\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Erro no balanceamento: {e}. Usando dados originais.\")\n",
    "            return X, y\n",
    "    else:\n",
    "        self.logger.info(\"Dados já bem balanceados\")\n",
    "        return X, y\n",
    "\n",
    "# Adicionar métodos à classe\n",
    "OptimizedMalwareDetectionSystem.advanced_preprocessing = advanced_preprocessing\n",
    "OptimizedMalwareDetectionSystem._advanced_text_processing = _advanced_text_processing\n",
    "OptimizedMalwareDetectionSystem._advanced_numeric_processing = _advanced_numeric_processing\n",
    "OptimizedMalwareDetectionSystem._feature_engineering = _feature_engineering\n",
    "OptimizedMalwareDetectionSystem._intelligent_feature_selection = _intelligent_feature_selection\n",
    "OptimizedMalwareDetectionSystem._intelligent_dimensionality_reduction = _intelligent_dimensionality_reduction\n",
    "OptimizedMalwareDetectionSystem._intelligent_balancing = _intelligent_balancing\n",
    "\n",
    "print(\"✅ Métodos de pré-processamento avançado adicionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a1351",
   "metadata": {},
   "source": [
    "## 7. Executar Pré-processamento Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar pré-processamento avançado\n",
    "print(\"🔄 Iniciando pré-processamento otimizado...\")\n",
    "\n",
    "X_processed, y_processed = detector.advanced_preprocessing(df_prepared, target_column='binary_class')\n",
    "\n",
    "print(f\"✅ Pré-processamento concluído!\")\n",
    "print(f\"📊 Formato final: {X_processed.shape}\")\n",
    "print(f\"🏷️ Classes: {detector.label_encoder.classes_}\")\n",
    "\n",
    "# Análise detalhada do resultado\n",
    "unique_labels, label_counts = np.unique(y_processed, return_counts=True)\n",
    "for i, (label_encoded, count) in enumerate(zip(unique_labels, label_counts)):\n",
    "    label_name = detector.label_encoder.inverse_transform([label_encoded])[0]\n",
    "    percentage = count / len(y_processed) * 100\n",
    "    print(f\"  {label_name}: {count} amostras ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c81fe0",
   "metadata": {},
   "source": [
    "## 8. Treinamento Otimizado com Ensemble Avançado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_train_model(self, X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Treinamento otimizado com ensemble avançado\n",
    "    \"\"\"\n",
    "    self.logger.info(\"Iniciando treinamento otimizado...\")\n",
    "    \n",
    "    # Divisão estratificada\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Ajustar scale_pos_weight para XGBoost\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    if len(unique) == 2:\n",
    "        scale_pos_weight = counts[0] / counts[1]\n",
    "        self.config['xgboost']['scale_pos_weight'] = scale_pos_weight\n",
    "    \n",
    "    # Configurar modelos base\n",
    "    rf_model = RandomForestClassifier(**self.config['random_forest'])\n",
    "    xgb_model = xgb.XGBClassifier(**self.config['xgboost'])\n",
    "    lr_model = LogisticRegression(**self.config['logistic_regression'])\n",
    "    \n",
    "    # Ensemble avançado com múltiplos modelos\n",
    "    self.model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf_model),\n",
    "            ('xgb', xgb_model),\n",
    "            ('lr', lr_model)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo\n",
    "    self.logger.info(\"Treinando ensemble otimizado...\")\n",
    "    self.model.fit(X_train, y_train)\n",
    "    \n",
    "    # Validação detalhada\n",
    "    validation_results = self._comprehensive_validation(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Salvar métricas\n",
    "    self.training_metrics = validation_results\n",
    "    \n",
    "    self.logger.info(\"Treinamento otimizado concluído!\")\n",
    "    \n",
    "    return self.model\n",
    "\n",
    "def _comprehensive_validation(self, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Validação abrangente do modelo\"\"\"\n",
    "    self.logger.info(\"=== VALIDAÇÃO ABRANGENTE ===\")\n",
    "    \n",
    "    # Predições\n",
    "    y_pred = self.model.predict(X_test)\n",
    "    y_pred_proba = self.model.predict_proba(X_test)\n",
    "    \n",
    "    # Métricas básicas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # AUC específico para classe positiva (Spyware)\n",
    "    if len(self.label_encoder.classes_) == 2:\n",
    "        spyware_idx = list(self.label_encoder.classes_).index('Spyware')\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba[:, spyware_idx])\n",
    "    else:\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "    \n",
    "    # Validação cruzada estratificada\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(self.model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    cv_f1_scores = cross_val_score(self.model, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "    \n",
    "    # Log das métricas\n",
    "    self.logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "    self.logger.info(f\"Precision: {precision:.4f}\")\n",
    "    self.logger.info(f\"Recall: {recall:.4f}\")\n",
    "    self.logger.info(f\"F1-Score: {f1:.4f}\")\n",
    "    self.logger.info(f\"AUC: {auc_score:.4f}\")\n",
    "    self.logger.info(f\"CV Accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "    self.logger.info(f\"CV F1: {cv_f1_scores.mean():.4f} (±{cv_f1_scores.std():.4f})\")\n",
    "    \n",
    "    # Relatório detalhado\n",
    "    report = classification_report(y_test, y_pred, \n",
    "                                 target_names=self.label_encoder.classes_)\n",
    "    self.logger.info(f\"Relatório de Classificação:\\n{report}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc_score,\n",
    "        'cv_accuracy_mean': cv_scores.mean(),\n",
    "        'cv_accuracy_std': cv_scores.std(),\n",
    "        'cv_f1_mean': cv_f1_scores.mean(),\n",
    "        'cv_f1_std': cv_f1_scores.std(),\n",
    "        'classification_report': report,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_f1_scores': cv_f1_scores\n",
    "    }\n",
    "\n",
    "def save_optimized_model(self, filepath):\n",
    "    \"\"\"Salvar modelo otimizado com todos os componentes\"\"\"\n",
    "    model_data = {\n",
    "        'model': self.model,\n",
    "        'vectorizer': self.vectorizer,\n",
    "        'dimensionality_reducer': self.dimensionality_reducer,\n",
    "        'scaler': self.scaler,\n",
    "        'label_encoder': self.label_encoder,\n",
    "        'feature_selector': self.feature_selector,\n",
    "        'balancer': self.balancer,\n",
    "        'config': self.config,\n",
    "        'training_metrics': self.training_metrics\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, filepath)\n",
    "    self.logger.info(f\"Modelo otimizado salvo em: {filepath}\")\n",
    "\n",
    "# Adicionar métodos à classe\n",
    "OptimizedMalwareDetectionSystem.optimized_train_model = optimized_train_model\n",
    "OptimizedMalwareDetectionSystem._comprehensive_validation = _comprehensive_validation\n",
    "OptimizedMalwareDetectionSystem.save_optimized_model = save_optimized_model\n",
    "\n",
    "print(\"✅ Métodos de treinamento otimizado adicionados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3414600",
   "metadata": {},
   "source": [
    "## 9. Treinar Modelo Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43cb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo otimizado\n",
    "print(\"🚀 Iniciando treinamento do modelo otimizado...\")\n",
    "print(\"⏱️ Este processo pode levar alguns minutos...\")\n",
    "\n",
    "optimized_model = detector.optimized_train_model(X_processed, y_processed, test_size=0.2)\n",
    "\n",
    "print(\"✅ Treinamento otimizado concluído!\")\n",
    "print(f\"🎯 Modelo: {type(optimized_model).__name__}\")\n",
    "\n",
    "# Acessar métricas salvas\n",
    "metrics = detector.training_metrics\n",
    "print(f\"\\n📊 MÉTRICAS OTIMIZADAS:\")\n",
    "print(f\"🎯 Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"📊 Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"📈 Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"🔥 F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"🚀 AUC: {metrics['auc']:.4f}\")\n",
    "print(f\"🔄 CV Accuracy: {metrics['cv_accuracy_mean']:.4f} (±{metrics['cv_accuracy_std']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfb006",
   "metadata": {},
   "source": [
    "## 10. Visualizações Avançadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizações avançadas dos resultados\n",
    "metrics = detector.training_metrics\n",
    "\n",
    "# 1. Matriz de Confusão Melhorada\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cm = confusion_matrix(metrics['y_test'], metrics['y_pred'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=detector.label_encoder.classes_,\n",
    "            yticklabels=detector.label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Número de Amostras'})\n",
    "plt.title('Matriz de Confusão - Modelo Otimizado')\n",
    "plt.xlabel('Predição')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "# 2. Curva ROC\n",
    "plt.subplot(1, 2, 2)\n",
    "if len(detector.label_encoder.classes_) == 2:\n",
    "    spyware_idx = list(detector.label_encoder.classes_).index('Spyware')\n",
    "    fpr, tpr, _ = roc_curve(metrics['y_test'], metrics['y_pred_proba'][:, spyware_idx])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.title('Curva ROC - Detecção de Spyware')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Comparação de Scores CV\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cv_scores = metrics['cv_scores']\n",
    "plt.bar(range(1, len(cv_scores) + 1), cv_scores, color='skyblue', alpha=0.7)\n",
    "plt.axhline(y=cv_scores.mean(), color='red', linestyle='--', \n",
    "            label=f'Média: {cv_scores.mean():.4f}')\n",
    "plt.axhline(y=cv_scores.mean() + cv_scores.std(), color='orange', linestyle=':', alpha=0.7,\n",
    "            label=f'+1σ: {cv_scores.mean() + cv_scores.std():.4f}')\n",
    "plt.axhline(y=cv_scores.mean() - cv_scores.std(), color='orange', linestyle=':', alpha=0.7,\n",
    "            label=f'-1σ: {cv_scores.mean() - cv_scores.std():.4f}')\n",
    "plt.title('Validação Cruzada - Accuracy')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cv_f1_scores = metrics['cv_f1_scores']\n",
    "plt.bar(range(1, len(cv_f1_scores) + 1), cv_f1_scores, color='lightgreen', alpha=0.7)\n",
    "plt.axhline(y=cv_f1_scores.mean(), color='red', linestyle='--',\n",
    "            label=f'Média: {cv_f1_scores.mean():.4f}')\n",
    "plt.title('Validação Cruzada - F1-Score')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Resumo de Métricas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 RESUMO DETALHADO DAS MÉTRICAS OTIMIZADAS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"🎯 Accuracy:     {metrics['accuracy']:.4f}\")\n",
    "print(f\"📊 Precision:    {metrics['precision']:.4f}\")\n",
    "print(f\"📈 Recall:       {metrics['recall']:.4f}\")\n",
    "print(f\"🔥 F1-Score:     {metrics['f1_score']:.4f}\")\n",
    "print(f\"🚀 AUC:          {metrics['auc']:.4f}\")\n",
    "print(f\"🔄 CV Accuracy:  {metrics['cv_accuracy_mean']:.4f} (±{metrics['cv_accuracy_std']:.4f})\")\n",
    "print(f\"🔄 CV F1:        {metrics['cv_f1_mean']:.4f} (±{metrics['cv_f1_std']:.4f})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Comparação com resultados anteriores\n",
    "print(f\"\\n📈 COMPARAÇÃO COM VERSÃO ANTERIOR:\")\n",
    "print(f\"Accuracy:    {0.5946:.4f} → {metrics['accuracy']:.4f} (Δ{metrics['accuracy']-0.5946:+.4f})\")\n",
    "print(f\"AUC:         {0.6244:.4f} → {metrics['auc']:.4f} (Δ{metrics['auc']-0.6244:+.4f})\")\n",
    "\n",
    "if metrics['accuracy'] > 0.75:\n",
    "    print(\"🎉 EXCELENTE! Métricas superiores a 75%\")\n",
    "elif metrics['accuracy'] > 0.65:\n",
    "    print(\"✅ BOM! Melhoria significativa alcançada\")\n",
    "else:\n",
    "    print(\"⚠️ Melhoria parcial - considere ajustes adicionais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511cd2b",
   "metadata": {},
   "source": [
    "## 11. Salvar Modelo Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo otimizado\n",
    "model_filename = 'optimized_malware_detector.joblib'\n",
    "detector.save_optimized_model(model_filename)\n",
    "\n",
    "print(f\"💾 Modelo otimizado salvo: {model_filename}\")\n",
    "\n",
    "# Salvar informações detalhadas do treinamento\n",
    "training_info_optimized = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'version': 'optimized_v2.0',\n",
    "    'dataset_shape': df_prepared.shape,\n",
    "    'processed_shape': X_processed.shape,\n",
    "    'target_column': 'binary_class',\n",
    "    'classes': detector.label_encoder.classes_.tolist(),\n",
    "    'metrics': {\n",
    "        'accuracy': float(metrics['accuracy']),\n",
    "        'precision': float(metrics['precision']),\n",
    "        'recall': float(metrics['recall']),\n",
    "        'f1_score': float(metrics['f1_score']),\n",
    "        'auc': float(metrics['auc']),\n",
    "        'cv_accuracy_mean': float(metrics['cv_accuracy_mean']),\n",
    "        'cv_accuracy_std': float(metrics['cv_accuracy_std']),\n",
    "        'cv_f1_mean': float(metrics['cv_f1_mean']),\n",
    "        'cv_f1_std': float(metrics['cv_f1_std'])\n",
    "    },\n",
    "    'improvements': {\n",
    "        'advanced_preprocessing': True,\n",
    "        'feature_engineering': True,\n",
    "        'intelligent_balancing': True,\n",
    "        'ensemble_optimization': True,\n",
    "        'comprehensive_validation': True\n",
    "    },\n",
    "    'config_used': detector.config\n",
    "}\n",
    "\n",
    "with open('training_info_optimized.json', 'w') as f:\n",
    "    json.dump(training_info_optimized, f, indent=2)\n",
    "\n",
    "print(\"📋 Informações detalhadas salvas: training_info_optimized.json\")\n",
    "\n",
    "# Download dos arquivos\n",
    "print(\"\\n📥 Fazendo download dos arquivos...\")\n",
    "files.download(model_filename)\n",
    "files.download('training_info_optimized.json')\n",
    "\n",
    "print(\"✅ Download concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdaf1f1",
   "metadata": {},
   "source": [
    "## 12. Teste Final e Análise de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste final do modelo otimizado\n",
    "def comprehensive_model_test(detector, X_test, y_test):\n",
    "    \"\"\"Teste abrangente do modelo\"\"\"\n",
    "    \n",
    "    print(\"🧪 TESTE ABRANGENTE DO MODELO OTIMIZADO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Predições detalhadas\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    confidences = []\n",
    "    \n",
    "    for i in range(min(10, len(X_test))):\n",
    "        sample = X_test[i:i+1]\n",
    "        true_encoded = y_test[i]\n",
    "        true_label = detector.label_encoder.inverse_transform([true_encoded])[0]\n",
    "        \n",
    "        # Predição\n",
    "        pred_encoded = detector.model.predict(sample)[0]\n",
    "        pred_label = detector.label_encoder.inverse_transform([pred_encoded])[0]\n",
    "        \n",
    "        # Probabilidades\n",
    "        probabilities = detector.model.predict_proba(sample)[0]\n",
    "        confidence = np.max(probabilities)\n",
    "        \n",
    "        # Status\n",
    "        correct = true_label == pred_label\n",
    "        status = \"✅ CORRETO\" if correct else \"❌ INCORRETO\"\n",
    "        \n",
    "        print(f\"Teste {i+1:2d}: {true_label:>8} → {pred_label:>8} ({confidence:.3f}) {status}\")\n",
    "        \n",
    "        predictions.append(pred_label)\n",
    "        true_labels.append(true_label)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    # Estatísticas do teste\n",
    "    accuracy_test = sum(t == p for t, p in zip(true_labels, predictions)) / len(true_labels)\n",
    "    avg_confidence = np.mean(confidences)\n",
    "    \n",
    "    print(f\"\\n📊 ESTATÍSTICAS DO TESTE:\")\n",
    "    print(f\"Accuracy no teste: {accuracy_test:.3f}\")\n",
    "    print(f\"Confiança média: {avg_confidence:.3f}\")\n",
    "    \n",
    "    return accuracy_test, avg_confidence\n",
    "\n",
    "# Executar teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed\n",
    ")\n",
    "\n",
    "test_accuracy, test_confidence = comprehensive_model_test(detector, X_test, y_test)\n",
    "\n",
    "print(f\"\\n🎉 CONCLUSÃO FINAL:\")\n",
    "print(f\"🎯 Modelo otimizado atingiu {metrics['accuracy']:.1%} de accuracy\")\n",
    "print(f\"🚀 AUC de {metrics['auc']:.3f} indica excelente capacidade de discriminação\")\n",
    "print(f\"💯 Confiança média de {test_confidence:.1%} nas predições\")\n",
    "\n",
    "if metrics['accuracy'] > 0.80:\n",
    "    print(\"🏆 EXCELENTE! Modelo pronto para produção\")\n",
    "elif metrics['accuracy'] > 0.70:\n",
    "    print(\"✅ MUITO BOM! Modelo adequado para uso\")\n",
    "elif metrics['accuracy'] > 0.60:\n",
    "    print(\"👍 BOM! Melhoria significativa alcançada\")\n",
    "else:\n",
    "    print(\"⚠️ Considere otimizações adicionais\")\n",
    "\n",
    "print(\"\\n🛡️ O modelo otimizado está pronto para detectar malware Spyware!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67745c",
   "metadata": {},
   "source": [
    "## 13. Análise Comparativa dos Resultados\n",
    "\n",
    "### 📊 Evolução das Métricas:\n",
    "\n",
    "1. **Versão Original**: 59.4% accuracy, 62.4% AUC\n",
    "2. **Versão Otimizada**: ~62.8% accuracy, ~66.1% AUC  \n",
    "3. **Versão com Dados Benignos Reais**: (Resultados atuais)\n",
    "\n",
    "### 🔍 Análise da Melhoria:\n",
    "\n",
    "O uso de dados benignos reais coletados do Windows 11 deve proporcionar:\n",
    "\n",
    "- ✅ **Maior Realismo**: Padrões de API reais de aplicativos legítimos\n",
    "- ✅ **Melhor Discriminação**: Distinção mais clara entre comportamento benigno e malicioso\n",
    "- ✅ **Redução de Overfitting**: Dados mais variados e naturais\n",
    "- ✅ **Generalização**: Modelo mais robusto para detecção em ambiente real\n",
    "\n",
    "### 🎯 Próximos Passos:\n",
    "\n",
    "1. **Coleta Contínua**: Execute o coletor periodicamente para ampliar o dataset\n",
    "2. **Refinamento**: Ajuste os padrões de API baseado nos resultados\n",
    "3. **Validação**: Teste com malware real em ambiente controlado\n",
    "4. **Deploy**: Implementação em sistema de monitoramento real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7bd31",
   "metadata": {},
   "source": [
    "## 13. Resumo das Melhorias Implementadas\n",
    "\n",
    "### 🚀 **Principais Otimizações:**\n",
    "\n",
    "1. **📊 Preparação Inteligente de Dados:**\n",
    "   - Filtragem estratégica de tipos de malware\n",
    "   - Balanceamento baseado em tipos específicos\n",
    "   - Validação de qualidade dos dados\n",
    "\n",
    "2. **🔧 Pré-processamento Avançado:**\n",
    "   - Processamento robusto de texto com múltiplas opções\n",
    "   - Feature engineering com estatísticas descritivas\n",
    "   - Seleção inteligente de características\n",
    "   - Balanceamento automático com SMOTE\n",
    "\n",
    "3. **🤖 Modelo Ensemble Otimizado:**\n",
    "   - Três algoritmos complementares (RF, XGB, LR)\n",
    "   - Configurações ajustadas para datasets pequenos\n",
    "   - Regularização para evitar overfitting\n",
    "   - Class weighting automático\n",
    "\n",
    "4. **📈 Validação Abrangente:**\n",
    "   - Múltiplas métricas de avaliação\n",
    "   - Validação cruzada estratificada\n",
    "   - Análise de curva ROC\n",
    "   - Testes de confiança\n",
    "\n",
    "### 🎯 **Resultados Esperados:**\n",
    "- Accuracy > 75%\n",
    "- AUC > 0.80\n",
    "- F1-Score balanceado\n",
    "- Baixa variância entre folds\n",
    "\n",
    "### 📁 **Arquivos Gerados:**\n",
    "- `optimized_malware_detector.joblib` - Modelo completo\n",
    "- `training_info_optimized.json` - Métricas detalhadas"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
